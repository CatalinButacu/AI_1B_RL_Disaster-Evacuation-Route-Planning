\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithm}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\title{Disaster Evacuation Route Planning in Uncertain Environments using Dyna-Q}
\author{\IEEEauthorblockN{1\textsuperscript{st} Ionel-Cătălin Butacu}
\IEEEauthorblockA{\textit{Faculty of Automatic Control and Computer Engineering}\\
\textit{Technical University Gheorghe Asachi Iasi}\\
Iași, Romania \\
ionel-catalin.butacu@student.tuiasi.ro}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Radu-Ionuț Bălăiță}
\IEEEauthorblockA{\textit{Faculty of Automatic Control and Computer Engineering}\\
\textit{Technical University Gheorghe Asachi Iasi}\\
Iași, Romania \\
radu-ionut.balaita@student.tuiasi.ro}
}
\maketitle
\begin{abstract}
Disaster evacuation planning is a critical challenge in emergency management, requiring robust strategies to guide individuals to safety amidst dynamic and unpredictable hazards. This paper presents a Multi-Agent Reinforcement Learning (MARL) system that utilizes the Dyna-Q algorithm to solve evacuation problems in discrete grid-based environments. The system simulates dynamic hazards such as floods, fires, earthquakes, and tornadoes, creating a stochastic environment where agents must balance immediate survival with efficient route planning. Unlike traditional pathfinding algorithms which assume static terrains, our approach enables agents to adapt to changing hazard boundaries in real-time. Our results demonstrate that model-based reinforcement learning significantly accelerates convergence compared to model-free approaches. In widely simulated scenarios involving 15 agents on a $20 \times 20$ grid, the proposed system achieves high survival rates in stochastic flood scenarios, showcasing the effectiveness of Dyna-Q for real-time adaptive evacuation planning.
\end{abstract}
\begin{IEEEkeywords}
reinforcement learning, dyna-q, multi-agent systems, evacuation planning, disaster management
\end{IEEEkeywords}
\section{Introduction}
Disaster evacuation planning is a fundamental aspect of civil safety, yet it remains computationally challenging due to the inherent uncertainty of emergency situations. Traditional pathfinding algorithms, such as A* or Dijkstra, rely on static graph representations of the environment \cite{b4}. While optimal in stable conditions, these algorithms fail when the graph connectivity changes dynamically---such as when floodwaters rise, fires spread, or buildings collapse. In such scenarios, a pre-calculated path may lead agents directly into a newly formed hazard.
This project addresses this limitation by determining optimal policies using Multi-Agent Reinforcement Learning (MARL). We specifically utilize the \textbf{Dyna-Q} algorithm \cite{b2}, a model-based approach that combines direct experience with simulated planning. This hybrid architecture allows agents to learn optimal policies in uncertain, stochastic environments where hazard dynamics are not known a priori. By maintaining an internal model of the world, agents can "dream" about future consequences, effectively propagating safety information across the state space without the need for dangerous trial-and-error in the physical environment.
\subsection{Related Work}
\subsubsection{Classical Methods}
Simulation of crowd dynamics has traditionally relied on Social Force Models (SFM) \cite{b1}, which treat individuals as particles subject to physical forces. While effective for modeling panic and congestion, these models often lack high-level decision-making capabilities regarding optimal route selection in complex, changing mazes.
\subsubsection{Reinforcement Learning}
Recent advancements in Deep Reinforcement Learning (DRL), such as Deep Q-Networks (DQN), have shown success in complex, high-dimensional environments \cite{b3}. However, these methods often suffer from high sample complexity, requiring millions of interactions to converge, and lack interpretability---a black-box risk in safety-critical applications. In contrast, tabular methods like Dyna-Q provide transparency and significantly faster initial convergence for grid-based problems. Our work distinguishes itself by demonstrating that Dyna-Q's planning phase allows for the rapid propagation of hazard information (e.g., "this path leads to death") across the state space, a crucial feature for minimizing casualties during training and deployment.
\section{System Architecture}
The system is designed as a modular simulation framework composed of three main layers: the Simulation Environment, the Agent Model, and the Interaction Loop.
\subsection{Environment}
The environment is modeled as a discrete $N \times N$ grid, representing a simplified urban layout.
\begin{itemize}
    \item \textbf{State Space}: The state is defined by the agent's position $s = (x, y)$. For a grid of size $N=20$, the observation space size is $400$ discrete states.
    \item \textbf{Action Space}: Agents have 4 discrete actions: $A = \{Up, Down, Left, Right\}$. Movement is deterministic unless blocked by an obstacle.
    \item \textbf{Entities}:
    \begin{itemize}
        \item \textbf{Agents}: Independent entities seeking evacuation zones. They possess local observations of the grid.
        \item \textbf{Hazards}: Dynamic danger zones (Flood, Fire, etc.) that expand according to probabilistic rules.
        \item \textbf{Obstacles}: Static blocked cells representing walls or debris.
    \end{itemize}
\end{itemize}
\subsection{Hazard Dynamics}
We model four distinct hazard scenarios, each with unique uncertainty patterns, as detailed in Table \ref{tab:scenarios}. These dynamics challenge the agents to generalize their navigation policies rather than memorizing fixed paths.
\begin{table}[htbp]
\caption{Hazard Scenarios and Uncertainty Patterns}
\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Scenario} & \textbf{Uncertainty Pattern} \\
\hline
Flood & Deterministic spread from corner (unknown timing) \\
Fire & Stochastic circular spread (random rate) \\
Earthquake & Random building collapse (highly stochastic) \\
Tornado & Moving hazard zone (unpredictable path) \\
\hline
\end{tabular}
\label{tab:scenarios}
\end{center}
\end{table}
\section{Implementation Details}
We model the evacuation problem as a Markov Decision Process (MDP) defined by the tuple $(S, A, P, R, \gamma)$.
\subsection{Dyna-Q Algorithm}
The core algorithm integrates model learning with direct RL. The complete procedure is outlined in Algorithm 1.
\begin{algorithm}
\caption{Dyna-Q Algorithm for Evacuation}
\begin{algorithmic}
\STATE Initialize $Q(s, a)$ and $Model(s, a)$ for all $s \in S, a \in A$
\STATE Loop for each episode:
\STATE \quad $S \leftarrow$ current (start) state
\STATE \quad Loop until $S$ is terminal:
\STATE \quad \quad $A \leftarrow \epsilon$-greedy$(S, Q)$
\STATE \quad \quad Execute action $A$; observe resultant reward $R$ and state $S'$
\STATE \quad \quad $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma \max_{a} Q(S', a) - Q(S, A)]$
\STATE \quad \quad $Model(S, A) \leftarrow (R, S')$
\STATE \quad \quad Loop $K$ times (Planning):
\STATE \quad \quad \quad $S_{plan} \leftarrow$ random previously observed state
\STATE \quad \quad \quad $A_{plan} \leftarrow$ random action taken in $S_{plan}$
\STATE \quad \quad \quad $R_{plan}, S'_{plan} \leftarrow Model(S_{plan}, A_{plan})$
\STATE \quad \quad \quad $Q(S_{plan}, A_{plan}) \leftarrow Q(S_{plan}, A_{plan}) + \alpha [R_{plan} + \gamma \max_{a} Q(S'_{plan}, a) - Q(S_{plan}, A_{plan})]$
\STATE \quad \quad $S \leftarrow S'$
\end{algorithmic}
\end{algorithm}
\subsection{Configuration and Hyperparameters}
The system configuration is critical for performance. Table \ref{tab:config} outlines the key parameters used in our experiments.
\begin{table}[htbp]
\caption{System Configuration Parameters}
\begin{center}
\begin{tabular}{|l|c|l|}
\hline
\textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
\hline
GRID\_SIZE & 20 & Map dimensions ($20 \times 20$) \\
NUM\_AGENTS & 15 & Total agents to evacuate \\
NUM\_EXITS & 1 & Safe evacuation zones \\
PLANNING\_STEPS & 50 & Simulated steps per real step ($K$) \\
GAMMA ($\gamma$) & 0.95 & Discount factor \\
ALPHA ($\alpha$) & 0.15 & Learning rate \\
EPSILON ($\epsilon$) & 0.3 & Exploration rate (decaying) \\
\hline
\end{tabular}
\label{tab:config}
\end{center}
\end{table}
We chose a discount factor $\gamma=0.95$ to prioritize long-term survival over immediate rewards. The planning parameter $K=50$ was selected empirically to balance computational cost with convergence speed. A learning rate of $\alpha=0.15$ ensures stable updates without oscilliating.
\subsection{Reward Structure}
The reward function is shaped to guide agents towards optimal behavior:
\begin{itemize}
    \item \textbf{Evacuation (Goal)}: $+100$. This large positive reward is the primary objective function.
    \item \textbf{Hazard/Death}: $-50$. A severe penalty to discourage hazardous states.
    \item \textbf{Step Cost}: $-1$. This encourages the agent to find the shortest path to the exit.
    \item \textbf{Obstacle/Wall}: $-2$. A penalty for colliding with static geometry.
    \item \textbf{Heuristic Shaping}: $+3$ for moving closer to a known exit, $-1$ for moving away. This dense reward signal helps guide the initial exploration phase.
\end{itemize}
\begin{figure}[htbp]
\centerline{\fbox{\includegraphics[width=0.9\linewidth]{Example.png}}}
\caption{Snapshot of the simulation environment showing agents (colored figures) navigating towards the green exit zone while avoiding the blue spreading flood hazard. Grey blocks represent static obstacles.}
\label{fig:env}
\end{figure}
\section{Evaluation and Results}
\subsection{Experimental Setup}
We evaluated the system on a $20 \times 20$ grid with 15 agents over 5000 training episodes. The primary metric is the \textbf{Survival Rate}, defined as the percentage of agents that successfully reach the exit. Agents spawn at random locations in the safe zone at the start of each episode.
\subsection{Comparative Analysis}
We conducted a comparative benchmark between Dyna-Q (Model-Based) and Standard Q-Learning (Model-Free) over 500 episodes in the Flood scenario. As illustrated in Figure \ref{fig:benchmark}, Dyna-Q demonstrates superior usage of sparse data.
\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{Benchmark_Plot.png}}
\caption{Survival Rate comparison between Dyna-Q and Q-Learning over 500 training episodes. Dyna-Q (blue) converges faster due to simulated planning.}
\label{fig:benchmark}
\end{figure}
\begin{itemize}
    \item \textbf{Convergence Speed}: Dyna-Q agents achieve survival considerably faster than Q-Learning agents. This is directly attributable to the Planning steps ($K=50$).
    \item \textbf{Safety Awareness}: In standard Q-learning, reward propagation is slow; an agent deep in the grid only learns about a distant hazard after many episodes of traversing the entire path. In Dyna-Q, once a single agent experiences a hazard (receiving $R=-50$), the model records this. During the planning phase, this negative value is propagated back to adjacent states almost immediately. Effectively, adjacent states "learn" they are dangerous before the agent ever visits them again physically.
\end{itemize}
\section{Conclusions}
This project validates the efficacy of Dyna-Q for disaster evacuation planning. By integrating model-based planning with model-free learning, agents can learn optimal policies with fewer real-world interactions. The system successfully handles diverse hazard dynamics, demonstrating robust adaptability. The use of a shaped reward function combined with the Dyna-Q planning architecture ensures that agents not only find the shortest path but also adapt to the stochastic nature of the hazards.
\begin{thebibliography}{00}
\bibitem{b1} D. Helbing, I. Farkas, and T. Vicsek, "Simulating dynamical features of escape panic," \textit{Nature}, vol. 407, pp. 487--490, 2000.
\bibitem{b2} R. S. Sutton, "Dyna, an integrated architecture for learning, planning, and reacting," \textit{ACM SIGART Bulletin}, vol. 2, no. 4, pp. 160--163, 1991.
\bibitem{b3} V. Mnih et al., "Human-level control through deep reinforcement learning," \textit{Nature}, vol. 518, pp. 529--533, 2015.
\bibitem{b4} P. E. Hart, N. J. Nilsson, and B. Raphael, "A Formal Basis for the Heuristic Determination of Minimum Cost Paths," \textit{IEEE Transactions on Systems Science and Cybernetics}, vol. 4, no. 2, pp. 100--107, 1968.
\end{thebibliography}
\end{document}
