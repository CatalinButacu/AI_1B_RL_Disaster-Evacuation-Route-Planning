\documentclass[conference,a4paper]{IEEEtran}
\pdfpagewidth=210mm
\pdfpageheight=297mm
\pdfoptionpdfminorversion=7
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithm}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\title{Disaster Evacuation Route Planning in Uncertain Environments using Dyna-Q}
\author{\IEEEauthorblockN{1\textsuperscript{st} Ionel-Cătălin Butacu}
\IEEEauthorblockA{\textit{Faculty of Automatic Control and Computer Engineering}\\
\textit{Technical University Gheorghe Asachi Iasi}\\
Iași, Romania \\
ionel-catalin.butacu@student.tuiasi.ro}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Radu-Ionuț Bălăiță}
\IEEEauthorblockA{\textit{Faculty of Automatic Control and Computer Engineering}\\
\textit{Technical University Gheorghe Asachi Iasi}\\
Iași, Romania \\
radu-ionut.balaita@student.tuiasi.ro}
}
\maketitle
Disaster evacuation planning involves guiding individuals to safety amidst dynamic hazards. This paper presents a Multi-Agent Reinforcement Learning (MARL) system utilizing the Dyna-Q algorithm family in non-stationary, grid-based environments. We examine the effects of "sparse rewards" and environmental shifts using a Dyna-Q+ architecture. The system simulates floods, fires, earthquakes, and tornadoes with distinct propagation patterns. Using distance-based reward shaping, we direct agents toward evacuation zones while penalizing hazard contact. Our experiments on a $20 \times 20$ grid show that the Dyna-Q+ implementation improves survival rates compared to a Random Walk baseline, reaching convergence in 3 out of 4 scenarios after 5,000 training episodes.
\begin{IEEEkeywords}
reinforcement learning, dyna-q, non-stationary environments, reward shaping, disaster management
\end{IEEEkeywords}
\section{Topic Description}
Disaster evacuation planning requires addressing the non-stationarity of emergency situations. Traditional pathfinding algorithms, such as A* or Dijkstra, rely on static graph representations of the environment \cite{b4}. While identifying optimal paths in stable conditions, these algorithms are limited when graph connectivity changes dynamically---such as when floodwaters rise, fires spread, or buildings collapse. In such scenarios, a pre-calculated path may lead agents into a newly formed hazard.

This project addresses this limitation by determining optimal policies using Multi-Agent Reinforcement Learning (MARL). We specifically utilize the \textbf{Dyna-Q} algorithm \cite{b2}, a model-based approach that combines direct experience with simulated planning. This hybrid architecture allows agents to learn optimal policies in uncertain, stochastic environments where hazard dynamics are not known a priori. We implement a refined reward structure utilizing "Reward Shaping" to overcome the sparse reward problem common in large-scale grids. By maintaining an internal model of the world, agents can "dream" about future consequences, effectively propagating safety information across the state space without the need for dangerous trial-and-error in the physical environment.
% Related Work integrated into Topic Description
\section{System Architecture}
The system is designed as a modular simulation framework composed of three main layers: the Simulation Environment, the Agent Model, and the Interaction Loop.

\subsection{States and Actions Design}
The evacuation process follows a Markov Decision Process (MDP) with a specialized state-action configuration:
\begin{itemize}
    \item \textbf{States ($S$)}: Represented by the coordinates $(x, y)$ in the $20 \times 20$ grid. The agents operate within a 400-state space, where each state encodes path accessibility or hazard presence.
    \item \textbf{Actions ($A$)}: A discrete set $\{Up, Down, Left, Right\}$. Movement results in a deterministic state transition unless intercepted by grid boundaries or static obstacles.
\end{itemize}

\subsection{Class Hierarchy and Algorithmic Evolution}
To support a comparative study of model-free and model-based learning, the system utilizes an object-oriented inheritance structure. This hierarchy ensures that each agent variant builds upon the baseline capabilities of its predecessor:
\begin{itemize}
    \item \textbf{\texttt{QLearning} (Base)}: Manages the core Q-table, $\epsilon$-greedy action selection, and temporal difference updates. It serves as the reactive baseline.
    \item \textbf{\texttt{DynaQ} (Layer 1)}: Inherits from \texttt{QLearning}, introducing an internal \textit{EnvironmentModel} that stores $(s, a, r, s')$ transitions. It enables background planning loops.
    \item \textbf{\texttt{DynaQPlus} (Layer 2)}: Extends \texttt{DynaQ} by implementing a time-weighted exploration bonus $\kappa\sqrt{\tau}$. This identifies states that have not been visited for extended periods, facilitating adaptation to non-stationary hazards.
\end{itemize}



\subsection{Hazard Dynamics}
We model four distinct hazard scenarios, originating from the corner (0,0). The hazard dynamics are detailed in Table \ref{tab:scenarios}.
\begin{table}[htbp]
\caption{Hazard Scenarios and Propagation Patterns}
\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Scenario} & \textbf{Propagation Pattern} \\
\hline
Flood & Radial spread from origin (spread rate 0.6) \\
Fire & Wind-directed stochastic neighbor spread \\
Earthquake & Wave-based shockwaves with collapses \\
Tornado & Non-stationary moving circular hazard \\
\hline
\end{tabular}
\label{tab:scenarios}
\end{center}
\end{table}
To address the sparse reward problem, we implemented a dense \textbf{Heuristic Shaping} function $H(s) = \phi(s_{next}) - \phi(s_{curr})$. The specific reward weights are categorized in Table \ref{tab:rewards}. This shaping provides a gradient for agents far from the exit, mitigating the "blind walk" effect in large $20 \times 20$ grids.

\begin{table}[htbp]
\caption{Reward weights and Signal Intensity}
\begin{center}
\begin{tabular}{|l|c|l|}
\hline
\textbf{Event} & \textbf{Reward} & \textbf{Signaling Purpose} \\
\hline
Evacuation & $+200$ & Objective Completion \\
Hazard Contact & $-50$ & Survivability Penalty \\
Wall/Obstacle & $-2$ & Geometry Constraint \\
Step Cost & $-1$ & Path Optimization \\
Distance Decrease & $+2$ & Directional Guidance \\
Distance Increase & $-1.5$ & Corrective Feedback \\
\hline
\end{tabular}
\label{tab:rewards}
\end{center}
\end{table}

\section{Implementation Details}
The software implementation translates the theoretical Dyna-Q architecture into a highly optimized Python framework.

\subsection{Dyna-Series Implementation}
The core logic integrates direct learning with heuristic planning (Algorithm 1). Key technical optimizations include:
\begin{itemize}
    \item \textbf{Vectorized Q-Table}: Implemented as a NumPy utility for fast indexing.
    \item \textbf{State Representation}: States are flattened into a 1D observation space ($0 \dots 399$) to enable O(1) tabular lookup.
    \item \textbf{Optimized Dyna-Q+}: Implementation of time-stamping rather than counters for exploration tracking to minimize memory overhead.
\end{itemize}

Empirical testing shows that the number of planning steps ($K$) directly influences learning stability. While higher $K$ values (e.g., $K=50$) accelerate initial convergence, they propagate "stale" information if the hazard dynamics shift rapidly. Our chosen $K=5$ provides a balance between learning speed and representative accuracy.

\subsection{Dyna-Q Algorithm}
The core algorithm integrates model learning with direct RL. The complete procedure is outlined in Algorithm 1.
\begin{algorithm}
\caption{Dyna-Q Algorithm for Evacuation}
\begin{algorithmic}
\STATE Initialize $Q(s, a)$ and $Model(s, a)$ for all $s \in S, a \in A$
\STATE Loop for each episode:
\STATE \quad $S \leftarrow$ current (start) state
\STATE \quad Loop until $S$ is terminal:
\STATE \quad \quad $A \leftarrow \epsilon$-greedy$(S, Q)$
\STATE \quad \quad Execute action $A$; observe resultant reward $R$ and state $S'$
\STATE \quad \quad $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma \max_{a} Q(S', a) - Q(S, A)]$
\STATE \quad \quad $Model(S, A) \leftarrow (R, S')$
\STATE \quad \quad Loop $K$ times (Planning):
\STATE \quad \quad \quad $S_{plan} \leftarrow$ random previously observed state
\STATE \quad \quad \quad $A_{plan} \leftarrow$ random action taken in $S_{plan}$
\STATE \quad \quad \quad $R_{plan}, S'_{plan} \leftarrow Model(S_{plan}, A_{plan})$
\STATE \quad \quad \quad $Q(S_{plan}, A_{plan}) \leftarrow Q(S_{plan}, A_{plan}) + \alpha [R_{plan} + \gamma \max_{a} Q(S'_{plan}, a) - Q(S_{plan}, A_{plan})]$
\STATE \quad \quad $S \leftarrow S'$
\end{algorithmic}
\end{algorithm}
\subsection{Configuration and Hyperparameters}
The system configuration underwent significant tuning during development. Initial experiments revealed that aggressive hyperparameters caused learning instability. Table \ref{tab:config} outlines the final tuned parameters.
\begin{table}[htbp]
\caption{System Configuration Parameters}
\begin{center}
\begin{tabular}{|l|c|l|}
\hline
\textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
\hline
GRID\_SIZE & 20 & Map dimensions ($20 \times 20$) \\
NUM\_AGENTS & 15 & Agents per scenario \\
NUM\_EXITS & 3 & Safe corner evacuation zones \\
K (PLANNING) & 5 & Steps per real interaction \\
GAMMA ($\gamma$) & 0.95 & Discount factor \\
ALPHA ($\alpha$) & 0.1 & Learning rate \\
$\epsilon$ (EPSILON) & 0.5 & Initial exploration rate \\
EPS\_MIN & 0.1 & Minimum exploration rate \\
KAPPA ($\kappa$) & 0.0001 & Dyna-Q+ bonus \\
\hline
\end{tabular}
\label{tab:config}
\end{center}
\end{table}

\textbf{Parameter Selection}: The system uses a \textit{Linear Epsilon Decay} to shift from exploratory to exploitative behavior over 5,000 episodes. We limited planning steps ($K=5$) to prevent the agents from overfitting to old hazard positions in the rapidly shifting Tornado and Fire scenarios. Fixed environmental constants (exits and wall layouts) were used to provide a stable baseline for comparing the three algorithm variants.
% Redundancy removed - data moved to Table II
\begin{figure}[htbp]
\centerline{\fbox{\includegraphics[width=0.9\linewidth]{Example.png}}}
\caption{Snapshot of the simulation environment showing agents (colored figures) navigating towards the green exit zone while avoiding the blue spreading flood hazard. Grey blocks represent static obstacles.}
\label{fig:env}
\end{figure}
\section{Evaluation \& Results}
\subsection{Experimental Setup}
We evaluated the system on a $20 \times 20$ grid with 15 agents over 5,000 training episodes per algorithm/scenario combination. The evaluation metric is the \textbf{Survival Rate}, defined as the percentage of agents that successfully reach the exit before being caught by hazards or timing out.

Initial experiments showed that agents' survival rates declined during training without intervention. Analysis identified two factors:
\begin{enumerate}
    \item \textbf{Sparse Rewards:} On 400-cell grids, the probability of random path selection hitting an exit is negligible. We solved this via "Reward Shaping," providing high-density distance gradients ($+2$ for closer, $-1.5$ for farther).
    \item \textbf{Non-Stationarity:} Hazard propagation makes states that were "safe" at $T=0$ lethal at $T=50$. Our Dyna-Q+ implementation addresses this by encouraging exploration of unvisited states. This highlights the \textit{Curiosity Paradox}: while exploration is mathematically required for adaptation, it increases lethal risk during the learning phase.
\end{enumerate}

\begin{figure*}[htbp]
\centerline{\includegraphics[width=0.85\textwidth]{../outputs/benchmark/benchmark_all_scenarios.png}}
\caption{Consolidated Learning Curves across 5,000 episodes. The plots illustrate survival rate dynamics for Q-Learning, Dyna-Q, and Dyna-Q+. Deterministic scenarios show stable convergence, while stochastic environments reveal the adaptability of model-based planning.}
\label{fig:all_benchmarks}
\end{figure*}

We benchmarked the Random Walk baseline against Q-Learning, Dyna-Q, and Dyna-Q+ across all disaster scenarios. The Random Walk baseline represents a performance floor, with survival rates consistently falling below 5\% on the $20 \times 20$ grid. This outcome highlights the difficulty of the evacuation task in the absence of a learned heuristic. As detailed in Table \ref{tab:results}, the RL agents achieved convergence, with Q-Learning and Dyna-Q both reaching 100\% survival in the deterministic Flood scenario.

\begin{table}[htbp]
\caption{Final Survival Rates by Algorithm and Scenario (\%)}
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Scenario} & \textbf{Random} & \textbf{Q-Learning} & \textbf{Dyna-Q} & \textbf{Dyna-Q+} \\
\hline
Flood & 2.5\% & 100.0\% & 100.0\% & 100.0\% \\
Fire & 2.0\% & 99.5\% & 99.4\% & 86.6\% \\
Earthquake & 3.3\% & 80.0\% & 100.0\% & 93.3\% \\
Tornado & 2.9\% & 99.1\% & 66.1\% & 93.9\% \\
\hline
\end{tabular}
\label{tab:results}
\end{center}
\end{table}

\subsection{Evacuation Efficiency and Pathway Optimization}
Beyond the binary metric of survival, we analyzed the \textbf{Evacuation Efficiency} to determine if the agents simply survived or if they converged toward the theoretical optimal pathway (geodesic). Table \ref{tab:efficiency} presents the mean evacuation steps across all three algorithms.

\begin{table}[htbp]
\caption{Comparative Evacuation Efficiency (Mean Steps)}
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Scenario} & \textbf{Random} & \textbf{Q-Learning} & \textbf{Dyna-Q} & \textbf{Dyna-Q+} \\
\hline
Flood & 22.5 & 12.9 & 12.7 & 12.8 \\
Fire & 17.9 & 13.0 & 13.4 & 13.0 \\
Earthquake & 22.0 & 11.3 & 12.9 & 12.3 \\
Tornado & 23.6 & 12.9 & 12.7 & 12.6 \\
\hline
\end{tabular}
\label{tab:efficiency}
\end{center}
\end{table}

The data reveals that all algorithms achieved a mean step count between 12 and 14, with a global \textbf{Minimum Step Count of 4} recorded in every scenario. This indicates the agents successfully realized the Manhattan distance lower bound for certain starting positions, validating that the "Reward Shaping" gradient effectively guided the policy toward pathway minimization.

\subsection{Discussion: Algorithmic Trade-offs}
The results demonstrate a nuanced hierarchy of performance across the three algorithms. In deterministic scenarios like \textbf{Flood}, all RL agents converged to optimal Manhattan-distance pathways, outperforming the Random baseline by a factor of 40. This confirms that the reward shaping successfully mitigates the sparse reward problem.

In the \textbf{Earthquake} scenario, model-based \textbf{Dyna-Q} achieved a perfect 100\% survival rate, while model-free Q-Learning struggled at 80\%. This proves that the internal world model allowed the Dyna agent to propagate hazard information more effectively back from terminal states, creating a "topological defense" before the agent ever encountered the waves in the physical environment.

Conversely, in the \textbf{Fire} scenario, \textbf{Dyna-Q+} showed a lower survival rate (86.6\%) compared to standard Dyna-Q (99.4\%). This highlights the \textit{Curiosity Penalty} in dangerous environments: the exploration bonus ($\kappa\sqrt{\tau}$) occasionally incentivizes the agent to investigate high-risk states near the spreading hazard origin to reduce uncertainty. In disaster management, this suggests that "curiosity" must be strictly bounded to prevent self-destructive exploratory behavior in non-stationary lethal zones.

The \textbf{Tornado} scenario, being highly stochastic, favored Q-Learning and Dyna-Q+. The standard Dyna-Q model likely overfit to stale tornado paths, leading to a performance drop (66.1\%), whereas the reactive nature of Q-Learning and the exploration-rich policy of Dyna-Q+ maintained better resilience against the random walk of the vortex.

\begin{itemize}
    \item \textbf{Heuristic Predetermination:} The dense reward shaping provides a strong initial gradient, effectively "pre-wiring" the agents for evacuation and allowing even simple Q-Learning to reach near-perfect survival in certain high-entropy states.
    \item \textbf{Mental Modeling:} Dyna-Q's ability to plan in the background is required for deterministic propagation like Earthquake waves, where future state-action pairs can be reliably predicted from the model.
\end{itemize}
\section{Conclusions}
This project examines the behavior of the Dyna-Q algorithm family in disaster evacuation scenarios. Observations from the grid simulations identify three effects:
\begin{enumerate}
    \item \textbf{Integration of Reward Shaping and Planning}: Model-based planning (Dyna) enables hazard avoidance through simulated transitions, while high-density reward shaping facilitates initial policy convergence in large $20 \times 20$ state spaces.
    \item \textbf{Efficiency and Adaptability}: Model-free Q-Learning identifies optimal paths in stable environments, whereas Dyna-family algorithms maintain higher survival rates in stochastic hazards such as fire or earthquake waves.
    \item \textbf{Infrastructure Influence}: The "Topological Redundancy" provided by the Triple-Exit strategy shows a greater effect on survival than algorithmic variation alone. This suggests that disaster management involves both infrastructure design and routing policy.
\end{enumerate}
In summary, the Dyna-Q+ implementation delivers survival gains consistent with autonomous evacuation requirements, particularly in stochastic environments where exploration is required. By utilizing the planning phase to propagate state information before hazard contact, the system reduces the performance gap between reactive escape and proactive risk avoidance.
\subsection{Future Research: Towards Deep RL}
The current tabular approach is limited by the "Curse of Dimensionality" for larger grids. Future work will investigate the use of \textbf{Deep Q-Networks (DQN)} with Convolutional Neural Networks (CNNs) to process the grid as an image \cite{b3}. This would allow the agents to perceive complex hazard shapes as visual features, potentially improving survival in stochastic non-stationary environments like Tornadoes without manual feature engineering.
\begin{thebibliography}{00}
\bibitem{b1} D. Helbing, I. Farkas, and T. Vicsek, "Simulating dynamical features of escape panic," \textit{Nature}, vol. 407, pp. 487--490, 2000.
\bibitem{b2} R. S. Sutton, "Dyna, an integrated architecture for learning, planning, and reacting," \textit{ACM SIGART Bulletin}, vol. 2, no. 4, pp. 160--163, 1991.
\bibitem{b3} V. Mnih et al., "Human-level control through deep reinforcement learning," \textit{Nature}, vol. 518, pp. 529--533, 2015.
\bibitem{b4} P. E. Hart, N. J. Nilsson, and B. Raphael, "A Formal Basis for the Heuristic Determination of Minimum Cost Paths," \textit{IEEE Transactions on Systems Science and Cybernetics}, vol. 4, no. 2, pp. 100--107, 1968.
\end{thebibliography}
\end{document}