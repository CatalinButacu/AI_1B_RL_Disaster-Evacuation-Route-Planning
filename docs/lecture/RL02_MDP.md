# RL02_MDP

**Source:** c:\Users\catalin.butacu\Downloads\RL\resourses\lecture\RL02_MDP.pdf

**Pages:** 64

---


## Page 1

Reinforcement Learning
2. Markov Decision Processes 
Florin Leon
â€œGheorghe Asachiâ€ Technical University of IaÈ™i, Romania
Faculty of Automatic Control and Computer Engineering
https://florinleon.byethost24.com/lect_rl.html
2025


## Page 2

Markov Decision Processes 
1. Formalization
2. Value Iteration
3. Policy Iteration
2


## Page 3

Markov Decision Processes 
1. Formalization
2. Value Iteration
3. Policy Iteration
3


## Page 4

Markov Decision Processes
ï®Markov Decision Processes (MDPs) are a formal framework for 
sequential decision making
ï®Unlike bandit problems, actions affect both immediate and future 
rewards by influencing the next state
ï®The goal is to learn a series of actions that maximize cumulative 
reward over time
ï®Actions influence not just immediate rewards, but also subsequent 
states, and through those future rewards 
ï®MDPs involve delayed reward and the need to trade off immediate 
and delayed reward
4


## Page 5

The Agent-Environment Interaction Loop
ï®The agent selects an action
ï®The environment presents a new state and a reward
ï®This loop continues until a terminal state is reached
ï®This results in a trajectory of the form: S0, A0, R1, S1, A1, R2, S2, A2, R3, ...
5


## Page 6

Key Components of an RL Problem
ï®Agent: the decision maker that takes actions
ï®Environment: the system with which the agent interacts
ï®State: a representation of the environment at a given time
ï®Action: a choice available to the agent at each state
ï®Reward: a numerical value that indicates the desirability of 
taking an action in a state; it guides the agentâ€™s learning
6


## Page 7

Agent-Environment Boundary
ï®
The boundary between agent and environment is not always the physical 
boundary (e.g., of a robot)
ï®
Mechanical components and sensors of a robot are typically considered part 
of the environment rather than the agent
ï®
Rewards are computed â€œinsideâ€ the agent but are still considered part of the 
environment because they define the task and cannot be arbitrarily changed 
by the agent
ï®
The agent controls its actions but cannot arbitrarily modify its environment
ï®
The agent-environment boundary represents the limit of the agentâ€™s control, 
not of its knowledge
ï®
The agent may have knowledge about how rewards are computed or how the 
environment works, but this does not change the fundamental distinction
ï®
Even with full knowledge of the environment, a task can still be challenging, like 
solving a Rubikâ€™s cube despite knowing all the rules
7


## Page 8

Formalization
ï®An MDP is defined by: 
ï®A finite set of states S
ï®A finite set of actions A available in each state
ï®A transition probability function p
ï®A reward function r
ï®A discount factor Î³ âˆŠ [0, 1]  (explained later)
ï®The solution of this problem is a policy Ï€
ï®E.g., Ï€(s) is the action that should be taken in state s
8


## Page 9

Function Definitions
ï®
The functions for transition probability, reward, and policy can be defined in 
different ways, depending on the problem
ï®
Starting from the general form of the transition probability function (the 
first in the next slide), one can obtain and use different forms for transitions 
and rewards by marginalization (eliminating some variable from the joint 
probability distribution by summing them out) 
ï®
The equations in the next slide look complex, but in fact they only show that 
we have the flexibility to define, e.g., the reward function as r(s, a, s'), 
r(s, a), or r(s'), as we see fit depending on the problem, and different 
representations can be converted using the general transition probabilities
9


## Page 10

Possible Function Definitions
10


## Page 11

Reward Hypothesis
ï®All of what we mean by goals and purposes can be understood 
as maximizing the expected value of the cumulative sum of a 
received scalar signal, called the reward
ï®Any goal can be reduced to preferences over outcomes, and 
preferences can be ranked numerically
ï®Numbers allow consistent comparison and accumulation over time, 
and make complex objectives manageable
ï®The reward function defines what is desirable, while specific RL 
algorithms focus on how to achieve it
11


## Page 12

Designing Rewards
ï®Designing an effective reward function is critical for agent learning
ï®Bad reward signals lead to undesired behaviors, for example, a chess 
agent should be rewarded for winning, not capturing pieces
ï®Rewards can be sparse (only at goal) or dense (frequent feedback)
ï®Sparse: a chess agent gets a positive reward for winning and a negative 
reward for losing
ï®Dense: a navigation agent gets a small negative reward for each step it 
takes, encouraging it to reach the goal quickly
12


## Page 13

The Markov Property
ï®
The future is independent of the past given the present 
ï®
A process is Markovian if the next state depends only on the current state
ï®
The state captures all relevant information from the history; once the state 
is known, the history may be ignored
ï®
E.g., in a chess game only the current board position matters, not how it 
was reached
ï®
The Markov assumption allows efficient dynamic programming solutions
ï®
Most real scenarios are unlikely to be Markov, but we usually can transform 
or approximate this property, e.g., by introducing sensors for the relevant 
features of the environment and taking actions based on these current 
observations
13
p(St+1 | St ) = p(St+1 | S1 , â€¦, St )


## Page 14

Example: Bioreactor Optimization
ï®Reinforcement learning controls temperature and stirring rates
ï®Actions: target temperatures and stirring intensities
ï®States: sensor readings on chemical production levels
ï®Rewards: positive for optimal production rate, negative for 
system failure
14


## Page 15

Example: Pick-and-Place Robot
ï®Robot picks and places objects in an assembly line
ï®Actions: motor torque commands for arm movement
ï®States: joint angles, velocities, and object positions
ï®Rewards: positive for a successful placement, negative for jerky 
motion
15


## Page 16

Example: Self-Driving Car Navigation
ï®Autonomous vehicles use MDPs to optimize driving decisions
ï®Actions: speed control, lane changes, braking decisions
ï®States: traffic conditions, GPS location, speed limits
ï®Rewards: positive for safe travel and reaching destination, 
negative for delays, large negative for near-collisions
16


## Page 17

Example: Recycling Robot
ï®A mobile robot collects empty soda cans in an office environment
ï®It has sensors to detect cans, an arm and gripper to pick them up, 
and operates on a rechargeable battery
ï®The control system handles sensory input, navigation, and arm 
control
ï®States: battery level (high or low) â‡’S = {high, low} 
ï®Actions: search, wait, or return to recharge â‡’
A(high) = {search, wait}, A(low) = {search, wait, recharge}
17


## Page 18

Example: Recycling Robot
ï®
Rewards: +1 for each can collected, â€“3 if battery depletes
ï®
Transition probabilities: 
ï®
A period of searching that begins with a high energy level leaves the energy level 
high with probability Î± and reduces it to low with probability 1 â€“ Î±
ï®
A period of searching undertaken when the energy level is low leaves it low with 
probability Î² and depletes the battery with probability 1 â€“ Î²


## Page 19

Episodic and Continuing Tasks
ï®An episodic task is one where the agent-environment interaction is 
broken into discrete episodes. Each episode starts from an initial 
state and ends in a terminal state after a finite number of steps
ï®Examples: games, maze solving, car parking
ï®A continuing task has no terminal state. The interaction 
continues indefinitely
ï®Examples: industrial control, stock trading, pole balancing (it can fall, but 
ideally, it could be kept in equilibrium forever)
19


## Page 20

The Return Function
ï®The return is the total reward from time step t onward: 
where T is a final time step 
ï®The agent must maximize the expected return
ï®For continuing tasks, T = âˆ 
20
1
2
3
t
t
t
t
T
G
R
R
R
R
+
+
+
=
+
+
+â€¦+


## Page 21

Discounted Rewards
ï®In continuing tasks, rewards can be discounted over time to avoid 
infinite sums
ï®The discounted return is: 
ï®Î³ (0 â‰¤ Î³ â‰¤ 1) is the discount factor
ï®Î³ = 0 â‡’Short-sighted agent (only considers immediate rewards)
ï®Î³ â‰ˆ 1 â‡’Far-sighted agent (values future rewards more strongly)
21


## Page 22

Finite Returns in Continuing Tasks
ï®Suppose that all rewards are bounded
ï®In this case, returns are finite, and the algorithms can compare them 
to determine the optimal expected value
22
t
max
R
R
t
â‰¤
âˆ€
1
max
max
0
0
0
k
k
k
t
t k
k
k
k
G
R
R
R
Î³
Î³
Î³
âˆ
âˆ
âˆ
+ +
=
=
=
â‰¤
â‰¤
=
ïƒ¥
ïƒ¥
ïƒ¥
0
1
[0,1)
1
k
k
if
Î³
Î³
Î³
âˆ
=
=
âˆˆ
âˆ’
ïƒ¥
max
1
t
R
G
Î³
ïƒ
â‰¤
âˆ’
Î³ â‰ 1


## Page 23

Unified Notation for Episodic and 
Continuing Tasks
ï®The unified notation treats episodic tasks as entering an absorbing 
state, which transitions only to itself and generates only zero rewards
ï®This convention allows both episodic and continuing tasks to be 
described using a single mathematical framework
ï®In episodic tasks, Î³ can be 1 (but it can be < 1, too)
ï®In continuing tasks, T can be considered âˆ (but from the practical 
point of view, it will be in fact finite)
23


## Page 24

Policy Functions
ï®A policy is the agentâ€™s decision-making rule. It defines how the 
agent chooses actions based on the current state to maximize 
expected return
ï®Ï€(s) denotes a deterministic policy
ï®
ï®It gives the action that the agent will take in state s
ï®Ï€(aâˆ£s) denotes a stochastic policy
ï®
ï®It gives the probability of taking action a in state s
24
( ) :
s
Ï€
â†’
S
A
(
) :
[0,1]
(
)
1
a
a s
with
a s
s
Ï€
Ï€
âˆˆ
Ã—
â†’
=
âˆ€âˆˆ
ïƒ¥
A
S
A
S
âˆ£
âˆ£


## Page 25

Value Function or State-Value Function
ï®The value function or state-value function of a state s under a 
policy Ï€ is the expected return when starting in s and following 
Ï€ thereafter 
25


## Page 26

Quality Function or Action-Value Function 
ï®The quality function or action-value function of taking action a 
in state s under a policy Ï€ is the expected return starting from 
s, taking the action a, and thereafter following policy Ï€
ï®From the practical point of view, agents (RL algorithms) can choose 
actions by computing argmaxa q(s, a)
ï®In contrast, v(s) gives no information about which action to take. 
Especially in non-deterministic environments (but not only), 
â€œwanting to reachâ€ a state is not enough. The agent must still know 
which actions can lead to the next states
26


## Page 27

Bellman Equation
ï®The expectation term is complicated to assess (assume t = 0)
ï®The Bellman equation expresses vÏ€(s) as a recursive relationship 
which is easier to compute
ï®It decomposes long-term value into local steps, which forms the basis 
for learning algorithms
ï®In the fully deterministic case (both policy and environment)
27
0
0
1
0
0
( )
[
]
k
k
k
v
s
G
S
s
R
S
s
Ï€
Ï€
Ï€
Î³
âˆ
+
=
ïƒ©
ïƒ¹
=
=
=
=
ïƒª
ïƒº
ïƒ«
ïƒ»
ïƒ¥
E
E
âˆ£
âˆ£
[
]
( )
(
)
(
, )
( )
( )
a
s
v
s
a s
p s
s a
r s
v
s
Ï€
Ï€
Ï€
Î³
â€²
â€²
â€²
â€²
=
+
ïƒ¥
ïƒ¥
âˆ£
âˆ£
( )
( )
( )
v
s
r s
v
s
Ï€
Ï€
Î³
â€²
â€²
=
+


## Page 28

Bellman Equation
ï®Similar expressions can be deduced for the action-value 
function
ï®Return-based definition (t = 0)
ï®General recursive form (stochastic policy and environment)
ï®Deterministic case (policy and environment)
28
[
]
0
0
0
1
0
0
0
( , )
,
,
k
k
k
q
s a
G
S
s A
a
R
S
s A
a
Ï€
Ï€
Ï€
Î³
âˆ
+
=
ïƒ©
ïƒ¹
=
=
=
=
=
=
ïƒª
ïƒº
ïƒ«
ïƒ»
ïƒ¥
âˆ£
âˆ£
E
E
( , )
(
, )
( )
(
)
( ,
)
s
a
q
s a
p s
s a
r s
a
s q
s a
Ï€
Ï€
Î³
Ï€
â€²
â€²
ïƒ©
ïƒ¹
â€²
â€²
â€²
â€²
â€²
â€²
=
+
ïƒª
ïƒº
ïƒ«
ïƒ»
ïƒ¥
ïƒ¥
âˆ£
âˆ£
( , )
( )
( , ( ))
q
s a
r s
q
s
s
Ï€
Ï€
Î³
Ï€
â€²
â€²
â€²
=
+


## Page 29

Optimal Policies and Value Functions 
ï®An optimal policy Ï€*maximizes the expected return from every 
state
ï®The optimal value function is: 
ï®Similarly, the optimal action-value function is:
ï®Relation between v* and q*:
29
*( )
max
( )
v s
v
s
Ï€
Ï€
=
*( , )
max
( , )
q s a
q
s a
Ï€
Ï€
=
*
*
( )
max
( , )
a
v s
q s a
=


## Page 30

Optimal Policies
ï®All optimal policies achieve the optimal value function:
âˆ— = âˆ—()
ï®The value of a state under an optimal policy is the highest expected 
reward achievable
ï®All optimal policies achieve the optimal action-value function:
âˆ—,  = âˆ—(, )
ï®Solving the Belmann equations (one for each state) provides a way to 
compute the optimal policy
ï®Still, it can be unfeasible in practice if the number of states is very 
large
ï®Approximation methods are used in this case, e.g., using (deep) 
neural networks
30


## Page 31

Markov Decision Processes 
1. Formalization
2. Value Iteration
3. Policy Iteration
31


## Page 32

Dynamic Programming
ï®Dynamic programming (DP) refers to a set of algorithms for 
computing optimal policies given a perfect, known MDP model
ï®DP provides a theoretical foundation for many RL methods, which 
aim to achieve similar results with less computation and without a 
perfect model
ï®There are two important DP algorithms for solving an MDP:
ï®Value iteration
ï®Policy iteration
32


## Page 33

Value Iteration Algorithm Outline
ï®It is an algorithm for computing the optimal policy Ï€*
ï®The value of each state V(s) is initialized to 0
ï®V(s) approximates v*(s)
ï®State values are iteratively updated
ï®The state values are used to select the optimal action for each 
state
ï®The state with the highest value is chosen
33


## Page 34

Solving an MDP
ï®There are n states
ï®There is one Bellman equation for each state
ï®
â‡’n equations with n unknowns: V(s)
ï®It cannot be solved as a system of linear equations due to the max
operator
ï®Therefore, it is solved iteratively (k is the solving iteration)
ï®For each state s:
34


## Page 35

35


## Page 36

Synchronous vs. Asynchronous VI
ï®Synchronous updates: compute all the new values of V(s) from all the 
old values of V(s), then update V(s) with the new values
ï®Asynchronous updates: compute and update V(s) for each state one 
at a time 
ï®The previous VI pseudocode is an asynchronous in-place variant
ï®There is no temporary copy of the V array
ï®Once a V(s) is changed, it is used in the other updates
ï®The asynchronous version uses less memory and usually converges 
faster
ï®The synchronous version is easier to parallelize
36


## Page 37

Example: Simple Mars Rover
ï®The environment models a Mars rover navigating a linear grid with 
six states, indexed from 0 to 5
ï®The rover can perform two actions at each step:
ï®Move, which advances the rover by one cell with probability 90%, or stays in 
place with probability 10%
ï®Speed, which advances the rover two cells forward with probability 80%, 
one cell forward with probability 10%, or stays in place with probability 10%
ï®Rewards are assigned as follows: 
ï®Reaching the goal state (state 4) yields +100 and terminates the episode
ï®Overshooting the goal (state 5) yields â€“100 and terminates
ï®Landing on an obstacle (state 2) incurs a penalty of â€“20
ï®All other transitions cost â€“1
37


## Page 39

Example: Simple Mars Rover
ï®SimpleMarsEnv.py  (Gymnasium environment)
ï®1 SimpleMarsRoverAgent_HP.py (with hardcoded policy)
39
initial 
state
obstacle (â€“20)
agent
goal state 
(+100)
unsafe 
termination 
(â€“100)


## Page 40

Simple Mars Rover with VI
ï®SimpleMarsEnv.py  (the same environment)
ï®2 SimpleMarsRoverAgent_VI.py (an agent implementing the 
value iteration algorithm)
40


## Page 41

VI Application Example
ï®V0(s) = 0 for all states (S0 to S5)
ï®Î³ = 1 (no discounting; the environment is episodic, so that is ok)
41
move
speed
The value of terminal states is always 0 
because there is no reward from there onward


## Page 42

VI Application Example
42
move
speed
Action move: p1 = 0.9 to go 1 step to S4 (+100), p2 = 0.1 to remain in S3 (â€“1)
Action speed: p1 = 0.8 to go 2 steps to S5 (â€“100), p2 = 0.1 to go 1 step to S4 (+100),
p3 = 0.1 to remain in S3 (â€“1)
S3
S4
S5


## Page 43

Iteration 1, S0
43
Evaluate Q(s=0, a=0):
p = 0.9, R(1) = -1, V[1] = 0.00  =>  0.9 * (-1 + 1.0 * 0.00) = -0.90
p = 0.1, R(0) = -1, V[0] = 0.00  =>  0.1 * (-1 + 1.0 * 0.00) = -0.10
Q(s=0, a=0) = -1.00
Evaluate Q(s=0, a=1):
p = 0.8, R(2) = -20, V[2] = 0.00  =>  0.8 * (-20 + 1.0 * 0.00) = -16.00
p = 0.1, R(1) = -1, V[1] = 0.00  =>  0.1 * (-1 + 1.0 * 0.00) = -0.10
p = 0.1, R(0) = -1, V[0] = 0.00  =>  0.1 * (-1 + 1.0 * 0.00) = -0.10
Q(s=0, a=1) = -16.20
Update V[s=0] = max Q = -1.00


## Page 44

Iteration 1, S3
44
Evaluate Q(s=3, a=0):
p = 0.9, R(4) = 100, V[4] = 0.00  =>  0.9 * (100 + 1.0 * 0.00) = 90.00
p = 0.1, R(3) = -1, V[3] = 0.00  =>  0.1 * (-1 + 1.0 * 0.00) = -0.10
Q(s=3, a=0) = 89.90
Evaluate Q(s=3, a=1):
p = 0.8, R(5) = -100, V[5] = 0.00  =>  0.8 * (-100 + 1.0 * 0.00) = -80.00
p = 0.1, R(4) = 100, V[4] = 0.00  =>  0.1 * (100 + 1.0 * 0.00) = 10.00
p = 0.1, R(3) = -1, V[3] = 0.00  =>  0.1 * (-1 + 1.0 * 0.00) = -0.10
Q(s=3, a=1) = -70.10
Update V[s=3] = max Q = 89.90


## Page 45

Iteration 10, S3
45
Evaluate Q(s=3, a=0):
p = 0.9, R(4) = 100, V[4] = 0.00  =>  0.9 * (100 + 1.0 * 0.00) = 90.00
p = 0.1, R(3) = -1, V[3] = 99.89  =>  0.1 * (-1 + 1.0 * 99.89) = 9.89
Q(s=3, a=0) = 99.89
Evaluate Q(s=3, a=1):
p = 0.8, R(5) = -100, V[5] = 0.00  =>  0.8 * (-100 + 1.0 * 0.00) = -80.00
p = 0.1, R(4) = 100, V[4] = 0.00  =>  0.1 * (100 + 1.0 * 0.00) = 10.00
p = 0.1, R(3) = -1, V[3] = 99.89  =>  0.1 * (-1 + 1.0 * 99.89) = 9.89
Q(s=3, a=1) = -60.11
Update V[s=3] = max Q = 99.89


## Page 46

Results
46
Optimal Value Function:
State 0: V = 95.31
State 1: V = 96.42
State 2: V = 97.65
State 3: V = 99.89
State 4: V = 0.00
State 5: V = 0.00
Optimal Policy (0: move, 1: speed):
State 0: 0
State 1: 1
State 2: 1
State 3: 0
State 4: None
State 5: None


## Page 47

Markov Decision Processes 
1. Formalization
2. Value Iteration
3. Policy Iteration
47


## Page 48

Policy Iteration
ï®The policy vector Ï€ is initialized randomly and modified only 
when necessary
ï®The algorithm alternates between two steps: 
ï®Policy evaluation: computes the values of all states given policy Ï€i 
ï®Policy improvement: computes a new policy Ï€i+1 based on state values Vi
48


## Page 49

Policy Evaluation
ï®The policy is arbitrarily initialized
ï®Unlike in case of value iteration, where max is used
ï®Here we know the action given by the policy (the policy may be bad 
at first, but it is known)
ï®So we have a linear system of Bellman equations, one for every state
49


## Page 50

Policy Evaluation
ï®This system of n equations with n unknowns can be solved 
algebraically or iteratively
ï®The iterative form is more efficient for large MDPs
50


## Page 51

51


## Page 52

Policy Improvement Theorem 
ï®The policy improvement theorem says that if we evaluate the current 
policy and then act greedily (i.e., choose actions that maximize the 
expected return under the current value estimates), the new policy 
will be at least as good
ï®If we have a current policy Ï€ and a new policy Ï€' such that for all 
states s
ï®then the new policy Ï€' is at least as good as the old one
ï®If the inequality is strict for at least one state, then Ï€â€² is strictly better 
than Ï€
ï®Iterating this improvement process leads to the optimal policy
52
( ,
( ))
( )
q
s
s
v
s
Ï€
Ï€
Ï€ â€²
â‰¥
( )
( )
v
s
v
s
s
Ï€
Ï€
â€²
â‰¥
âˆ€


## Page 53

Policy Improvement Step
ï®The goal is to improve the current policy Ï€ using the value function 
VÏ€ obtained from the policy evaluation step
ï®For each state s, we identify the action that maximizes expected 
return (the â€œoptimal so farâ€ action):
ï®If âˆ—() â‰ (), the policy is updated: 
  â†âˆ—
ï®
This two steps (policy evaluation and policy improvement) are repeated 
until the policy no longer changes, meaning that the optimal policy has been 
found
53
*
,
( )
argmax
( ,
, )
( )
a
s r
a
s
p s r s a
r
V
s
Ï€
Î³
â€²
ïƒ©
ïƒ¹
â€²
â€²
=
+
ïƒ«
ïƒ»
ïƒ¥
âˆ£


## Page 55

Simple Mars Rover with PI
ï®SimpleMarsEnv.py  (the same environment)
ï®3 SimpleMarsRoverAgent_PIi.py (an agent implementing the 
policy iteration algorithm with iterative policy evaluation)
ï®4 SimpleMarsRoverAgent_PIs.py (policy evaluation with exact 
solving of the linear equation system)
55


## Page 56

Iteration 1
56
assume pi(all) = 0
but this can be arbitrary
Current policy:
Ï€(0) = 0
Ï€(1) = 0
Ï€(2) = 0
Ï€(3) = 0
Ï€(4) = None
Ï€(5) = None
Policy evaluation:
State values VÏ€:
V[0] = 75.44
V[1] = 76.56
V[2] = 96.67
V[3] = 99.89
V[4] = 0.00
V[5] = 0.00
VI after I1
V[0] = -1.00
V[1] = -2.90
V[2] = 77.90
V[3] = 89.90


## Page 57

Iteration 1
57
Policy improvement:
Q(s=1, a=0):
p=0.9, R(2)=-20, V[2]=96.67  =>  69.00
p=0.1, R(1)= -1, V[1]=76.56  =>  7.56
Q = 76.56
Q(s=1, a=1):
p=0.8, R(3)= -1, V[3]=99.89  =>  79.11
p=0.1, R(2)=-20, V[2]=96.67  =>  7.67
p=0.1, R(1)= -1, V[1]=76.56  =>  7.56
Q = 94.33
Best action for state 1 = 1 (changed)
Current policy:
Ï€(0) = 0
Ï€(1) = 0
Ï€(2) = 0
Ï€(3) = 0
Ï€(4) = None
Ï€(5) = None


## Page 58

Iteration 3
58
Current policy:
Ï€(0) = 0
Ï€(1) = 1
Ï€(2) = 1
Ï€(3) = 0
Ï€(4) = None
Ï€(5) = None
State values VÏ€:
V[0] = 95.31
V[1] = 96.42
V[2] = 97.65
V[3] = 99.89
V[4] = 0.00
V[5] = 0.00
Policy improvement:
Q(s=0, a=0):
p=0.9, R(1)= -1, V[1]=96.42  =>  85.88
p=0.1, R(0)= -1, V[0]=95.31  =>  9.43
Q = 95.31
Q(s=0, a=1):
p=0.8, R(2)=-20, V[2]=97.65  =>  62.12
p=0.1, R(1)= -1, V[1]=96.42  =>  9.54
p=0.1, R(0)= -1, V[0]=95.31  =>  9.43
Q = 81.10
Best action for state 0 = 0
Q(s=3, a=0):
p=0.9, R(4)=100, V[4]=0.00  =>  90.00
p=0.1, R(3)= -1, V[3]=99.89  =>  9.89
Q = 99.89
Q(s=3, a=1):
p=0.8, R(5)=-100, V[5]=0.00  =>  -80.00
p=0.1, R(4)=100, V[4]=0.00  =>  10.00
p=0.1, R(3)= -1, V[3]=99.89  =>  9.89
Q = -60.11
Best action for state 3 = 0


## Page 59

Policy Iteration vs. Value Iteration
ï®Value iteration
ï®Updates values directly using Bellman optimality equations
ï®It may need more iterations to converge, but each iteration is simple
ï®Policy iteration
ï®Alternates between policy evaluation and policy improvement
ï®Converges in fewer iterations but requires solving a set of equations
ï®This can be expensive in large state spaces, but it is very fast and accurate in 
small ones
ï®Value iteration scales better for large MDPs
ï®Policy iteration is better for small MDPs
59


## Page 60

Generalized Policy Iteration
ï®In GPI, the evaluation and improvement steps 
are interleaved
ï®This is a general framework, not a specific 
algorithm
ï®The granularity of the updates is not specified
ï®The policy is continually improved with respect 
to the value function, while the value function 
is continually updated toward the value of the 
current policy
60


## Page 61

Generalized Policy Iteration
ï®This can involve a single or a few evaluation operations
ï®And a single or a few improvement operations
ï®GPI allows approximate or partial updates, which reduce 
computation
ï®It allows to trade off accuracy and speed
ï®GPI is a â€œconceptual umbrellaâ€ that unifies many RL algorithms
61


## Page 62

Generalized Policy Iteration
ï®
The evaluation and improvement are both competing and cooperating
ï®
When the policy is improved based on the current value estimates, those value 
estimates become outdated
ï®
When the value function is updated to match the current policy, it may reveal that 
some actions are better than what the policy currently chooses
ï®
But the continuous updates of Ï€ and V lead in the long run to a single joint solution: 
the optimal value function and an optimal policy 
62


## Page 63

Conclusions
ï®MDPs formalize sequential decisions with states, actions, 
transitions, rewards, and discounting
ï®The Markov property means the next state depends only on the 
current state, not the full history
ï®Value iteration finds optimal policies through iterative Bellman 
updates; it is scalable to large MDPs
ï®Policy iteration alternates between evaluation and 
improvement and needs fewer iterations but costs more per 
step
63


## Page 64

Main Reference
ï®Sutton, R.S. and Barto, A.G. (2018). Reinforcement Learning: An 
Introduction. 2nd edition. MIT Press, Cambridge, MA. 
http://incompleteideas.net/book/the-book-2nd.html
64
