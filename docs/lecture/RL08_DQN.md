# RL08_DQN

**Source:** c:\Users\catalin.butacu\Downloads\RL\resourses\lecture\RL08_DQN.pdf

**Pages:** 76

---


## Page 1

Reinforcement Learning 
8. Deep Q-Networks 
 
Florin Leon 
 
â€œGheorghe Asachiâ€ Technical University of IaÈ™i, Romania 
Faculty of Automatic Control and Computer Engineering 
 
https://florinleon.byethost24.com/lect_rl.html 
 
2025 
 


## Page 2

Deep Q-Networks 
1. Standard DQN 
2. Double DQN 
3. Dueling DQN 
4. Rainbow DQN 
2 


## Page 3

Short Timeline 
ï®Standard DQN: 2015 
ï®Double DQN: 2016 
ï®Prioritized Experience Replay: 2016 
ï®Dueling DQN: 2016 
ï®Distributional Q-learning (C51): 2017 
ï®Noisy Networks: 2018 
ï®Rainbow DQN: 2018 


## Page 4

Deep Q-Networks 
1. Standard DQN 
2. Double DQN 
3. Dueling DQN 
4. Rainbow DQN 
4 


## Page 5

NN Approximation of Q Function 
ï®Tabular Q-learning stores one value per state-action pair in a table 
ï®It works for small, discrete state spaces with limited observations 
ï®Rich sensory inputs (images, large vectors, text) create enormous, 
near-continuous state spaces 
ï®We can represent the Q function as ğ‘„ğ‘ , ğ‘; ğœƒ with a neural network 
with parameters ğœƒ 
ï®Input: state s; output: Q-values 
ï®The policy still selects the action with the maximum predicted Q-value 
ï®Deep networks can process raw inputs, form hierarchical features, 
and output action values in one model 
ï®Hence the name Deep Q-Network (DQN) (Google DeepMind, 2015) 
5 


## Page 6

Atari 2600 Games 
ï®49 games: Pong, Breakout, Space Invaders, Seaquest, Beam 
Rider, etc. 
 
 
 
 
ï®DQN was intended as a step toward artificial general 
intelligence, because it uses the same algorithm for quite 
different games 
6 


## Page 7

Input Preprocessing 
ï®A human player sees 210 x 160 images with 128 RGB colors 
ï®For DQN, each frame is converted into an 84 x 84 matrix of 
luminance values 
ï®L = 0.2126 Â· R + 0.7152 Â· G + 0.0722 Â· B 
7 


## Page 8

Game Model 
ï®Inputs: 4 consecutive game frames, so the agent can estimate the 
speed and direction of characters or objects in the game 
ï®Outputs: the Q-values of all possible actions 
ï®The number of actions depends on the game and can range from 2 to 
18 (matching joystick commands), e.g., up, down, right, left, fire, 
accelerate, brake, pick up the key, open the door, etc. 
ï®The network interacts with an Atari game emulator 
ï®Convolutional neural networks (CNN) are used 
ï®The environment provides feedback only through images and score 
8 


## Page 9

Q Function Approximation 
Direct approach 
Optimized approach (in DQN): the outputs are 
the Q values of all possible actions in the current 
state. They are all computed in a single step.  
For each game, the number of outputs equals 
the number of valid actions 


## Page 10

Q-Learning Updates 
ï®Tabular Q-Learning update: 
  
 
ï®Consider a transition ğ‘ ğ‘¡, ğ‘ğ‘¡, ğ‘Ÿğ‘¡+1, ğ‘ ğ‘¡+1  
ï®The one-step TD target in naive deep Q-learning would be: 
  
ğ‘¦ğ‘¡= ğ‘Ÿğ‘¡+1 + ğ›¾max
ğ‘â€² ğ‘„ğœƒğ‘ ğ‘¡+1, ğ‘â€²  
ï®The network predicts ğ‘„ğœƒğ‘ ğ‘¡, ğ‘ğ‘¡ 
ï®The squared TD error for this sample is: 
  
ğ¿ğ‘¡ğœƒ= ğ‘¦ğ‘¡âˆ’ğ‘„ğœƒğ‘ ğ‘¡, ğ‘ğ‘¡
2 
ï®This looks similar to supervised learning, but the target ğ‘¦ğ‘¡ depends 
on the current network 
10 


## Page 11

Moving Target Problem 
ï®After each gradient step, the network changes and therefore the 
target changes as well 
ï®Distorted Q-values can enter the targets and then propagate or even 
amplify over time 
ï®The learning process follows a target that keeps moving as the 
network updates 
 
iterations 
11 


## Page 12

Training Instability 
ï®Problems: 
ï®The target values are not fixed 
ï®Successive experiences are correlated and depend on the policy 
ï®Small changes to the parameters cause large changes in the 
policy, which lead to large shifts in the data distribution 
ï®Solutions: 
ï®Fixed target Q-network 
ï®Experience replay 
ï®Clipped error 
12 


## Page 13

Target Network and Loss 
ï®DQN introduces 2 networks with the same architecture: 
ï®
The online network ğ‘„ğœƒğ‘ , ğ‘ interacts with the environment and is updated by 
gradient descent 
ï®
The target network ğ‘„ğœƒâˆ’ğ‘ , ğ‘ is a delayed copy used only to compute TD targets 
ï®For a transition ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²  the DQN target is: 
  
ğ‘¦= ğ‘Ÿ+ ğ›¾max
ğ‘â€² ğ‘„ğœƒâˆ’ğ‘ â€², ğ‘â€²  
ï®The loss for the online network is: 
  
ğ¿ğœƒ= ğ‘¦âˆ’ğ‘„ğœƒğ‘ , ğ‘
2 
  
or, usually, a minibatch average of this expression 
 
13 


## Page 14

Target Network as a Stabilizer 
ï®The target network stays fixed for a while, so the mapping from ğ‘„ğœƒ 
to targets y changes more slowly 
ï®This slower change breaks the tight positive feedback loop that 
causes runaway updates 
ï®The online network can take many gradient steps toward the current 
target network before the target changes 
ï®The design mimics a teacher-student scenario: 
ï®
The target network ğ‘„ğœƒâˆ’ acts as a teacher that provides temporary values 
ï®
The online network ğ‘„ğœƒ acts as a student that tries to match those values 
ï®After some training, the student parameters replace the teacher 
parameters 
14 


## Page 15

Hard and Soft Target Updates 
ï®Hard target updates copy the online network parameters every C 
steps: 
ğœƒâˆ’â†ğœƒ    every  ğ¶ updates 
ï®Between the hard updates, the target network stays fixed 
ï®Soft target updates use an exponential moving average: 
  
  
ğœƒâˆ’â†ğœğœƒ+ 1 âˆ’ğœğœƒâˆ’   with a small ğœâˆˆ0,1  
ï®Hard updates give clear separation between the student and the 
teacher 
ï®Soft updates produce a smoother evolution of the target network and 
allow a fine control via ğœ 
ï®In both cases the target network changes more slowly than the online 
network, which stabilizes learning 
 
15 


## Page 16

Target Update Time Scale 
ï®Very frequent target updates remove the stabilizing separation from 
the online network 
ï®Very rare updates create stale teachers that ignore the latest 
understanding 
ï®Update period or ğœ controls how quickly the target tracks 
improvements 
ï®Appropriate lag prevents immediate reuse of each fresh estimate as 
its own target 
ï®This delay reduces the chance of runaway bootstrapping feedback 
16 


## Page 17

Experience Replay 
ï®The agent can update parameters only once per environment step if 
it uses strictly online updates 
ï®Each transition is used for a single gradient step and then discarded 
ï®Valuable or rare experiences cannot influence learning for very long 
if they are not replayed 
ï®Consecutive transitions may be strongly correlated  
ï®If the best action is usually â€œgo rightâ€, the training data will be dominated by 
â€œgo rightâ€ 
ï®Sudden changes in the policy or environment can drive the network 
into poor regions of parameter space 
17 


## Page 18

Replay Memory / Buffer 
ï®During gameplay, all transitions (s, a, r, s') are stored in a structure 
called replay memory or replay buffer 
ï®When the network is trained, random minibatches from the replay 
memory are used instead of the most recent transition 
ï®This method avoids the problem that successive training samples are 
too similar, which would push the network toward a local optimum 
ï®It is also possible to collect transitions from a human playerâ€™s game 
and train the network on those 
 
18 


## Page 19

Experience Replay 
ï®An action is chosen in an Ñ”-greedy way 
ï®The transition (st , at , rt+1 , st+1) is added to the replay memory 
ï®The system moves to state st+1 and the game continues, but the 
network weights are updated using a small number of transitions 
sampled from the replay memory (the current transition may be 
used as well but only from the replay memory) 
19 


## Page 20

Details 
ï®DQN maintains a replay memory D with capacity N 
ï®At each time step the agent observes a transition ğ‘ ğ‘¡, ğ‘ğ‘¡, ğ‘Ÿğ‘¡+1, ğ‘ ğ‘¡+1  
and stores it in D 
ï®When D is full, the oldest transitions are removed to make room for 
new ones 
ï®The replay buffer aggregates experience from many episodes and 
stages of learning 
ï®However, very old transitions may mislead if the dynamics or visited 
regions have changed, but FIFO replacement limits their influence 
20 


## Page 21

Reward Normalization and Error Clipping 
ï®Atari games have very different raw score scales across tasks 
ï®DQN standardizes rewards to reduce this variation 
ï®r = +1 if the game score increases 
ï®r = â€“1 if the game score decreases 
ï®r = 0 otherwise 
ï®The TD error is clipped to [â€“1, 1] 
 
 
ï®These two methods limit the size of parameter updates 
ï®The same parameters can work for many different games 
21 


## Page 22

Ñ”-greedy Policy 
ï®DQN typically uses an Ñ”-greedy policy for exploration 
ï®With probability Ñ”, the agent chooses a random action 
ï®With probability 1 â€“ Ñ”, the agent chooses ğ‘ğ‘¡= argmaxğ‘ğ‘„ğœƒğ‘ ğ‘¡, ğ‘ 
ï®At the start of training, Ñ” often equals 1 to encourage wide 
exploration 
ï®Over time, Ñ” decays linearly or according to a schedule down to a 
small value such as 0.1 
ï®The decay schedule balances exploration of new behaviors and 
exploitation of learned Q-values 
22 


## Page 23

23 
Each episode is a complete game 
t represents each step of the game 
Ï† represents the processed images x 


## Page 24

Full DQN Architecture 
ï®No pooling because positions are very important during gameplay 
24 


## Page 25

Practical DQN Techniques 
ï®The agent selects a new action only every k frames, typically k = 4 
(frame skipping) 
ï®During skipped frames the environment repeats the last chosen 
action 
ï®Frame skipping reduces computation and still captures relevant 
dynamics 
ï®The network input also includes several recent frames (also 4 in the 
original DQN), which encodes motion and short-term history 
ï®Optimization uses minibatch stochastic gradient descent with 
RMSProp or Adam 
25 


## Page 26

Training Methodology 
ï®DQN learned each game by interacting with the game emulator for 
50 million frames, equivalent to 38 days of gameplay 
ï®To evaluate performance after learning, for each Atari game the 
score was computed as the average over 30 games of 5 minutes each. 
Each game began from a random initial state 
ï®The same hyperparameters and network architecture were applied to 
all games 
26 


## Page 27

Results 
ï®
The x axis shows the score 
obtained by the DQN 
model as a percentage 
relative to human players 
ï®
In the game Montezumaâ€™s 
Revenge, where DQN gets 
0%, the hero can die very 
quickly and the network 
fails to learn 


## Page 28

Example: Breakout 
28 
https://www.youtube.com/watch?v=V1eYniJ0Rnk 


## Page 29

Strengths and Limitations 
ï®DQN learns directly from raw high-dimensional observations 
(images) 
ï®It uses a single network architecture and learning rule for many tasks 
ï®It demonstrates that general-purpose deep reinforcement learning 
(DRL) can reach human-level performance in some games 
 
ï®It tends to overestimate the Q-values 
ï®It requires many environment interactions and substantial 
computation (its sample efficiency is low) 
29 


## Page 30

Deep Q-Networks 
1. Standard DQN 
2. Double DQN 
3. Dueling DQN 
4. Rainbow DQN 
30 


## Page 31

Max-Induced Bias 
ï®Standard DQN target for transition ğ‘ â€², ğ‘Ÿ: 
 
ğ‘¦DQN = ğ‘Ÿ+ ğ›¾max
ğ‘â€² ğ‘„target ğ‘ â€², ğ‘â€²  
ï®The max operator assumes the largest estimated value is also the 
most accurate 
ï®Noisy value estimates make this assumption systematically optimistic 
ï®Overestimation enters directly into every bootstrapped target and 
distorts value predictions, which results in suboptimal policy 
learning 
 
 
31 


## Page 32

Example 
ï®Consider 2 actions with true values both equal to 10 
ï®Network estimates fluctuate, e.g. (9, 11) in one visit, and (12, 8) in 
another 
ï®Noise is symmetric around 10 (unbiased) but affects each action 
differently 
ï®max(9, 11) and max(12, 8) are both greater than 10 
ï®The average of the maximum is a biased overestimate of the true 
maximum 
32 


## Page 33

33 


## Page 34

Example (cont.) 
ï®The 2 independent estimators QA and QB are both noisy but unbiased 
around 10 
ï®QA(s', Â·) = (9, 11); QB(s', Â·) = (12, 8) 
ï®Updating QA: a* = argmaxa QA(s', a) = a2 
ï®The target for QA uses the other network: QB(s', a*) = 8, not  
maxa QA = 11 
ï®Next update may flip roles, so sometimes estimates fall above 10, 
sometimes below 
ï®Since selection and evaluation use independent noise, these errors 
cancel, and the expected value is 10 
 
34 


## Page 35

Double DQN 
ï®The online network ğ‘„ğ‘ â€², ğ‘â€²; ğœƒ selects the best action via 
arg max
ğ‘â€² ğ‘„ğ‘ â€², ğ‘â€²; ğœƒ 
ï®The target network ğ‘„ğ‘ â€², ğ‘â€²; ğœƒâ€²  evaluates the value of that selected 
action 
ï®The target is: 
35 
Here we use the ğœƒâ€² notation for the target network instead of ğœƒâˆ’,  
but it is exactly the same concept 


## Page 36

Details: Action Selection 
ï®First, we compute the Q values of all the next state-action pairs using 
the online (main) network Î¸, and we select action a', which has the 
maximum Q value:  
 
36 


## Page 37

Details: Q Value Computation 
ï®Once we have selected action a', we compute the Q value using the 
target network ğœƒâ€² for the selected action a' 
 
37 


## Page 38

DQN vs. Double DQN 
38 


## Page 40

Results 
ï®Double DQN outperformed standard DQN in most metrics for the 
same Atari games 
ï®Learning curves typically look smoother and more stable across 
games 
ï®Many scores improve substantially, e.g., Road Runner and Double 
Dunk 
ï®In Wizard of Wor and Asterix, DQN values can grow by orders of 
magnitude 
ï®The actual performance may stagnate or even degrade during this 
explosion 
ï®Double DQN keeps value growth modest and aligned with score 
improvements 
40 


## Page 41

Results: Human-Normalized Scores  
Game 
DQN 
Double DQN 
Wizard of Wor 
67.49 % 
110.67 % 
Asterix 
69.96 % 
180.15 % 
Road Runner 
232.91 % 
617.42 % 
Double Dunk 
17.10 % 
396.77 % 
41 


## Page 42

Example: Road Runner 
42 


## Page 43

Deep Q-Networks 
1. Standard DQN 
2. Double DQN 
3. Dueling DQN 
4. Rainbow DQN 
43 


## Page 44

Motivation 
ï®In many states, actions have similar outcomes; standard Q-networks 
model all (state, action) pairs equally, but this is inefficient 
ï®The dueling network learns common state information, and only 
computes specific action information when necessary 
ï®This improves generalization, especially in environments with 
redundant or uninformative actions 
 
44 


## Page 45

The Advantage Function 
ï®Q(s, a) gives the expected return for action a in state s 
ï®
ğ‘‰ğ‘ = ğ”¼ğ‘âˆ¼ğœ‹ğ‘„ğ‘ , ğ‘ estimates the value of a state under policy Ï€ 
ï®The advantage function A(s, a) = Q(s, a) â€“ V(s) measures how much 
better an action is compared to the average 
ï®A large positive advantage indicates an especially good action in that 
state 
45 


## Page 46

Dueling DQN 
ï®Dueling DQN introduces a network that separately estimates state 
value and action advantage 
ï®The network shares initial layers, then splits into two streams: one 
for V(s), one for A(s, a) 
ï®This architecture accelerates learning by modeling only meaningful 
action differences when necessary 
 
ï®In a sense, V and A compete (duel) to explain Q 
ï®Is the state good, or is a particular action better? 
 
46 


## Page 47

Dueling DQN Architecture 
The loss function remains the same  
47 


## Page 48

The Identifiability Problem 
ï®NaÃ¯ve approach: ğ‘„ğ‘ , ğ‘= ğ‘‰ğ‘ + ğ´ğ‘ , ğ‘ 
ï®This decomposition is not unique. For any constant c: 
  
ğ‘‰â€² ğ‘ = ğ‘‰ğ‘ + ğ‘,â€ƒğ´â€² ğ‘ , ğ‘= ğ´ğ‘ , ğ‘âˆ’ğ‘ 
  
give the same Q 
ï®The network can change the values between streams without 
changing Q(s, a) 
ï®Training becomes unstable because the roles of V and A drift 
48 


## Page 49

Normalized Advantage 
ï®The dueling architecture normalizes the advantages: 
  
ğ‘„ğ‘ , ğ‘= ğ‘‰ğ‘ +
ğ´ğ‘ , ğ‘âˆ’1
ğ’œ ğ´ğ‘ , ğ‘â€²
ğ‘â€²
 
      where |A| is the number of actions 
ï®The advantages in each state average to 0 
ï®V(s) captures the expected Q-value across actions 
ï®A(s, a) represents the deviations around this average 
ï®Subtracting the same mean from all A(s, a) preserves their ordering 
ï®argmax over Q(s, a) matches argmax over raw advantages 
49 


## Page 50

Example: Corridor with Redundant Actions 
ï®Consider a corridor state where many actions behave like no-ops 
ï®Only a few actions meaningfully move the agent forward along the 
corridor 
ï®A single-stream Q-network outputs one separate Q(s, a) value for 
each action 
ï®It must learn low values for many useless actions individually from 
data 
ï®Learning slows dramatically as the number of near-redundant 
actions in the state increases 
50 


## Page 51

State With Redundant Actions 
ï®Now consider a single state s with actions a1, â€¦, a5 
ï®Each action from s gives reward r = 1 and then transitions to a 
terminal state 
ï®The optimal values are identical: Q*(s, ai) = 1  âˆ€i 
ï®A single-stream network has separate outputs Q(s, a1), â€¦, Q(s, a5) for 
this state 
ï®Each update only changes the Q(s, ai) for the selected action; the 
others may remain poorly trained for a long time 
 
51 


## Page 52

Accelerating Learning 
ï®A dueling network produces one value V(s) and advantages  
A(s, a1), â€¦, A(s, a5) 
ï®It combines them as: ğ‘„ğ‘ , ğ‘=  ğ‘‰ğ‘ +  ğ´ğ‘ , ğ‘âˆ’
1
5  ğ´ğ‘ , ğ‘ğ‘– 
ï®Every sample from state s directly updates the shared value V(s), 
regardless of the chosen action 
ï®The network quickly sets V(s) â‰ˆ 1 and keeps advantages  
A(s, ai) â‰ˆ 0 
ï®All Q(s, ai) â‰ˆ 1, even for actions rarely sampled, which accelerates 
learning with redundant actions 
 
52 


## Page 53

Dueling DQN Benefits 
ï®The dueling network separately estimates state value and action 
advantages in each state. This separation reduces sensitivity to small, 
noisy differences in estimated returns between actions 
ï®It speeds up learning in environments that have many similar or 
redundant actions 
ï®It improves state evaluation because the network can share 
experience from all actions in a state 
ï®In tasks with few, always-critical actions, the dueling architecture 
provides only modest gains 
53 


## Page 54

Results 
ï®In a synthetic corridor task with 5, 10 or 20 actions, dueling networks 
learned faster as action redundancy increased 
ï®On 57 Atari games it improved performance in most cases, especially 
in games with many actions or few critical ones 
ï®The dueling architecture included Double DQN training 
54 


## Page 55

Results: Human-Normalized Scores  
Game 
Double DQN 
Dueling DQN  
Atlantis 
576.1% 
2285.3% 
Krull 
592.3% 
923.1% 
Road Runner 
563.2% 
887.4% 
Star Gunner 
620.5% 
924.0% 
55 


## Page 56

Example: Atlantis 
56 


## Page 57

Deep Q-Networks 
1. Standard DQN 
2. Double DQN 
3. Dueling DQN 
4. Rainbow DQN 
57 


## Page 58

Rainbow DQN 
ï®Combines 6, previously independent, improvements: 
ï®Double Q-learning: overestimation 
ï®Prioritized experience replay: important transitions 
ï®Dueling networks: efficiency and generalization 
ï®Multi-step learning: n-step return 
ï®Distributional Q-learning: entire return distribution 
ï®Noisy networks: stochastic NN layers 
58 


## Page 59

Prioritized Experience Replay 
ï®Basic DQN samples transitions uniformly from the replay buffer 
ï®Rainbow DQN assigns priorities based on TD error magnitude 
ï®Large TD errors indicate surprising or underrepresented experiences 
ï®Such transitions may deserve more frequent replay during training 
ï®Non-uniform sampling biases the learning update away from the 
true behavior distribution 
ï®Importance sampling downscales the losses of oversampled 
transitions to partially correct this bias 
 
ï®Two variants: proportional or rank-based prioritization 
59 


## Page 60

Proportional Prioritization 
ï®We define the transition priority pi using its TD error ğ›¿ğ‘–: ğ‘ğ‘–= ğ›¿ğ‘– 
ï®Absolute value keeps priorities non-negative for all transitions 
ï®If ğ›¿ğ‘–= 0, that transition is never sampled 
ï®A small Ñ” > 0 is added to avoid zero priority: ğ‘ğ‘–= ğ›¿ğ‘–+ ğœ– 
ï®Priorities are converted to probabilities: ğ‘ƒ(ğ‘–) =
ğ‘ğ‘–
 ğ‘ğ‘˜
ğ‘˜
 
ï®We can also control the strength of prioritization: ğ‘ƒ(ğ‘–) =
ğ‘ğ‘–
ğ›¼
 ğ‘ğ‘˜
ğ›¼
ğ‘˜
 
ï®Î± = 1: we strongly prioritize large pi 
ï®Î± = 0: uniform random sampling 
60 


## Page 61

Rank-based Prioritization 
ï®Rank-based prioritization defines priority from the rank of each 
transition in the replay buffer 
ï®The replay buffer orders transitions from high TD error to low;  
Ranki is the position of transition i 
ï®The priority of transition i is: ğ‘ğ‘–=
1
ğ‘…ğ‘ğ‘›ğ‘˜ğ‘– 
ï®The probabilities P(i) are computed in the same way from pi 
 
61 


## Page 62

Correcting the Sampling Bias 
ï®Proportional and rank-based prioritization over-sample transitions 
with high TD errors 
ï®Learning then focuses on a small subset of high-error transitions, 
which increases the risk of overfitting 
ï®Importance weights wi are used to downweight frequently sampled, 
high-priority transitions 
 
 
 
ï®where N is the replay buffer size, P(i) is the sampling probability, and 
Î² gradually increases from 0.4 to 1 
 
62 


## Page 63

Multi-Step Learning 
ï®
Rainbow DQN incorporates multi-step returns into Q-learning updates for 
faster value propagation 
ï®
Sparse-reward tasks benefit because reward information travels back more 
quickly 
ï®
n-step target for starting time t: 
ğºğ‘¡:ğ‘¡+ğ‘›=  ğ›¾ğ‘˜ğ‘…ğ‘¡+ğ‘˜+1
ğ‘›âˆ’1
ğ‘˜=0
+ ğ›¾ğ‘›max
ğ‘â€² ğ‘„ğ‘†ğ‘¡+ğ‘›, ğ‘â€²  
ï®
Replay entries correspond to length-n trajectories 
ğ‘†ğ‘¡, ğ´ğ‘¡, ğ‘…ğ‘¡+1, â€¦ , ğ‘…ğ‘¡+ğ‘›, ğ‘†ğ‘¡+ğ‘›, not only single steps 
ï®
For each start t, we form the transition (ğ‘†ğ‘¡, ğ´ğ‘¡, ğºğ‘¡:ğ‘¡+ğ‘›, ğ‘†ğ‘¡+ğ‘›)  
ï®
Usually, a small n (e.g., 3) is used to preserve off-policy Q-learning while 
improving updates under delayed or sparse rewards 
 
63 


## Page 64

Distributional RL 
ï®Classical Q-learning learns the expected return: 
  
ğ‘„ğ‘ , ğ‘= ğ”¼ğºğ‘¡
ğ‘ ğ‘¡= ğ‘ , ğ‘ğ‘¡= ğ‘ 
ï®Different actions can have the same Q-value but have very different 
return variability 
ï®One action can yield a steady medium reward; another can alternate 
between very high and very low rewards 
ï®Distributional RL models the random return Z(s, a) for each  
state-action pair 
ï®The expected value remains: 
  
ğ‘„ğ‘ , ğ‘= ğ”¼ğ‘ğ‘ , ğ‘ 
  
but the learning signal comes from the full distribution 
64 


## Page 65

C51: Fixed-Atom Representation 
ï®C51 represents each return distribution using 51 fixed atoms 
ğ‘§1, â€¦ , ğ‘§51  
ï®The atoms lie evenly between the bounds ğ‘£min and ğ‘£max or returns; 
they provide the same support for all (s, a) 
ï®The network outputs 51 probabilities for each (s, a) 
ï®These probabilities are updated after each transition 
65 


## Page 66

Example: Same Mean, Different 
Distributions 
ï®Consider 2 actions in one state with one-step returns 
ï®Action A: reward â€“10 with probability 0.5, +10 with probability 0.5 
ï®Action B: reward always 0 
ï®Both actions have the mean return equal to 0 
ï®The standard DQN uses only expectations and cannot distinguish 
these 2 actions 
ï®C51 learns different distributions  
ï®Assume the atoms are {â€“10, 0, 10} 
ï®Then: pA = [0.5, 0, 0.5], pB = [0, 1, 0] 
 
66 


## Page 67

Noisy Networks 
ï®Standard DQN commonly uses Ñ”-greedy exploration for action 
selection 
ï®A single global Ñ” ignores state-dependent uncertainty and exploration 
needs 
ï®Preset decay schedules reduce exploration even when important 
regions remain poorly explored 
ï®Noisy Networks add randomness to network parameters, and the 
stochastic policies become state-dependent 
67 


## Page 68

Noisy Linear Layers 
ï®A standard linear layer computes ğ‘¦ =  ğ‘Šğ‘¥ +  ğ‘ 
ï®Noisy Networks replace this with a parameterized noisy 
transformation: 
  
ğ‘¦= ğ‘Š+ ğœğ‘ŠâŠ™ğœ–ğ‘Šğ‘¥+ ğ‘+ ğœğ‘âŠ™ğœ–ğ‘ 
ï®
ğ‘Š, ğ‘ are the usual weights and biases, but ğœğ‘Š, ğœğ‘ control noise 
scale or magnitude. ğœ–ğ‘Š, ğœ–ğ‘ are random variables 
ï®Learning adjusts these scales Ïƒ, so the agent decides itself where 
strong or weak randomness is useful 
68 


## Page 69

Adaptive Exploration 
ï®Rainbow DQN replaces standard final linear layers in the value 
network with noisy layers 
ï®External Ñ”-greedy exploration is removed or reduced to a small 
constant if desired 
ï®In familiar states, learning drives Ïƒ towards 0; the policy becomes 
effectively deterministic 
ï®In uncertain or high-impact regions, Ïƒ can remain large to keep 
exploration active 
ï®Exploration adapts automatically, guided by the same loss that trains 
the value distribution 
69 


## Page 70

Example: Two-Armed Bandit 
ï®Letâ€™s consider a two-armed bandit with actions 1 and 2, each with an 
unknown reward 
ï®A small neural network takes a constant input and produces 2 
outputs through a noisy final layer 
ï®Early in training, the learned scales Ïƒ are large, so sampled Q-values 
differ between forward passes 
ï®Some samples rank action 1 higher, others rank action 2 higher, so 
both arms are explored 
ï®As one actionâ€™s expected value becomes clearly better, gradients push 
its corresponding Ïƒ toward smaller values 
70 


## Page 71

Example: Corridor Junction 
ï®The environment is a corridor with a junction: the left branch gives a 
small immediate reward, the right a delayed (distant) large reward 
ï®Ñ”-greedy exploration often breaks long right-branch runs because 
random action flips interrupt the trajectory 
ï®A sampled noisy-parameter set tends to stay roughly fixed over many 
steps, sometimes for an entire episode 
ï®One such sample can consistently prefer right-branch actions, 
allowing the agent to reach the distant large reward 
ï®The observed large reward then updates both the base weights and 
the noise scales to make similar coherent trajectories more likely in 
the future 
71 


## Page 72

Results: Human-Normalized Scores  
Game 
Dueling DQN 
Rainbow DQN 
Alien 
67% 
134% 
H.E.R.O. 
68% 
184% 
Ice Hockey 
79% 
102% 
Yarsâ€™ Revenge 
45% 
193% 
Montezumaâ€™s Revenge 
0% 
8% 
72 


## Page 73

Example: Montezumaâ€™s Revenge 
73 


## Page 74

Atari Games after DQN 
ï®OpenAI using PPO (2018): 1570% of average human performance for 
Montezumaâ€™s Revenge  
ï®Uber AI Go-Explore (2019): 920% for Montezumaâ€™s Revenge without 
domain knowledge, 14 000% with domain knowledge 
ï®DeepMind Agent57 (2020): First agent better than humans on all 57 
Atari games; 200% for Montezumaâ€™s Revenge 


## Page 75

Conclusions 
ï®Standard DQN demonstrates end-to-end control from pixels but 
suffers from overestimation and unstable, sample-inefficient learning 
ï®Double DQN reduces value overestimation, stabilizes training, and 
usually achieves better performance than standard DQN 
ï®Dueling DQN separates state value and action advantage, which 
improves generalization and learning speed in states with redundant 
actions 
ï®Rainbow DQN combines six extensions into one architecture, and 
delivers the strongest, most robust Atari results among DQN variants 
75 


## Page 76

Main References 
ï®Ravichandiran, S. (2020). Deep Reinforcement Learning with 
Python. 2nd edition. Packt Publishing, Birmingham, UK 
ï®Hessel, M., et al. (2018). Rainbow: Combining Improvements in 
Deep Reinforcement Learning. Proceedings of AAAI-18. AAAI 
Press, New Orleans, LA, USA. https://arxiv.org/pdf/1710.02298 
 
 
76 
