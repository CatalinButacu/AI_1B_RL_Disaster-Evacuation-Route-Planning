# RL12_MARL

**Source:** c:\Users\catalin.butacu\Downloads\RL\resourses\lecture\RL12_MARL.pdf

**Pages:** 63

---


## Page 1

Reinforcement Learning 
12. Multi-Agent Reinforcement Learning 
 
Florin Leon 
 
â€œGheorghe Asachiâ€ Technical University of IaÈ™i, Romania 
Faculty of Automatic Control and Computer Engineering 
 
https://florinleon.byethost24.com/lect_rl.html 
 
2025 
 


## Page 2

Multi-Agent Reinforcement Learning 
1. Multi-Agent Reinforcement Learning 
2. Independent Q-Learning (IQL) 
3. Centralized Training with Decentralized Execution (CTDE) 
4. Multi-Agent Deep Deterministic Policy Gradient (MADDPG) 
5. Multi-Agent Proximal Policy Optimization (MAPPO) 
6. MARL Approaches in Multi-Player Games 
2 


## Page 3

Multi-Agent Reinforcement Learning 
1. Multi-Agent Reinforcement Learning 
2. Independent Q-Learning (IQL) 
3. Centralized Training with Decentralized Execution (CTDE) 
4. Multi-Agent Deep Deterministic Policy Gradient (MADDPG) 
5. Multi-Agent Proximal Policy Optimization (MAPPO) 
6. MARL Approaches in Multi-Player Games 
3 


## Page 4

From Single-Agent RL to MARL 
ï®Reinforcement Learning typically studies a single agent maximizing 
cumulative reward in an environment 
ï®Many real-world systems involve multiple agents interacting in a 
shared environment 
ï®Agents may interact competitively, cooperatively, or in mixed 
cooperativeâ€“competitive forms 
ï®These interactions introduce complexities beyond the assumptions of 
single-agent RL algorithms 
ï®Multi-Agent Reinforcement Learning (MARL) extends RL to handle 
such multi-agent environments 
4 


## Page 5

Complexity and Stateâ€“Action Growth 
ï®In multi-agent settings, each agentâ€™s state and actions influence the 
global environment state 
ï®Overall state space grows quickly as more agents and local state 
variables appear 
ï®For n agents, each with m actions, joint action space size is mn 
ï®Tabular methods like Q-learning become infeasible on exponentially 
large stateâ€“action spaces 
5 


## Page 6

Curse of Dimensionality 
ï®Increased agent count produces very high-dimensional state and 
action representations 
ï®Learning an optimal policy becomes computationally expensive in 
these large spaces 
ï®Each agent must reason about how othersâ€™ actions affect its own 
future rewards 
ï®Traditional single-agent algorithms assume more modest, tractable 
state spaces 
ï®MARL seeks algorithms that tolerate and exploit large, coupled 
decision spaces 
6 


## Page 7

Dependencies and Non-Stationarity 
ï®Single-agent RL often assumes environment dynamics depend only 
on one agentâ€™s actions 
ï®In MARL, dynamics depend on all agentsâ€™ actions and their changing 
policies 
ï®When one agent improves its policy, others effectively experience a 
new environment 
ï®This non-stationarity destabilizes learning for algorithms that 
assume fixed dynamics 
ï®MARL algorithms address environments where transition and 
reward structures evolve during learning 
7 


## Page 8

Coordination and Communication 
ï®Many MARL tasks require agents to coordinate actions to achieve 
shared goals 
ï®Autonomous driving illustrates vehicles coordinating maneuvers to 
ensure safety and traffic efficiency 
ï®Single-agent RL frameworks lack built-in mechanisms for multi-
agent coordination 
ï®MARL introduces communication or information-sharing among 
agents when the task demands it 
ï®Coordination mechanisms help agents reach better collective 
outcomes than independent learning 
8 


## Page 9

Defining Multi-Agent Reinforcement 
Learning 
ï®MARL studies multiple agents learning simultaneously in a shared 
environment 
ï®Each agent interacts with the environment and updates its behavior 
from experience 
ï®One agentâ€™s actions affect the environment state and the rewards of 
other agents 
ï®Goal: each agent learns a policy that maximizes its long-term reward 
under multi-agent interaction 
ï®Settings range from fully cooperative through purely competitive to 
mixed cooperative-competitive tasks 
9 


## Page 10

Motivation for MARL 
ï®Many realistic domains involve several autonomous decision makers, 
not a single controller 
ï®Single-agent RL cannot capture strategic interactions and mutual 
influence between agents 
ï®Multi-agent environments appear non-stationary from each 
individual agentâ€™s viewpoint 
ï®MARL develops methods that adapt to other agentsâ€™ changing 
strategies 
ï®This capability is essential for complex, dynamic tasks with 
interacting learners 
10 


## Page 11

Applications: Autonomous Driving and 
Robotics 
ï®Autonomous driving uses MARL to manage interactions among many 
self-driving vehicles 
ï®Vehicles must avoid collisions while optimizing traffic flow and travel 
efficiency 
ï®MARL supports cooperative behaviors such as platooning and 
dynamic speed adjustment 
ï®In robotics, multiple agents coordinate exploration, search-and-
rescue, or manufacturing tasks 
ï®Swarm intelligence uses MARL to divide work, reduce redundancy, 
and handle uncertain environments 
11 


## Page 12

Applications: Games and Resource 
Management 
ï®MARL powers agents in complex strategy games requiring both 
cooperation and competition 
ï®Systems like AlphaStar learn to play StarCraft II at expert human 
levels 
ï®Game agents manage resources, build units, and coordinate tactics 
across large environments 
ï®In smart grids, MARL coordinates power generation, distribution, 
and consumption 
ï®Resource-management agents balance local decisions with global 
efficiency and reliability 
12 


## Page 13

Fundamental MARL Concepts 
ï®Each agent i has policy ğœ‹ğ‘–ğ‘ğ‘–
ğ‘  specifying probabilities of actions 
ğ‘ğ‘– in state s 
ï®Joint policy ğ…= ğœ‹1, â€¦ , ğœ‹ğ‘› describes behavior of all agents together 
ï®Joint action space: ğ’œ= ğ´1 Ã— â‹¯Ã— ğ´ğ‘› 
ï®Competitive rewards for agent i and shared cooperative reward: 
ğ‘…ğ‘–ğ‘ , ğš, ğ‘ â€²  
ï®Transition dynamics depend on joint actions: ğ‘ƒğ‘ â€²
ğ‘ , ğš  
13 


## Page 14

Multi-Agent Reinforcement Learning 
1. Multi-Agent Reinforcement Learning 
2. Independent Q-Learning (IQL) 
3. Centralized Training with Decentralized Execution (CTDE) 
4. Multi-Agent Deep Deterministic Policy Gradient (MADDPG) 
5. Multi-Agent Proximal Policy Optimization (MAPPO) 
6. MARL Approaches in Multi-Player Games 
14 


## Page 15

From Single-Agent to Multi-Agent Q-
Learning 
ï®Standard Q-learning estimates ğ‘„ğ‘ , ğ‘ as the expected return from 
action a in state s under an optimal policy 
ï®The Q-learning update is: 
ğ‘„ğ‘ , ğ‘â†ğ‘„ğ‘ , ğ‘+ ğ›¼ğ‘Ÿ+ ğ›¾max
ğ‘â€² ğ‘„ğ‘ â€², ğ‘â€² âˆ’ğ‘„ğ‘ , ğ‘ 
ï®In multi-agent settings, multiple agents simultaneously select actions, 
producing joint transitions and coupled rewards 
ï®Independent Q-Learning (IQL) treats each agent as isolated, applying 
Q-learning separately without coordination or modeling others 
ï®IQL assumes other agentsâ€™ effects on state transitions and rewards 
are external, unmodeled environmental dynamics 
15 


## Page 16

Q-Function Updates in IQL 
ï®Each agent iii maintains its own Q-function ğ‘„ğ‘–ğ‘ , ğ‘ğ‘–, updated 
independently based on local actions and rewards 
ï®The IQL update rule is: 
ğ‘„ğ‘–ğ‘ , ğ‘ğ‘–â†ğ‘„ğ‘–ğ‘ , ğ‘ğ‘–+ ğ›¼ğ‘Ÿğ‘–+ ğ›¾max
ğ‘ğ‘–
â€² ğ‘„ğ‘–ğ‘ â€², ğ‘ğ‘–
â€² âˆ’ğ‘„ğ‘–ğ‘ , ğ‘ğ‘–
 
ï®The environment state s includes all agentsâ€™ positions and features, 
allowing the Q-function to condition on global state 
ï®No agent explicitly observes or predicts other agentsâ€™ actions; their 
effects are absorbed into the observed transition 
ï®IQL assumes transitions ğ‘ , ğ‘ğ‘–, ğ‘ â€²  result from marginalizing over the 
unknown joint action ğ‘ğ‘–, ğ‘âˆ’ğ‘– 
 
16 


## Page 17

Joint Action Space and Environmental 
Dynamics 
ï®At each step, all agents act simultaneously, forming a joint action 
vector ğ®= ğ‘1, ğ‘2, â€¦ , ğ‘ğ‘ 
ï®The next stateğ‘ â€² âˆ¼ğ‘ƒğ‘ â€²
ğ‘ , ğ® depends on the full joint action, not 
just individual agent actions 
ï®Each agent receives a reward ğ‘Ÿğ‘–= ğ‘…ğ‘–ğ‘ , ğ® that reflects the effects of 
all agentsâ€™ behaviors 
ï®IQL agents experience transitions from s to s' due to ğ‘ğ‘–â€‹, but these 
transitions are non-stationary 
ï®As each agent changes its policy over time, the effective environment 
for any agent becomes non-Markovian 
 
17 


## Page 18

Full Observability and State Encoding 
ï®Fully observable states encode all agentsâ€™ positions, goals, and 
features in a structured vector or tensor 
ï®For example, a grid-world state may be represented as a 3-channel 
tensor: one for each agent and one for goals 
ï®The Q-function ğ‘„ğ‘–ğ‘ , ğ‘ğ‘– takes the global state s and the agentâ€™s own 
action ğ‘ğ‘–â€‹ as input 
ï®When agent 2 is present, the state s contains features that alter agent 
1â€™s Q-values compared to being alone 
ï®Despite no explicit modeling, the influence of other agents is 
embedded in the changing input state representations 
 
18 


## Page 19

Non-Stationarity and Learning Instability 
ï®IQL environments are non-stationary due to simultaneous policy 
updates by all agents during learning 
ï®This violates the Markov assumption required for Q-learning 
convergence in standard MDPs 
ï®Each agentâ€™s environment appears to change over time even if the 
physical environment is static 
ï®In cooperative settings, agents may converge if their joint behaviors 
stabilize 
ï®In competitive or mixed settings, instability can cause oscillation or 
divergence of learned policies 
 
19 


## Page 20

Implicit Modeling Through State 
Representation 
ï®IQL agents do not construct predictive models of othersâ€™ actions or 
policies 
ï®The full state s includes observable features such as positions, 
velocities, or goals of other agents 
ï®Differences between states ğ‘ 1â€‹ (agent alone) and ğ‘ 2â€‹ (with others) lead 
to different Q-values for the same action 
ï®This implicit modeling allows agents to learn reactive behaviors 
without estimating othersâ€™ intentions 
ï®The environment design must ensure that all relevant agent 
information is embedded in the state input to the Q-function 
 
20 


## Page 21

Per-Agent Learning from Marginal 
Transitions 
ï®IQL assumes transitions ğ‘ , ğ‘ğ‘–, ğ‘ â€²  are averaged over unknown joint 
action distributions ğ‘ƒğ‘âˆ’ğ‘– 
ï®The Q-function ğ‘„ğ‘–ğ‘ , ğ‘ğ‘–= ğ”¼ 
ğ›¾ğ‘¡ğ‘Ÿğ‘–
ğ‘¡
âˆ
ğ‘¡=0
depends on expectations 
over othersâ€™ behavior 
ï®The dynamics seen by agent iii reflect marginal distributions, not a 
fixed transition model 
ï®Each Q-update incorporates the effects of hidden, evolving agent 
policies into the observed outcome 
ï®This approach works only if the other agentsâ€™ influence is consistent 
or stabilizes over time 
 
21 


## Page 22

Grid World Example with Structured State 
ï®In a 5Ã—5 grid world, the global state may be encoded as a 5Ã—5Ã—3 
tensor 
ï®Channel 1 indicates Agent 1â€™s position; channel 2 shows Agent 2â€™s; 
channel 3 marks goal locations 
ï®Each agent receives this full tensor as input to its Q-function 
ğ‘„ğ‘–ğ‘ , ğ‘ğ‘–, supporting reactive learning 
ï®If Agent 2 is near Agent 1, the state input reflects proximity, 
influencing Agent 1â€™s action choices 
ï®Coordination emerges implicitly, as agents learn which actions yield 
better outcomes in the presence of others 
 
22 


## Page 23

Advantages of Independent Q-Learning 
ï®IQL is simple to implement using standard single-agent Q-learning 
algorithms per agent 
ï®No explicit communication or coordination protocols are required 
between agents 
ï®The method scales to many agents by avoiding centralized 
representations of joint actions 
ï®Agents can operate in parallel, each using local Q-functions and 
global observations 
ï®IQL often performs well in cooperative environments where shared 
goals reduce policy conflict 
 
23 


## Page 24

Limitations and Assumptions in IQL 
ï®IQL breaks the Markov assumption due to changing transition 
dynamics caused by learning agents 
ï®Q-learning guarantees no longer apply; convergence is not ensured 
in general 
ï®In competitive or mixed settings, learning may become unstable or 
fail to converge 
ï®The method assumes that either policies eventually stabilize or that 
the environmentâ€™s stochasticity absorbs the non-stationarity 
ï®IQL is unsuitable for tasks requiring explicit reasoning about othersâ€™ 
intentions or tight inter-agent coordination 
 
24 


## Page 25

Multi-Agent Reinforcement Learning 
1. Multi-Agent Reinforcement Learning 
2. Independent Q-Learning (IQL) 
3. Centralized Training with Decentralized Execution (CTDE) 
4. Multi-Agent Deep Deterministic Policy Gradient (MADDPG) 
5. Multi-Agent Proximal Policy Optimization (MAPPO) 
6. MARL Approaches in Multi-Player Games 
25 


## Page 26

Centralized Training, Decentralized 
Execution 
ï®CTDE allows agents to train using global state and joint action 
information but execute policies using only local observations 
ï®During training, agents can access full environmental state s, actions 
ğ‘1, â€¦ , ğ‘ğ‘, and rewards ğ‘Ÿ1, â€¦ , ğ‘Ÿğ‘â€‹ 
ï®During execution, each agent iii follows a decentralized policy ğœ‹ğ‘–ğ‘œğ‘–, 
using only local observation ğ‘œğ‘– 
ï®This approach addresses non-stationarity caused by concurrent 
learning in multi-agent settings 
ï®CTDE supports scalable, real-time execution while enabling 
coordinated learning through centralized feedback 
 
26 


## Page 27

Learning Together, Acting Alone 
ï®CTDE reflects a natural learning strategy: full guidance during 
training, independent decision-making in deployment 
ï®Like humans trained with teachers and peers, agents benefit from 
global context before acting solo in dynamic environments 
ï®CTDE mirrors real systems, such as teams, swarms, and societies, 
that coordinate through shared learning, not constant 
communication 
ï®The paradigm bridges practical constraints and theoretical 
challenges, creating robust multi-agent coordination frameworks 
ï®It shows that shared training, even without runtime messaging, can 
create implicitly coordinated autonomous agents 
 
27 


## Page 28

Formal CTDE Framework and Components 
ï®The environment is modeled as a Dec-POMDP 
ğ‘†, {ğ´ğ‘–}, ğ‘ƒ, {ğ‘Ÿğ‘–}, ğ‘, ğ‘‚, ğ›¾ 
ï®The centralized critic uses the global state s and joint action 
ğš= ğ‘1, â€¦ , ğ‘ğ‘ to estimate value functions 
ï®The critic is trained using temporal-difference loss: 
â„’ğœƒ= ğ¸
ğ‘Ÿ+ ğ›¾max
ğšâ€² ğ‘„tot ğ‘ â€², ğšâ€²; ğœƒâˆ’âˆ’ğ‘„tot ğ‘ , ğš; ğœƒ
2  
ï®Each agent learns an independent policy ğœ‹ğ‘–ğ‘œğ‘–, guided by the 
centralized value function or critic 
ï®Centralized training exploits information unavailable at execution 
time, improving stability and coordination 
 
28 


## Page 29

Centralized Critic Methods 
ï®Centralized critic methods separate actor and critic roles; actors are 
decentralized, critics use full state and joint actions 
ï®In MADDPG, actors use local inputs ğ‘œğ‘–â€‹, but critics are trained with s 
and a 
ï®The actor gradient in MADDPG is âˆ‡ğœ“ğ‘–log ğœ‹ğ‘–
ğ‘ğ‘–
ğ‘œğ‘–âˆ‡ğ‘ğ‘–ğ‘„ğ‘–ğ‘ , ğš  
ï®This framework allows decentralized actors to benefit from 
centralized critics during training only 
ï®Centralized critics reduce instability by removing ambiguity from 
other agentsâ€™ policies during training 
 
29 


## Page 30

Value Function Factorization 
ï®In cooperative tasks with shared rewards, the total value can be 
decomposed into agent-specific components 
ï®QMIX factorizes ğ‘„tot ğ‘ , ğš using a monotonic mixing network: 
ğ‘„tot ğ‘ , ğš= ğ‘“ğ‘„1 ğ‘œ1, ğ‘1 , â€¦ , ğ‘„ğ‘ğ‘œğ‘, ğ‘ğ‘; ğ‘  
ï®The mixing network ensures that maximizing each ğ‘„ğ‘–â€‹ leads to the 
joint optimal action under constraints 
ï®VDN, a simpler variant, uses additive decomposition ğ‘„tot =  ğ‘„ğ‘–
ğ‘–
â€‹, 
without state-dependent mixing 
ï®These methods support decentralized execution while maintaining 
joint optimality during training 
 
30 


## Page 31

Strengths of CTDE 
ï®Rich training signals from global state and actions enhance 
coordination and convergence stability 
ï®CTDE mitigates non-stationarity caused by simultaneous multi-agent 
learning 
ï®Policies trained under CTDE execute independently, avoiding 
communication overhead during deployment 
ï®CTDE can align decentralized policies with a global objective, 
improving team performance 
ï®Scalability is achieved by decoupling training complexity from 
execution runtime 
 
31 


## Page 32

Limitations and Challenges of CTDE 
ï®Centralized training assumes availability of full environment state, 
which may not exist in real-world applications 
ï®Joint action spaces grow exponentially with the number of agents, 
challenging critic scalability 
ï®Mismatch between training (with full information) and execution 
(with partial observation) may reduce performance 
ï®Designing effective critics or mixing networks is essential for stable 
and generalizable training 
ï®CTDE may require simulation or full observability during training, 
limiting its use in partially observable or adversarial environments 
32 


## Page 33

Applications of CTDE 
ï®CTDE methods are used in robotic teams, such as drones or 
warehouse robots trained with shared global state 
ï®Self-driving vehicles use CTDE during simulation to learn 
cooperative driving behavior with access to joint information 
ï®In games like StarCraft (SMAC benchmark), agents train via CTDE to 
coordinate strategies while executing independently 
ï®Value factorization methods like QMIX and QPLEX have been applied 
to multi-robot path planning and cooperative control 
ï®CTDE outperforms independent Q-learning by leveraging global 
training data while preserving decentralized policies 
 
33 


## Page 34

Comparison to Single-Agent Q-Learning 
ï®Single-agent Q-learning assumes a stationary environment; 
transitions depend only on the agent's own actions 
ï®In multi-agent settings, co-adapting policies violate this assumption 
and introduce non-stationarity 
ï®CTDE mitigates these issues by incorporating other agentsâ€™ behavior 
into centralized training with global information 
ï®During execution, decentralized policies operate robustly despite the 
dynamic environment 
ï®Single-agent methods cannot adapt to the inter-agent dynamics that 
CTDE explicitly models during training 
 
34 


## Page 35

Concluding Remarks on CTDE 
ï®CTDE enables agents to learn in rich, fully informed environments 
while acting autonomously after deployment 
ï®Centralized critics and value decomposition networks support 
coordination without requiring communication at runtime 
ï®CTDE remains a foundational design in modern MARL, balancing 
theory with real-world applicability 
ï®Future research aims to improve critic scalability, mixing 
architectures, and robustness to trainingâ€“execution mismatch 
ï®CTDE enables complex behaviors in cooperative and mixed-motive 
environments that are infeasible under purely decentralized learning 
 
35 


## Page 36

Multi-Agent Reinforcement Learning 
1. Multi-Agent Reinforcement Learning 
2. Independent Q-Learning (IQL) 
3. Centralized Training with Decentralized Execution (CTDE) 
4. Multi-Agent Deep Deterministic Policy Gradient (MADDPG) 
5. Multi-Agent Proximal Policy Optimization (MAPPO) 
6. MARL Approaches in Multi-Player Games 
36 


## Page 37

DDPG Recap and Extension 
ï®DDPG uses an actor-critic architecture for continuous action spaces, 
with off-policy learning and target networks 
ï®The critic minimizes TD error using the target: 
  
ğ‘¦= ğ‘Ÿ+ ğ›¾ğ‘„ğ‘ â€², ğœ‹ğ‘ â€²; ğœƒğœ‹âˆ’; ğœƒğ‘„âˆ’ 
ï®The critic loss is â„’ğœƒğ‘„= ğ¸
ğ‘„ğ‘ , ğ‘; ğœƒğ‘„âˆ’ğ‘¦2 , with replay buffer 
sampling 
ï®Extending DDPG to multi-agent settings requires addressing joint 
action effects and agent co-adaptation 
ï®MADDPG introduces agent-specific centralized critics to handle the 
joint state-action space 
 
37 


## Page 38

Motivation for MADDPG 
ï®Independent agents using DDPG suffer from unstable learning due to 
non-stationarity in multi-agent environments 
ï®MADDPG extends DDPG to multi-agent systems using centralized 
training with decentralized execution (CTDE) 
ï®Each agentâ€™s critic is trained with access to global state and all agentsâ€™ 
actions, stabilizing the learning process 
ï®Each actor relies only on local observations, enabling deployment 
without centralized information 
ï®MADDPG supports coordination in continuous control tasks such as 
robotics and multi-agent navigation 
 
38 


## Page 39

Formal Framework and Setup 
ï®MADDPG models the environment as a Dec-POMDP: 
ğ‘†, {ğ´ğ‘–}, ğ‘ƒ, {ğ‘Ÿğ‘–}, {ğ‘‚ğ‘–}, ğ›¾ 
ï®Each agent iii has an actor ğœ‹ğ‘–ğ‘œğ‘–; ğœƒğ‘–
ğœ‹ using local observations ğ‘œğ‘–â€‹ 
ï®The centralized critic ğ‘„ğ‘–ğ‘¥, ğš; ğœƒğ‘–
ğ‘„ is trained on the full state x and 
joint action a 
ï®The critic evaluates how agent iâ€™s action performs in the context of all 
agentsâ€™ actions 
ï®This setup allows agents to learn policies aligned with global 
outcomes despite decentralized execution 
 
39 


## Page 40

Critic and Actor Updates 
ï®
For agent i, the criticâ€™s TD target is ğ‘¦= ğ‘Ÿğ‘–+ ğ›¾ğ‘„ğ‘–ğ‘¥â€², ğšâ€²; ğœƒğ‘–
ğ‘„âˆ’, with 
ğ‘ğ‘—
â€² = ğœ‹ğ‘—ğ‘œğ‘—
â€²; ğœƒğ‘—
ğœ‹âˆ’ 
ï®
The critic loss is 
â„’ğœƒğ‘–
ğ‘„= ğ¸
ğ‘„ğ‘–ğ‘¥, ğš; ğœƒğ‘–
ğ‘„âˆ’ğ‘¦
2  
ï®
The actor gradient is 
âˆ‡ğœƒğ‘–
ğœ‹ğ½â‰ˆğ¸âˆ‡ğ‘ğ‘–ğ‘„ğ‘–ğ‘¥, ğš; ğœƒğ‘–
ğ‘„âˆ‡ğœƒğ‘–
ğœ‹ğœ‹ğ‘–ğ‘œğ‘–; ğœƒğ‘–
ğœ‹
 
ï®
Updates use experiences from a shared replay buffer D, sampled off-policy 
ï®
Target networks for actor and critic are updated using soft updates with 
parameter Ï„ 
 
40 


## Page 41

Core Features of MADDPG 
ï®Centralized critics provide stronger learning signals by evaluating 
actions in global context 
ï®Decentralized actors ensure scalability and deployment without 
shared observations 
ï®Replay buffer and target networks help stabilize learning in dynamic 
environments 
ï®The algorithm supports continuous control, making it well-suited for 
robotic and physical systems 
ï®MADDPG accommodates cooperative and mixed-motive tasks 
without requiring communication during execution 
 
41 


## Page 42

Key Limitations and Open Issues 
ï®The centralized critic must process large joint action and state 
spaces, increasing computational cost as agent count grows 
ï®Replay buffer experiences may become stale if agentsâ€™ policies change 
significantly over time 
ï®Execution under partial observability may diverge from training 
conditions that use full state 
ï®Effective credit assignment remains difficult, especially when 
rewards are sparse or delayed 
ï®Scalability and generalization depend on careful architecture design 
and training discipline 
 
42 


## Page 43

Comparison to Independent Methods 
ï®Independent actor-critic methods lack stability in multi-agent 
environments due to unmodeled interactions 
ï®MADDPG improves stability by incorporating joint state and actions 
into critic updates during training 
ï®Shared global rewards can be leveraged more effectively with 
centralized critics than with independent learning 
ï®Decentralized execution retains practical feasibility while improving 
learning through centralized feedback 
ï®MADDPG achieves better coordination and efficiency than fully 
independent policy learning 
 
43 


## Page 44

Real-World Applications of MADDPG 
ï®Autonomous vehicles train together for coordinated driving and then 
act independently using local sensors 
ï®Multi-robot systems, such as delivery drones or manipulators, use 
MADDPG for collaborative control tasks 
ï®MADDPG has shown strong performance in simulated multi-agent 
domains such as StarCraft Multi-Agent Challenge (SMAC) 
ï®Continuous control problems in partially observable environments 
benefit from MADDPGâ€™s centralized learning structure 
ï®MADDPG supports robust, scalable policies without requiring real-
time inter-agent communication 
 
44 


## Page 45

Final Remarks on MADDPG 
ï®MADDPG advances MARL by integrating centralized learning with 
decentralized, actor-only execution 
ï®Centralized critics solve non-stationarity and support coordinated 
behavior in dynamic agent environments 
ï®Replay-based, off-policy training with target networks adds stability 
to multi-agent learning 
ï®The method is well suited for cooperative and continuous-action 
domains where global coordination is essential 
ï®Continued research explores improvements in critic design, 
experience replay, and hybrid CTDE methods 
 
45 


## Page 46

Multi-Agent Reinforcement Learning 
1. Multi-Agent Reinforcement Learning 
2. Independent Q-Learning (IQL) 
3. Centralized Training with Decentralized Execution (CTDE) 
4. Multi-Agent Deep Deterministic Policy Gradient (MADDPG) 
5. Multi-Agent Proximal Policy Optimization (MAPPO) 
6. MARL Approaches in Multi-Player Games 
46 


## Page 47

PPO Foundations and Motivation 
ï®
PPO optimizes a clipped surrogate objective to ensure stable policy updates: 
ğ¿PPO ğœƒ= ğ¸ğ‘¡min ğ‘Ÿğ‘¡ğœƒğ´ğ‘¡
 , clip ğ‘Ÿğ‘¡ğœƒ, 1 âˆ’ğœ–, 1 + ğœ–ğ´ğ‘¡
 
 
ï®
The ratio ğ‘Ÿğ‘¡ğœƒ=
ğœ‹ğœƒğ‘ğ‘¡ğ‘ ğ‘¡
ğœ‹ğœƒold
ğ‘ğ‘¡ğ‘ ğ‘¡ controls the update magnitude 
ï®
Clipping prevents drastic policy changes, improving learning stability and 
preventing policy collapse 
ï®
PPO performs well in single-agent environments with stationary dynamics 
and localized state inputs 
ï®
Multi-agent settings introduce non-stationarity, shared rewards, and 
coordination needs that PPO does not address 
 
47 


## Page 48

Challenges in Multi-Agent PPO 
ï®Policy changes by one agent affect the environment observed by 
others, breaking stationarity assumptions 
ï®Assigning credit for joint rewards across multiple agents complicates 
policy evaluation 
ï®Coordinated behavior cannot emerge from independent agents 
optimizing in isolation 
ï®Multi-agent learning dynamics require centralized information for 
stable training signals 
ï®Direct application of PPO fails to handle joint interdependencies and 
temporal credit assignment 
 
48 


## Page 49

MAPPO and Centralized Training 
ï®MAPPO extends PPO using Centralized Training with Decentralized 
Execution (CTDE) principles 
ï®Each agent has a decentralized actor ğœ‹ğœƒğ‘–ğ‘ğ‘¡
ğ‘–
ğ‘œğ‘¡
ğ‘–, dependent on 
local observations only 
ï®A shared centralized critic evaluates joint action outcomes using 
global state and all agentsâ€™ actions 
ï®This structure addresses non-stationarity by conditioning value 
estimates on full joint context 
ï®During execution, agents act independently, ensuring practical 
deployment without communication 
 
49 


## Page 50

MAPPO Objective and Advantage 
ï®
Each agentâ€™s PPO-style objective is: 
ğ¿MAPPO ğœƒğ‘–= ğ¸ğ‘¡min ğ‘Ÿğ‘¡
ğ‘–ğœƒğ‘–ğ´ğ‘¡
ğ‘–
 , clip ğ‘Ÿğ‘¡
ğ‘–ğœƒğ‘–, 1 âˆ’ğœ–, 1 + ğœ–ğ´ğ‘¡
ğ‘–
 
 
ï®
The probability ratio is: 
ğ‘Ÿğ‘¡
ğ‘–ğœƒğ‘–=
ğœ‹ğœƒğ‘–ğ‘ğ‘¡
ğ‘–
ğ‘œğ‘¡
ğ‘–
ğœ‹ğœƒğ‘–,old ğ‘ğ‘¡
ğ‘–
ğ‘œğ‘¡
ğ‘– 
ï®
The advantage estimate uses centralized value functions: 
ğ´ğ‘¡
ğ‘–
 = ğ‘„ğ‘ ğ‘¡, ğ‘ğ‘¡
1, â€¦ , ğ‘ğ‘¡
ğ‘âˆ’ğ‘‰ğ‘ ğ‘¡ 
ï®
The critic evaluates how joint actions contribute to the reward, aiding credit 
assignment 
ï®
This advantage formulation promotes cooperation and improves learning signal 
quality 
 
50 


## Page 51

Key Differences from PPO 
ï®Critic Structure: PPO uses local or global value functions; MAPPO 
requires a centralized critic with joint input 
ï®Execution Mode: PPO acts through a single agent; MAPPO supports 
many agents acting independently 
ï®Non-Stationarity Handling: PPO assumes fixed dynamics; MAPPO 
uses centralized critics to stabilize multi-agent training 
ï®Coordination Capability: PPO optimizes isolated agents; MAPPO 
facilitates coordinated behavior through shared training signals 
ï®Stability and Efficiency: Both inherit PPOâ€™s clipped updates, but 
MAPPO enhances sample efficiency in cooperative environments 
 
51 


## Page 52

Advantages of MAPPO 
ï®MAPPO maintains PPOâ€™s policy stability via constrained updates and 
trust regions 
ï®Centralized critics yield consistent advantage estimates that reduce 
variance and improve convergence 
ï®Decentralized execution ensures real-world scalability and autonomy 
in runtime operation 
ï®The algorithm supports cooperation by aligning agent updates 
through globally informed feedback 
ï®MAPPO has demonstrated strong empirical performance in team-
based domains like multi-robot control 
 
52 


## Page 53

Remaining Challenges in MAPPO 
ï®The centralized critic grows in complexity with more agents and 
larger joint action/state spaces 
ï®The training-execution mismatch persists; actors may act 
suboptimally when deprived of training-time state access 
ï®Credit assignment remains difficult in tasks with delayed or sparse 
rewards, even with joint critic feedback 
ï®Learning stability depends on accurate advantage estimation, which 
hinges on critic design and global observability 
ï®Scalability and efficiency tradeoffs emerge when balancing global 
input richness with computational tractability 
 
53 


## Page 54

Use Cases and Applications 
ï®MAPPO performs well in robot soccer, where agents learn to pass, 
defend, and coordinate under shared objectives 
ï®Autonomous fleets (e.g., drone swarms, vehicle platoons) benefit 
from centralized training for strategic coordination 
ï®Multi-agent game environments like StarCraft use MAPPO to train 
units for collective combat and navigation 
ï®The algorithm allows teams to learn coordinated policies without 
requiring runtime communication 
ï®These applications highlight MAPPOâ€™s blend of safe policy updates, 
coordination, and practical deployment 
 
54 


## Page 55

Summary and Final Observations 
ï®MAPPO adapts PPOâ€™s stable updates to cooperative multi-agent 
settings via centralized critics and decentralized actors 
ï®It addresses core MARL problems: non-stationarity, credit 
assignment, and coordinated behavior 
ï®During training, full-state access enables better policy evaluation and 
advantage estimation 
ï®During execution, policies operate independently using only local 
observations 
ï®MAPPO balances robustness and practicality, making it a key method 
for scalable multi-agent learning 
 
55 


## Page 56

Multi-Agent Reinforcement Learning 
1. Multi-Agent Reinforcement Learning 
2. Independent Q-Learning (IQL) 
3. Centralized Training with Decentralized Execution (CTDE) 
4. Multi-Agent Deep Deterministic Policy Gradient (MADDPG) 
5. Multi-Agent Proximal Policy Optimization (MAPPO) 
6. MARL Approaches in Multi-Player Games 
56 


## Page 57

MARL in Real-Time Strategy Games 
ï®Real-time strategy games like StarCraft II and DotA 2 challenge 
multi-agent reinforcement learning systems 
ï®Agents manage resources, plan strategies, and control units in 
dynamic, partially observable, real-time environments 
ï®State and action spaces become enormous due to many units, 
abilities, maps, and game events 
ï®Long time horizons require reasoning about early economic choices 
and later large-scale battles 
ï®These properties motivate sophisticated deep RL and MARL 
algorithms for competitive and cooperative play 
57 


## Page 58

RTS Challenges for Reinforcement Learning 
ï®Agents process high-dimensional observations, including raw pixels 
or structured features like unit positions and health 
ï®Real-time control demands frequent action decisions while managing 
multiple units simultaneously 
ï®Multi-agent interactions require coordination among units or heroes 
for effective attacks, defenses, and objectives 
ï®Large action branching factors and long episodes challenge 
exploration and credit assignment 
ï®Deep reinforcement learning approximates complex policies that 
map rich game states to suitable actions 
58 


## Page 59

Hierarchical Learning in RTS Games 
ï®RTS decisions split naturally into macro strategy and micro-level unit 
control 
ï®Macro level handles economy, base expansion, unit production, and 
long-term strategic planning 
ï®Micro level manages precise movements, targeting, spell usage, and 
retreating during battles 
ï®Hierarchical reinforcement learning separates these levels to simplify 
learning and improve coordination 
ï®Systems like AlphaStar and OpenAI Five exploit macroâ€“micro 
structure for effective play 
59 


## Page 60

Centralized Training and Decentralized 
Execution 
ï®Centralized Training with Decentralized Execution (CTDE) addresses 
non-stationarity and coordination challenges 
ï®During training, agents share information and learn using a 
centralized critic observing the global game state 
ï®The critic evaluates joint actions and rewards, encouraging coherent 
multi-agent strategies 
ï®After training, each agent acts independently based only on its local 
observations 
ï®CTDE underpins both AlphaStar and OpenAI Five in their multi-
agent learning setups 
60 


## Page 61

AlphaStar for StarCraft II 
ï®AlphaStar combines supervised learning from human replays with 
reinforcement learning for StarCraft II 
ï®Initial imitation learning provides baseline strategies and tactics 
similar to expert human players 
ï®League-based multi-agent training creates agents specializing in 
diverse strategies, such as aggressive or defensive play 
ï®Self-play exposes agents to many strategies and counter-strategies 
across maps and situations 
ï®AlphaStar uses a multi-agent variant of Proximal Policy Optimization 
(PPO) for stable policy updates 
61 


## Page 62

OpenAI Five and Population-Based Training 
ï®OpenAI Five controls five DotA 2 heroes, each with unique abilities, 
against human teams 
ï®Agents learn through massive self-play, improving coordination and 
tactics over thousands of games 
ï®LSTM networks provide memory, enabling decisions under partial 
observability and long temporal dependencies 
ï®Training uses CTDE with a centralized critic plus careful reward 
shaping and hyperparameter tuning 
ï®Population-based training maintains diverse agents, promotes strong 
strategies, and improves robustness against varied opponents 
62 


## Page 63

Conclusions 
ï®
MARL extends single-agent RL to environments where multiple agents 
interact, cooperate, compete, and reshape each otherâ€™s dynamics 
ï®
Independent methods like IQL treat other agents as part of the 
environment, causing non-stationarity and limited coordination 
ï®
Centralized Training with Decentralized Execution stabilizes learning by 
training critics on global information while actors use local observations 
ï®
Actor-critic MARL algorithms such as MADDPG and MAPPO enable 
coordination in continuous or cooperative tasks with shared rewards 
ï®
Self-play, hierarchical control, and population-based training achieved 
superhuman performance in complex games like StarCraft II and DotA 2 
63 
