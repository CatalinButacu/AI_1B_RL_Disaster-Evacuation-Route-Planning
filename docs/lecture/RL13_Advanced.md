# RL13_Advanced

**Source:** c:\Users\catalin.butacu\Downloads\RL\resourses\lecture\RL13_Advanced.pdf

**Pages:** 43

---


## Page 1

Reinforcement Learning 
13. Advanced Topics in Reinforcement Learning 
 
Florin Leon 
 
â€œGheorghe Asachiâ€ Technical University of IaÈ™i, Romania 
Faculty of Automatic Control and Computer Engineering 
 
https://florinleon.byethost24.com/lect_rl.html 
 
2025 
 


## Page 2

Advanced Topics in RL 
1. Imitation Learning, RL from Human Feedback  
2. Offline RL 
3. Safe RL 
4. Hierarchical RL 
 
 
2 


## Page 3

Imitation learning and RLHF: overall goal 
ï®Both methods steer agents toward human-like, decent behavior 
ï®Imitation learning copies expert actions in observed states 
ï®RLHF learns what humans like and optimizes for that objective 
ï®Imitation focuses on actions; RLHF focuses on outcomes and 
preferences 
ï®Both rely on the same underlying policy machinery 
3 


## Page 4

Behavior cloning: basic imitation setup 
ï®There is an expert policy acting in the environment, possibly human 
or learned 
ï®We record demonstration data as (s, a) pairs from expert trajectories 
ï®States s describe situations; actions a are expert choices 
ï®Goal: learn policy ğœ‹ğœƒğ‘
ğ‘  that matches expert behavior 
ï®Central question: â€œIn these expert-like states, what would the expert 
do?â€ 
4 


## Page 5

Behavior cloning as supervised learning 
ï®Treat (s, a) pairs exactly like labeled supervised examples 
ï®Learn ğœ‹ğœƒğ‘
ğ‘  with standard supervised training: 
ğœ‹ğœƒğ‘
ğ‘ 
â‰ˆexpertâ€™s action distribution 
ï®For discrete actions, use cross-entropy loss with softmax over actions 
ï®For continuous actions, use mean squared error on action vectors 
ï®Network views states as inputs and expert actions as labels 
5 


## Page 6

Behavior cloning: lack of rollout awareness 
ï®Supervised loss ignores downstream consequences of actions 
ï®The model does not ask what happens several steps after an action 
ï®A locally correct action may still lead to future failure 
ï®Horizon length matters; one-step errors can cascade over time 
ï®This blind spot motivates analysis of covariate shift 
6 


## Page 7

Covariate shift and distribution mismatch 
ï®Training states come from expert trajectories near a â€œgood manifoldâ€ 
in state space 
ï®Deployed policy is imperfect; small action errors occur 
ï®These errors move the system into states absent from the 
demonstrations 
ï®Policy predictions degrade further on these unfamiliar states 
ï®This mismatch in state distributions is covariate shift 
7 


## Page 8

Compounding errors and stability issues 
ï®Small initial imitation errors appear at early time steps 
ï®Subsequent states drift further from expert trajectories 
ï®The agent eventually reaches highly unfamiliar, poorly modeled 
regions 
ï®Performance can collapse over long horizons despite low per-step 
error 
ï®Robotics often experiences drift and awkward poses from this 
phenomenon 
8 


## Page 9

Fixes for distribution drift (high-level) 
ï®Dataset aggregation brings in states visited by the learned policy, for 
example DAgger 
ï®Regularization keeps the learned policy near behavior in the 
demonstration dataset 
ï®Control-theoretic methods impose explicit stability conditions on the 
closed-loop system 
ï®Despite fixes, core limitation remains: supervised imitation ignores 
feedback from its own errors 
ï®Future state distribution depends on the learned policy, not the 
expert 
9 


## Page 10

RLHF motivation: beyond low-level action 
labels 
ï®Humans may handle tasks with complex long-term trade-offs poorly 
at the action level 
ï®We care about fuzzy goals: helpfulness, politeness, safety, avoidance 
of harm 
ï®Many environments lack clean, trustworthy reward signals 
ï®Handcrafted rewards often produce bad incentives or unintended 
behaviors 
ï®RLHF instead asks humans which trajectories or outputs they prefer 
10 


## Page 11

Preference data and reward modeling 
ï®Collect comparisons: context x with two outputs ğ‘¦better , ğ‘¦worse 
ï®Each data point: humans prefer ğ‘¦better over ğ‘¦worse for context x 
ï®Train reward model ğ‘Ÿğœ™ğ‘¥, ğ‘¦ using a Bradley-Terry style likelihood: 
ğ‘ƒprefer ğ‘¦better = ğœğ‘Ÿğœ™ğ‘¥, ğ‘¦better âˆ’ğ‘Ÿğœ™ğ‘¥, ğ‘¦worse
 
ï®Architecture usually reuses policy backbone with a scalar reward 
head 
ï®Resulting reward model predicts how much humans like each output 
11 


## Page 12

RL on learned reward and relationship to 
imitation 
ï®RL phase: sample trajectories, score with ğ‘Ÿğœ™, update policy via PPO 
ï®PPO objective often includes a KL penalty to stay near a base 
supervised policy 
ï®KL penalty limits reward-model exploitation and preserves safe, 
useful behavior 
ï®Practical pipeline: pretraining, supervised imitation, then RLHF fine-
tuning 
ï®Imitation copies expert behavior in-distribution; RLHF optimizes 
human-shaped objectives under distribution shift 
12 


## Page 13

Offline reinforcement learning: core idea 
ï®Offline RL learns a policy from a fixed dataset of past experience only 
ï®The algorithm receives the dataset upfront and collects no additional 
transitions 
ï®No exploration, no â€œtry it and seeâ€; interaction with environment is 
forbidden 
ï®Goal: extract a high-performing policy under this strict no-new-
samples regime 
ï®Attractive for domains where online experimentation is expensive or 
unsafe 
13 


## Page 14

Offline vs standard off-policy RL 
ï®Standard off-policy RL also learns from replayed past experience 
ï®However, it still interacts with the environment while training 
ï®The agent eventually tries poorly supported actions and observes bad 
returns 
ï®These new samples correct over-optimistic Q-values for risky actions 
ï®Offline RL removes this safety net; the replay buffer becomes the 
entire world 
14 


## Page 15

Behavior policy, learned policy, and 
distribution shift 
ï®Dataset arises from a behavior policy ğ›½ğ‘
ğ‘ , often unknown 
ï®Offline RL seeks a new policy ğœ‹ğ‘
ğ‘  that outperforms ğ›½ 
ï®State-action pairs in data follow ğ›½; ğœ‹ may choose very different 
actions 
ï®Result: distribution shift between visitation distributions of ğœ‹ and ğ›½  
ï®Dataset typically covers a small subset of all possible state-action 
pairs 
15 


## Page 16

Out-of-distribution actions and 
extrapolation 
ï®Actions outside dataset support are out-of-distribution (OOD) 
relative to the logged data 
ï®Online RL explores OOD actions and learns their consequences from 
experience 
ï®Offline RL must estimate values for OOD actions without observing 
real outcomes 
ï®Function approximators extrapolate Q-values from nearby in-
distribution samples 
ï®Extrapolation errors in these unsupported regions create major 
offline RL difficulties 
16 


## Page 17

NaÃ¯ve Q-learning and the deadly triad 
ï®Standard Q-learning update uses a bootstrapped target:  
target = ğ‘Ÿ+ ğ›¾max
ğ‘â€² ğ‘„ğ‘ â€², ğ‘â€²  
ï®Bootstrapping updates Q(s, a) using Q-values at the next state s' 
ï®Off-policy learning with function approximation already risks 
instability or divergence 
ï®This combination is the â€œdeadly triadâ€: function approximation, 
bootstrapping, off-policy updates 
ï®Offline RL intensifies these issues because the dataset remains fixed 
17 


## Page 18

Feedback loop from hallucinated high  
Q-values 
ï®Dataset contains only actions actually taken; backups still maximize 
over all possible actions 
ï®Function approximation can assign huge Q-values to unseen actions 
at state s' 
ï®No datapoint contradicts these fantasy values, so they persist 
ï®The max operator repeatedly selects these inflated values in targets 
ï®Over time, optimism spreads through Q-values and yields disastrous 
greedy policies 
18 


## Page 19

Pessimism principle in offline RL 
ï®Modern offline RL adopts deliberate pessimism about poorly 
supported actions 
ï®If data coverage seems weak, algorithms underestimate that actionâ€™s 
value 
ï®Pessimism discourages the policy from assigning probability to OOD 
actions 
ï®Policies remain near regions where Q-values reflect actual experience 
ï®Offline RL trades potential optimality for robustness against 
extrapolation errors 
19 


## Page 20

Policy constraints toward the behavior 
policy 
ï®Policy-constrained methods restrict ğœ‹ from deviating far from 
behavior policy ğ›½ 
ï®Imitation-style regularization adds objectives that keep ğœ‹ğ‘
ğ‘  
close to ğ›½ğ‘
ğ‘  
ï®Examples include KL divergence penalties or additional behavior 
cloning losses 
ï®Generative models approximate behaviorâ€™s action distribution; 
policies choose among sampled candidate actions 
ï®Hard support constraints forbid low-probability actions or project 
policies back into data support 
20 


## Page 21

Conservative Q-functions and uncertainty-
based pessimism 
ï®Conservative Q-learning (CQL) augments Bellman loss with penalties 
for high Q-values off the dataset 
ï®Objective encourages higher Q-values on in-dataset actions, lower 
values on broad action samples 
ï®Greedy policy then prefers actions well supported by the logged data 
ï®Other methods estimate uncertainty using Q-ensembles or Bayesian-
style approximations 
ï®Effective pessimistic value:  
ğ‘„pess ğ‘ , ğ‘= ğ‘„ ğ‘ , ğ‘âˆ’ğœ†â‹…uncertainty ğ‘ , ğ‘ 
21 


## Page 22

Offline RL as off-policy RL with principled 
fear 
ï®Offline RL removes exploration, so overestimated values rarely 
receive corrective feedback 
ï®Behavior policy defines the support where data-based estimates are 
trustworthy 
ï®NaÃ¯ve off-policy Q-learning propagates hallucinated values from 
unsupported regions 
ï®Modern methods constrain policies or adjust Q-values to encourage 
conservative behavior 
ï®Offline RL becomes â€œoff-policy RL with principled fearâ€ of actions 
absent from the dataset 
22 


## Page 23

Safe RL: core tension 
ï®Exploration drives agents toward high reward in unknown 
environments 
ï®Real systems impose â€œdo not crash, melt, or bankruptâ€ constraints 
ï®Safe RL studies reward hacking, hard safety constraints, and 
robustness 
ï®Focus lies on systems deployed in changing, imperfectly modeled 
worlds 
ï®Safety becomes a first-class design concern, not an afterthought 
23 


## Page 24

Reward hacking and specification gaming: 
definition 
ï®Agents optimize the provided reward, not designersâ€™ intentions 
ï®Misaligned reward functions invite clever shortcuts and loopholes 
ï®Reward hacking or specification gaming describes this behavior 
ï®Optimization targets proxy metrics rather than true human goals 
ï®Goodhartâ€™s law: targeted measures stop reflecting the underlying 
objective 
24 


## Page 25

Reward hacking: gaming proxy rewards 
ï®CoastRunners agent maximized points by circling respawning targets 
in a lagoon 
ï®Finishing the race became irrelevant; score already maximized 
ï®DeepMind racing agent spun around green blocks for shaping reward 
ï®In both cases, proxies stopped tracking â€œwin the raceâ€ 
ï®Optimization faithfully followed reward while violating designersâ€™ 
intent 
25 


## Page 26

Reward hacking in safety-critical domains 
ï®Recommendation systems maximizing clicks may promote harmful 
or sensational content 
ï®Trading agents maximizing profit without risk terms can take 
catastrophic bets 
ï®Robots with sparse success rewards may slam into walls without 
collision penalties 
ï®Safe RL treats such failures as central, not anecdotal 
ï®Two questions emerge: expressing constraints and handling 
distribution shift 
26 


## Page 27

CMDPs: rewards, costs, and safety budgets 
ï®Constrained MDPs split performance and safety into reward and cost 
ï®At time t: reward rt, cost ct for unsafe or resource usage 
ï®Example 1: ct = 1 on collision, otherwise 0 
ï®Example 2: ct equals energy consumption or distance to humans 
ï®Optimization problem:  
max
ğœ‹ğ¸
 ğ‘Ÿğ‘¡
ğ‘¡
â€ƒs.t.â€ƒğ¸ ğ‘ğ‘¡
ğ‘¡
â‰¤ğ‘‘ 
27 


## Page 28

Interpreting costs and budgets 
ï®Reward measures task performance quality 
ï®Cost measures safety budget usage or risk exposure 
ï®Budget d encodes maximum acceptable expected cumulative cost 
ï®Costs can represent forbidden regions, constraint violation counts, or 
risk measures 
ï®Frameworkâ€™s usefulness depends heavily on well-chosen cost 
definitions 
28 


## Page 29

Lagrangian methods for safe RL 
ï®Lagrangian approach converts constraint into a single unconstrained 
objective 
ï®Objective: 
ğ¸ ğ‘Ÿğ‘¡
ğ‘¡
ğœ†ğ¸ ğ‘ğ‘¡
ğ‘¡
âˆ’ğ‘‘ 
ï®Multiplier ğœ† penalizes policies exceeding the safety budget 
ï®Algorithms update policy and ğœ† together during learning 
ï®Safety appears as a separate channel, not folded into reward 
29 


## Page 30

Shielding, safe sets, and action filters 
ï®Shielding methods learn or specify a safe state set 
ï®Exploration and control remain inside this safe region 
ï®Action filters intercept proposed actions before execution 
ï®Unsafe actions are modified or replaced to satisfy constraints 
ï®Constraints apply during training and deployment, though emphasis 
may differ 
30 


## Page 31

Robustness: distribution shift in RL 
ï®Deployment environment often differs from the training 
environment 
ï®Dynamics can change: friction, masses, delays, or contact properties 
ï®Observations can change: sensor degradation, lighting, new obstacles 
ï®Other agents can adapt, altering interaction patterns in multi-agent 
settings 
ï®Policy-induced trajectories may differ from behavior policy 
trajectories, causing additional shift 
31 


## Page 32

Robust RL strategies and the three lenses 
ï®Domain randomization trains across varied parameters to improve 
transfer 
ï®Adversarial training introduces worst-case perturbations within 
bounded sets 
ï®Distributionally robust objectives optimize worst-case performance 
over nearby distributions 
ï®Safe RL unifies three lenses: reward hacking, constraints, and 
robustness 
ï®Overall goal: clear objectives, explicit safety limits, and resilience 
when the world shifts 
32 


## Page 33

Hierarchical RL: idea and motivation 
ï®HRL breaks big tasks into smaller skills and stitches skills into full 
solutions 
ï®Flat RL chooses primitive actions every step, for example â€œmove 
north/south/east/westâ€ 
ï®Long horizons make flat policies slow to learn and fragile to credit 
assignment 
ï®HRL separates high-level decisions (â€œgo to elevatorâ€) from low-level 
control (â€œwalk, turn, stopâ€) 
ï®Goal: reason in skills and subtasks rather than treating every time 
step as identical 
33 


## Page 34

Temporal abstraction and options 
ï®Options generalize actions to multi-step skills 
ï®Each option ğœ” has an initiation set ğ¼ğœ” of valid starting states 
ï®Intra-option policy ğœ‹ğœ”ğ‘
ğ‘  chooses primitive actions while option 
runs 
ï®Termination condition ğ›½ğœ”ğ‘  gives probability that option stops in 
state s 
ï®Primitive actions are degenerate one-step options that always 
terminate immediately 
34 


## Page 35

Meta-policy and SMDP view 
ï®A meta-policy ğœ‡ğœ”
ğ‘  selects which option to start in each state 
ï®Option value function: 
ğ‘„ğœ‡ğ‘ , ğœ”= expected return from ğ‘  using ğœ” then ğœ‡ 
ï®Decisions occur at option boundaries, not every primitive time step 
ï®The resulting process is a semi-Markov decision process (SMDP) 
ï®HRL compresses long trajectories into fewer, semantically 
meaningful jumps 
35 


## Page 36

Benefits and downsides of hierarchy 
ï®Sample efficiency: reuse skills such as â€œgo to doorâ€ across episodes 
and tasks 
ï®Long-term credit assignment improves when credit attaches to 
subgoals like â€œget keyâ€ 
ï®Structured exploration uses â€œtry a different skillâ€ instead of random 
primitive noise 
ï®Transfer: skills like â€œgrasp objectâ€ or â€œstand upâ€ move across tasks 
and environments 
ï®Downsides: discovering good subgoals is hard and fixed hierarchies 
can be suboptimal 
36 


## Page 37

Subgoal discovery strategies 
ï®Bottleneck states: door cells or chokepoints lie on many successful 
trajectories 
ï®Graph analyses identify high â€œbetweennessâ€ states and declare them 
subgoals 
ï®State abstraction and clustering define regions; boundaries between 
regions become subgoals 
ï®Intrinsic motivation rewards novel or interesting states, so consistent 
events become subgoals 
ï®Goal-conditioned values V(s, g) (UVFA) treat goals as inputs and 
yield goal-conditioned skills 
37 


## Page 38

MAXQ: value decomposition by task tree 
ï®MAXQ starts from a human-designed task hierarchy, not explicit 
options 
ï®Root task solves the full problem, for example â€œdeliver passengerâ€ in 
Taxi 
ï®Subtasks handle pieces: â€œnavigate to passengerâ€, â€œpick upâ€, â€œnavigate 
to destinationâ€, â€œdrop offâ€ 
ï®Each subtask is an MDP with its own termination condition and value 
function 
ï®Global value splits into current subtask value plus value after subtask 
completion, enabling reuse 
38 


## Page 39

HAMs and bottleneck-based options 
ï®Hierarchical Abstract Machines (HAMs) encode behavior as finite-
state controllers 
ï®Nodes represent modes like â€œsearch corridorâ€ or â€œsearch keyâ€; edges 
specify transitions 
ï®Some nodes output primitive actions, others invoke lower-level 
machines 
ï®Bottleneck-based methods analyze random trajectories to find states 
lying â€œbetweenâ€ many others 
ï®These bottlenecks become subgoals; corresponding options learn â€œgo 
to bottleneck iâ€ 
39 


## Page 40

Feudal networks and h-DQN 
ï®Feudal networks use a managerâ€“worker separation 
ï®Manager observes abstract representations and outputs goal vectors 
in latent space 
ï®Workers receive state and goal vector, then output primitive actions 
to move toward that goal 
ï®Hierarchical DQN (h-DQN) uses a meta-controller choosing subgoal g 
and a goal-conditioned DQN 
ï®Controller gets intrinsic reward for achieving g; meta-controller gets 
environment reward 
40 


## Page 41

HIRO, Option-Critic, and HAC 
ï®HIRO sets continuous subgoal states for a lower-level policy 
ï®Replay relabels high-level actions with subgoals actually achieved, 
stabilizing off-policy learning 
ï®Option-Critic parameterizes intra-option policies, terminations, and 
option-selection with neural networks 
ï®Hierarchical Actor-Critic (HAC) stacks actorâ€“critic levels, each setting 
state goals for the lower level 
ï®HAC uses hindsight relabeling and subgoal testing penalties to handle 
non-stationarity between levels 
41 


## Page 42

Environments and mental model 
ï®Four-room grid worlds: doors as subgoals, options as â€œgo to door iâ€ 
skills 
ï®Montezumaâ€™s Revenge: subgoals like â€œget keyâ€ or â€œreach doorâ€ solve 
sparse reward exploration 
ï®Robotic manipulation and locomotion: reusable skills for â€œreachâ€, 
â€œpickâ€, â€œplaceâ€, â€œopen drawerâ€ 
ï®Multi-agent domains: team strategies on top, individual tactics below 
fit hierarchical control 
ï®Big picture: HRL replaces direct state-to-action mapping with 
reasoning over subgoals, options, and skills 
42 


## Page 43

Conclusions 
ï®
Imitation learning and RLHF steer agents toward human-aligned 
behavior; imitation copies actions, RLHF optimizes learned preference-
based reward signals 
ï®
Offline RL learns policies from fixed logged data, and enforces 
pessimism for unsupported actions to avoid harmful extrapolation 
ï®
Safe RL formalizes constraints and costs, tackles reward hacking, and 
seeks robust performance under distribution shift 
ï®
Hierarchical RL introduces temporal abstraction through skills and 
subgoals, improving exploration, sample efficiency, and long-horizon 
credit assignment 
43 
