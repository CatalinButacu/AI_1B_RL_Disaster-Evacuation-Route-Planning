# RL06_FnApprox

**Source:** c:\Users\catalin.butacu\Downloads\RL\resourses\lecture\RL06_FnApprox.pdf

**Pages:** 95

---


## Page 1

Reinforcement Learning 
6. Function Approximation 
 
Florin Leon 
 
â€œGheorghe Asachiâ€ Technical University of IaÈ™i, Romania 
Faculty of Automatic Control and Computer Engineering 
 
https://florinleon.byethost24.com/lect_rl.html 
 
2025 
 


## Page 2

Function Approximation 
1. On-Policy Prediction with Approximation 
 
1.1. Value-Function Approximation  
 
1.2. Stochastic-Gradient and Semi-Gradient Methods 
 
1.3. Approximation with Linear Methods  
 
1.4. Feature Construction (Polynomials, Coarse Coding, Tile Coding, RBF) 
 
1.5. Approximation with Neural Networks  
 
1.6. Approximation with Memory-Based Methods 
2. On-Policy Control with Approximation  
 
2.1. Episodic Semi-Gradient Control  
 
2.2. Average Reward for Continuing Tasks  
 
2.3. The Deadly Triad  
 
 
2 


## Page 3

Function Approximation 
1. On-Policy Prediction with Approximation 
 
1.1. Value-Function Approximation  
 
1.2. Stochastic-Gradient and Semi-Gradient Methods 
 
1.3. Approximation with Linear Methods  
 
1.4. Feature Construction (Polynomials, Coarse Coding, Tile Coding, RBF) 
 
1.5. Approximation with Neural Networks  
 
1.6. Approximation with Memory-Based Methods 
2. On-Policy Control with Approximation  
 
2.1. Episodic Semi-Gradient Control  
 
2.2. Average Reward for Continuing Tasks  
 
2.3. The Deadly Triad  
 
 
3 


## Page 4

Memory and Generalization 
ï®So far, we have considered the tabular case 
ï®Memory requirements are high when the number of states is large 
ï®It is impossible to reuse information about one state for non-
neighboring states 
ï®Instead, we want good approximate solutions that require only 
limited computational resources 
ï®We need to generalize from previous encounters with states that are, 
in some sense, similar to the current one 
ï®We can obtain such generalization with function approximation, 
often based on supervised learning 
4 


## Page 5

Example: Pacman 
5 


## Page 6

Large Scale RL 
ï®Reinforcement learning has been used to solve large problems 
ï®Backgammon: 1020 states 
ï®Go: 10170 states 
ï®Helicopter flight: continuous state space 
 
6 


## Page 7

Value Function Approximation 
ï®Value function approximation (VFA) replaces the table with a 
general, usually parameterized, form 
7 


## Page 8

Value Function Approximation 
ï®
When we update the parameters, the value estimates of many states 
change simultaneously 
ï®
Typically, the number of parameters is much smaller than the number of 
states 
ï®
Changing one parameter changes the estimated values of many states 
(generalization) 
ï®
Generalization makes RL more powerful but also more difficult to 
manage and understand 
ï®
Extending RL with function approximation makes it applicable to 
partially observable problems, where the full state is not available 
ï®
In some models (including linear models and neural networks), the 
parameters are weights. Both Î¸ and w are commonly used as notations 
8 


## Page 9

Function Approximation in RL 
ï®Many supervised methods can approximate functions, such as linear 
models, neural networks or decision trees 
ï®RL needs online methods that learn from data that arrives 
incrementally during interaction 
ï®RL requires function approximators that handle changing target 
functions over time 
ï®Methods that rely on static datasets or fixed targets are usually 
unsuitable for RL 
ï®In RL, the type of supervised function approximation is regression  
(state â†’ target value), not classification 
9 


## Page 10

The Objective Function 
ï®In the tabular case, a continuous measure of prediction quality is not 
necessary because the learned value function can become equal to the 
true one, and updates affect only single states 
ï®With function approximation, updating one state value affects many 
others 
ï®More states than weights implies trade-offs: improving one state 
worsens others 
ï®A state distribution Î¼(s) specifies how much we care about error in 
each state (Î¼(s) is the normalized number of visits to s) 
ï®A natural objective function is the mean squared error between 
values â€“ the mean squared value error: 
10 


## Page 11

Gradient Methods 
ï®Minimizing VE requires selecting a function approximator and 
optimization strategy suited to the RL context 
ï®Linear gradient methods allow analysis and often guarantee 
convergence to the global VE minimum 
ï®Nonlinear methods lack convergence guarantees and may require 
careful tuning or constraints 
11 


## Page 12

GD vs. SGD 
ï®Gradient descent (GD) assumes access to all states or full training 
data for each update 
ï®It computes an exact objective gradient, then takes one step in the 
true direction 
ï®Stochastic gradient descent (SGD) updates from single samples or 
mini-batches, not the whole distribution 
ï®Each stochastic update uses a noisy gradient estimate that 
approximates the true gradient 
ï®RL agents interact online and data arrives sequentially, thus SGD is a 
natural choice here 
ï®SGD approximates GD over time 
12 


## Page 13

SGD in RL 
13 


## Page 14

SGD in RL 
14 


## Page 15

The Target 
15 


## Page 16

16 


## Page 17

Gradient Monte Carlo Algorithm 
ï®Each episode provides full return Gtâ€‹ as an unbiased training target 
for each visited state 
ï®Update rule: 
 
 
ï®Suitable for episodic tasks where full returns can be observed and 
stored 
ï®High variance of returns makes MC slower than bootstrapping 
methods in some cases 
 
[
(
,
)]
(
,
)
t
t
w
t
w
w
G
v S w
v S w
ï¡
ï‚¬
ï€«
ï€­
ïƒ‘
17 


## Page 18

ï®True value           is often unknown; we use sample-based targets Ut 
ï®Monte Carlo target Ut = Gtâ€‹ is unbiased: 
ï®Bootstrapped targets, e.g.,                              depend on current 
weights and are biased 
ï®Substituting such biased targets breaks the true gradient descent 
nature of the update 
ï®These updates are called semi-gradient methods 
 
Semi-Gradient Methods (for Bootstrapping) 
[
|
]
( )
t
t
G
S
s
v
s
ï°
ï€½
ï€½
1
1
(
,
)
t
t
t
R
v S
w
ï§
ï€«
ï€«
ï€«
( )
v
s
ï°
18 


## Page 19

19 


## Page 20

Convergence 
20 


## Page 21

Function Approximation 
1. On-Policy Prediction with Approximation 
 
1.1. Value-Function Approximation  
 
1.2. Stochastic-Gradient and Semi-Gradient Methods 
 
1.3. Approximation with Linear Methods  
 
1.4. Feature Construction (Polynomials, Coarse Coding, Tile Coding, RBF) 
 
1.5. Approximation with Neural Networks  
 
1.6. Approximation with Memory-Based Methods 
2. On-Policy Control with Approximation  
 
2.1. Episodic Semi-Gradient Control  
 
2.2. Average Reward for Continuing Tasks  
 
2.3. The Deadly Triad  
 
 
21 


## Page 22

Linear Function Approximation 
22 


## Page 23

Gradients for Linear Models 
23 


## Page 24

Convergence 
24 


## Page 25

Convergence 
ï®Linear function approximators have a unique global optimum or a 
flat set of equivalent optima 
ï®Gradient Monte Carlo converges to the global minimum of VE if the 
step size Î± decreases appropriately 
ï®Semi-gradient TD(0) converges under linear function approximation, 
but not to the global minimum of VE 
ï®TD(0) converges to a fixed point close to the global minimum 
   
ğ‘‰ğ¸(ğ°ğ‘‡ğ·) â‰¤
1
1 âˆ’ğ›¾min
ğ°ğ‘‰ğ¸(ğ°) 
  
ï®Often Î³ is close to 1, but TD methods still work well in practice 
25 


## Page 26

Example 
MDP: 
Linear model: 
26 


## Page 27

Program 
ï®1 Simple Grid â†’ LinearTDAgent.py 
ï®with 2 policies, default reward â€“0.04 
 
ï®Policy 1  
ï®up, up, right, right, right, then random 
ï®If the number of steps exceeds 20, terminate with 0 
Learned weight vector w for policy 1: [-0.306, 0.127, 0.275] 
 
Approximate value function over the grid for policy 1: 
0.65    0.78    0.90    [1] 
0.37    [###]   0.63    [-1] 
0.10    0.22    0.35    0.48 
27 


## Page 28

Program 
ï®1 Simple Grid â†’ LinearTDAgent.py 
ï®with 2 policies, default reward â€“0.04 
 
ï®Policy 2  
ï®right, right, up, up, right, then random 
ï®If the number of steps exceeds 20, terminate with 0 
Learned weight vector w for policy 2: [-0.605, -0.038, 0.148] 
 
Approximate value function over the grid for policy 2: 
-0.20   -0.24   -0.28   [1] 
-0.35   [###]   -0.42   [-1] 
-0.50   -0.53   -0.57   -0.61 
28 


## Page 29

Function Approximation 
1. On-Policy Prediction with Approximation 
 
1.1. Value-Function Approximation  
 
1.2. Stochastic-Gradient and Semi-Gradient Methods 
 
1.3. Approximation with Linear Methods  
 
1.4. Feature Construction (Polynomials, Coarse Coding, Tile Coding, RBF) 
 
1.5. Approximation with Neural Networks  
 
1.6. Approximation with Memory-Based Methods 
2. On-Policy Control with Approximation  
 
2.1. Episodic Semi-Gradient Control  
 
2.2. Average Reward for Continuing Tasks  
 
2.3. The Deadly Triad  
 
 
29 


## Page 30

Feature Construction 
ï®Especially with simple approximation models, like linear ones, direct 
state encoding may not offer enough information for problem 
representation and generalization 
ï®Constructed features can turn a simple linear model into a nonlinear 
approximator over the original state space 
ï®Constructed features may include domain knowledge 
ï®A state can be represented by a feature vector, e.g.: 
ï®The distance of a robot from some landmarks 
ï®Piece configurations in backgammon 
 
30 


## Page 31

Polynomials 
ï®Some problems have states with numeric dimensions such as s1 and 
s2. Using only (s1, s2) ignores interactions and yields value 0 when 
both are 0 
ï®A feature vector like (1, s1, s2, s1 Â· s2) adds an intercept and another 
term that can help capture interactions between dimensions 
ï®Adding more polynomial features approximates complex interactions 
while the model remains linear in weights 
ï®Polynomial models can be useful for linear model approximation, but 
less useful for neural networks 
31 


## Page 32

Example: Pole Balancing 
ï®In the pole balancing task, high angular velocity can be either good or 
bad depending on the angle 
ï®A linear value function cannot represent this if these features are 
coded separately for the angle and the angular velocity 
32 


## Page 33

Example: Mountain Car 
33 


## Page 34

Program: 2 Feature Construction â†’ 1 Poly.py 


## Page 35

Coarse Coding 
ï®Coarse coding uses overlapping features 
ï®Each binary feature is active if the state falls inside a predefined 
receptive field 
ï®A state is represented by the set of features whose regions contain it 
ï®Feature overlap leads to generalization across states, based on shared 
active features 
ï®Binary features simplify representation: active features are 1, inactive 
features are 0 
35 


## Page 36

Example 
ï®
Generalization from state s to state s' depends on the number of their 
features whose receptive fields (in this case, circles) overlap 
ï®
These states have one feature in common, so there will be slight 
generalization between them 
36 


## Page 37

Generalization in Coarse Coding 
ï®Small receptive fields cause narrow generalization 
ï®Large fields produce broader generalization 
ï®Feature shape and orientation (e.g., circles, ellipses) influence the 
pattern and direction of generalization 
ï®Carefully selected receptive fields allow precise control over locality 
and extent of generalization 
37 


## Page 38

Form Impacts Generalization 
38 


## Page 39

Example 
39 
explanations in the next slide 


## Page 40

Example 
ï®
An example of the strong effect of a feature width on initial generalization 
(first row) and weak effect on asymptotic accuracy (last row)  
ï®
A 1D square wave function is learned using linear function approximation 
with coarse coding 
ï®
States are represented by overlapping 1D interval features of three widths 
ï®
All 3 setups use about 50 features across the range and randomly sampled 
training examples 
ï®
Broad receptive fields give wide generalization and smooth updates over 
many states early 
ï®
Narrow receptive fields change only nearby states, so the learned function 
looks bumpier initially 
ï®
Eventually, all receptive field widths yield similar final approximations 
ï®
Shape mainly affects generalization behavior 
40 


## Page 41

Program: 2 Feature Construction â†’ 2 Coarse.py 
41 


## Page 42

Tile Coding 
ï®Tile coding uses multiple tilings of the state space, each partitioned 
into non-overlapping tiles 
ï®It may be the most practical feature representation  
ï®A state activates one tile per tiling, resulting in a sparse binary 
feature vector 
ï®Multiple tilings with offsets allow overlapping receptive fields for 
generalization 
ï®With n tilings, exactly n features are active per state, regardless of 
state location 
ï®Each tile corresponds to one component of the weight vector used in 
value approximation 
42 


## Page 43

Example 
43 
The feature vector x(s) has one component for each tile in each tiling. 
Here, there are 4 Ã— 4 Ã— 4 = 64 components, all of which will be 0 except for the four 
corresponding to the tiles that s falls within. 
 


## Page 44

Advantages of Tile Coding 
ï®Feature representation is consistent and enables uniform learning 
ï®The overall number of features that are active at one time is the same for any 
state 
ï®Exactly one feature is present in each tiling, so the total number of features 
present is always equal to the number of tilings 
ï®Tile coding supports high resolution learning with manageable 
computational cost 
ï®Sparse binary features allow fast computation and efficient updates via index 
lookups 
ï®Learning rate can be scaled with tiling count: step size Î± = 1 / nâ€‹ yields 
exact one-step learning, i.e., ğ‘£ (ğ‘†ğ‘¡, ğ‘¤ğ‘¡) becomes target Ut in one step 
ï®Slower rates are possible, e.g., Î± = 1 / 10n 
 
 
44 


## Page 45

Uniform Offsets 
45 


## Page 46

Asymmetrical Offsets 
ï®
Asymmetrical offsets are preferred in tile coding 
ï®
If the tilings are uniformly offset, then there are diagonal artifacts and 
substantial variations in the generalization 
ï®
With asymmetrically offset tilings, the generalization is more spherical and 
homogeneous 


## Page 47

Offset Recommendation 
ï®For a continuous space of dimension k, a good choice is to use the 
first odd integers (1, 3, 5, 7, . . . , 2k â€“ 1), with the number of tilings n 
set to an integer power of 2 greater than or equal to 4k 
ï®In the previous figure: k = 2, n = 23 â‰¥ 4k, and displacement vector  
(1, 3) 
ï®In 3D, the first four tilings would be offset in total from a base 
position by (0, 0, 0), (1, 3, 5), (2, 6, 10), and (3, 9, 15) 
47 


## Page 48

Program: 2 Feature Construction â†’ 3 Tile.py 


## Page 49

Example: Random Walk 
ï®
The space of 1000 states is treated as a single continuous dimension, 
covered with tiles each 200 states wide 
ï®
The multiple tilings are offset from each other by 4 states 
ï®
The step-size parameter is set so that the initial learning rate in the two 
cases is the same, Î± = 0.0001 for the single tiling and Î± = 0.0001 / 50 for the 
50 tilings 
Learning curves on the 1000-state 
random walk example for the gradient 
MC algorithm with a single tiling and 
with multiple tilings 


## Page 50

Radial Basis Functions (RBFs) 
ï®
RBF features vary continuously from 1 at center to 0 as distance increases 
(Gaussian shape) 
ï®
RBF networks produce smooth, differentiable approximations over 
continuous input spaces. 
ï®
RBFs support local generalization tuned by width Ïƒ 
50 
1D RBF 


## Page 51

Discussion 
ï®RBFs produce approximate functions that vary smoothly and are 
differentiable 
ï®However, in most cases this has no practical significance  
ï®The computational cost of RBFs is higher due to exponentials 
ï®Tile coding has better performance in high-dimensional cases 
51 


## Page 52

Program: 2 Feature Construction â†’ 4 Rbf.py 
52 


## Page 53

Generalization Structure 
ï®Tile/coarse coding 
ï®Local overlapping regions â‡’ controlled, local generalization 
ï®A weight update at one state affects nearby states that share active tiles 
ï®RBFs 
ï®Similar locality but smooth, because features decay with distance 
ï®Polynomials 
ï®Global features 
ï®A change to one weight affects the entire space 
53 


## Page 54

Inductive Bias (Prior Knowledge) 
ï®The approximator is just a mechanism; the encoding is where most 
of the prior knowledge is contained in classic RL 
ï®Assumptions: 
ï®Nearby states should have similar values â†’ local features (tiles, RBFs) 
ï®The value surface is smooth and low-curvature â†’ low-degree polynomials 
ï®The function might have sharp discontinuities â†’ more tiles, finer grids 
54 


## Page 55

Function Approximation 
1. On-Policy Prediction with Approximation 
 
1.1. Value-Function Approximation  
 
1.2. Stochastic-Gradient and Semi-Gradient Methods 
 
1.3. Approximation with Linear Methods  
 
1.4. Feature Construction (Polynomials, Coarse Coding, Tile Coding, RBF) 
 
1.5. Approximation with Neural Networks  
 
1.6. Approximation with Memory-Based Methods 
2. On-Policy Control with Approximation  
 
2.1. Episodic Semi-Gradient Control  
 
2.2. Average Reward for Continuing Tasks  
 
2.3. The Deadly Triad  
 
 
55 


## Page 56

Nonlinear Value Function Approximation 
ï®
There exist several nonlinear methods for 
approximating the value function, such as: 
ï®
Neural Networks (NNs) 
ï®
Memory-based (nonparametric) functions 
ï®
NNs have recently become the most 
popular approximators 
ï®
NNs are universal function approximators 
ï®
In deep architectures they can generate 
hierarchical representations of features 
automatically (vs. hand crafted features) 
ï®
They typically learn by stochastic gradient 
methods 
56 


## Page 57

NNs for TD Learning 
ï®NNs can be trained with TD errors to estimate value functions 
ï®The update rule adjusts the weights to reduce TD error 
ï®NN gradient computation applies equally in supervised and 
reinforcement learning settings 
ï®Function approximation with NNs allows generalization between 
states or state-action pairs 
57 


## Page 58

Challenges with Deep Networks 
ï®Deeper networks can overfit due to high capacity and limited data; 
regularization is often needed 
ï®Gradients may vanish during backpropagation, impairing learning in 
early layers 
ï®Generalization performance may degrade when adding more layers 
despite increased expressivity 
ï®In online RL, overfitting is less critical, but generalization between 
trajectories still matters 
58 


## Page 59

Function Approximation 
1. On-Policy Prediction with Approximation 
 
1.1. Value-Function Approximation  
 
1.2. Stochastic-Gradient and Semi-Gradient Methods 
 
1.3. Approximation with Linear Methods  
 
1.4. Feature Construction (Polynomials, Coarse Coding, Tile Coding, RBF) 
 
1.5. Approximation with Neural Networks  
 
1.6. Approximation with Memory-Based Methods 
2. On-Policy Control with Approximation  
 
2.1. Episodic Semi-Gradient Control  
 
2.2. Average Reward for Continuing Tasks  
 
2.3. The Deadly Triad  
 
 
59 


## Page 60

Instance-Based Models 
ï®Learning based on similarity 
ï®Predict the value of an instance using those of similar instances 
ï®Nearest neighbors 
ï®1NN: return the value of the most similar instance 
ï®kNN: average over the k nearest neighbors, usually with a weighting scheme 
(e.g., w = 1 / d) 
ï®Key issue: the distance metric, e.g., Euclidean distance 
ï®Trade-offs: small k gives relevant neighbors, large k gives smoother, more 
global functions 
60 


## Page 61

Non-parametric Models 
ï®Parametric models: 
ï®Fixed set of parameters 
ï®More data means better settings 
ï®Non-parametric models: 
ï®Complexity of the classifier increases with data 
ï®Better in the limit, often worse in the non-limit 
ï®kNN is a non-parametric method 
ï®Usually performance decreases for high-dimensional problems 
ï®The concept of distance becomes less relevant in high dimensions 
 
61 


## Page 62

kNN 
62 


## Page 63

Function Approximation 
1. On-Policy Prediction with Approximation 
 
1.1. Value-Function Approximation  
 
1.2. Stochastic-Gradient and Semi-Gradient Methods 
 
1.3. Approximation with Linear Methods  
 
1.4. Feature Construction (Polynomials, Coarse Coding, Tile Coding, RBF) 
 
1.5. Approximation with Neural Networks  
 
1.6. Approximation with Memory-Based Methods 
2. On-Policy Control with Approximation  
 
2.1. Episodic Semi-Gradient Control  
 
2.2. Average Reward for Continuing Tasks  
 
2.3. The Deadly Triad  
 
 
63 


## Page 64

Control with Approximation 
64 


## Page 65

Episodic Semi-Gradient Control 
65 


## Page 66

Sarsa 
66 


## Page 67

67 


## Page 68

Example: Mountain Car Problem 
ï®Mountain Car presents a standard continuous control benchmark 
ï®State includes car position and velocity on a one-dimensional track 
ï®Actions: full throttle left, zero throttle, or full throttle right 
ï®Reward is â€“1 each time step until reaching the goal 
ï®The optimal behavior requires reversing to gain momentum before 
climbing 
68 


## Page 69

Implementation Details 
69 


## Page 70

Learning 
Cost-to-go 
increases 
70 
Ñ” = 0 


## Page 71

Learning 
ï®Low (good) cost-to-go appears near the goal region at the hilltop 
ï®High (bad) cost-to-go appears in regions where the car remains stuck 
ï®The agent learns a policy to back up left, then accelerate right to 
reach the goal 
71 


## Page 72

Learning 
72 


## Page 73

Program 
ï®3 Control - Sarsa â†’ Sarsa_Agent.py, MountainCar_Env.py 
73 


## Page 74

74 


## Page 75

Function Approximation 
1. On-Policy Prediction with Approximation 
 
1.1. Value-Function Approximation  
 
1.2. Stochastic-Gradient and Semi-Gradient Methods 
 
1.3. Approximation with Linear Methods  
 
1.4. Feature Construction (Polynomials, Coarse Coding, Tile Coding, RBF) 
 
1.5. Approximation with Neural Networks  
 
1.6. Approximation with Memory-Based Methods 
2. On-Policy Control with Approximation  
 
2.1. Episodic Semi-Gradient Control  
 
2.2. Average Reward for Continuing Tasks  
 
2.3. The Deadly Triad  
 
 
75 


## Page 76

Continuing Tasks and Average Reward 
ï®Many tasks in reinforcement learning run indefinitely; there is no 
natural episode limit or reset 
ï®For example, an elevator controller runs indefinitely. We want to 
minimize the average waiting time in the long term, so favoring 
immediate rewards would be arbitrary 
ï®Short-term and delayed rewards have equal importance; the discount 
factor Î³ is no longer important 
ï®We define the goal of a policy Ï€ as its average reward r(Ï€) per step 
76 


## Page 77

Definition of Average Reward 
ï®The average reward r(Ï€) is the time-average of expected rewards 
under policy Ï€ 
ğ‘Ÿğœ‹= lim
â„â†’âˆ
1
â„ ğ”¼ğ‘…ğ‘¡
ğœ‹
â„
ğ‘¡=1
 
ï®This quantity measures reward per time step, not total reward over a 
finite horizon 
ï®Policies are compared and considered optimal according to their 
average rewards r(Ï€) 
77 


## Page 78

Steady-State and Ergodicity 
ï®The fraction of time spent in each state then depends only on the 
policy and dynamics 
ï®We call this steady long-run distribution over states the steady-state 
distribution 
ï®When this distribution exists and is unique, the Markov decision 
process is called ergodic 
ï®An ergodic system forgets where it started; state frequencies stabilize 
over time 
ï®In that case, the average reward r(Ï€) is well defined and independent 
of the start 
78 


## Page 79

Differential Return 
ï®Raw cumulative reward often diverges in continuing tasks, so we 
measure reward relative to the average 
ï®We define the differential return Gt by subtracting r(Ï€) from each 
future reward: 
ğºğ‘¡= ğ‘…ğ‘¡+1 âˆ’ğ‘Ÿğœ‹+ ğ‘…ğ‘¡+2 âˆ’ğ‘Ÿğœ‹+ â‹¯ 
ï®A positive Gt means this trajectory segment performed better than 
the usual average of the policy Ï€ 
ï®A negative Gt means that performance was worse than average, 
under the same policy Ï€ 
79 


## Page 80

Differential Value Functions 
ï®The differential state value ğ‘£ğœ‹ğ‘  is the expected differential return 
from state s 
ï®The differential action value ğ‘ğœ‹ğ‘ , ğ‘ is the expected differential 
return from (s, a) using policy Ï€: 
ğ‘ğœ‹ğ‘ , ğ‘= ğ”¼ğœ‹ğºğ‘¡
ğ‘†ğ‘¡= ğ‘ , ğ´ğ‘¡= ğ‘ 
ï®These values measure how much better or worse than average a 
state or action is 
ï®If we add the same constant to every differential value, choices and 
rankings remain unchanged 
80 


## Page 81

Bellman Equation for Differential q-Values 
ï®Differential action values satisfy a Bellman relation similar to the 
discounted case, without Î³ 
ï®For a fixed policy Ï€, ğ‘ğœ‹ğ‘ , ğ‘ equals a centered immediate reward 
plus an expected next ğ‘ğœ‹: 
ğ‘ğœ‹ğ‘ , ğ‘=  ğ‘ğ‘ â€², ğ‘Ÿ
ğ‘ , ğ‘
ğ‘Ÿâˆ’ğ‘Ÿğœ‹+  ğœ‹ğ‘â€²
ğ‘ â€² ğ‘ğœ‹ğ‘ â€², ğ‘â€²
ğ‘â€²
ğ‘ â€²,ğ‘Ÿ
 
ï®The term r â€“ r(Ï€) measures how good the immediate reward is 
relative to average 
ï®The second term propagates the expected differential value of the 
next state-action pair 
81 


## Page 82

TD Learning with an Average-Reward 
Baseline 
ï®The algorithms keep an estimate ğ‘…ğ‘¡ of the average reward r(Ï€) while 
learning 
ï®
ğ‘…ğ‘¡ is an estimate at time t of the average reward r(Ï€) 
ï®Each step produces a differential TD error : 
ğ›¿ğ‘¡= ğ‘…ğ‘¡+1 âˆ’ğ‘…ğ‘¡+ ğ‘ ğ‘†ğ‘¡+1, ğ´ğ‘¡+1, ğ‘¤ğ‘¡âˆ’ğ‘ ğ‘†ğ‘¡, ğ´ğ‘¡, ğ‘¤ğ‘¡ 
 
82 


## Page 83

Differential Semi-Gradient Sarsa 
ï®We approximate action values with a differentiable function 
ğ‘ ğ‘ , ğ‘, ğ° with weights w 
ï®A behavior policy, usually Ñ”-greedy, selects actions based on current 
estimates ğ‘  
ï®After each transition, we update the average reward and the weights 
based on the TD error Î´t: 
  
ğ‘… â†ğ‘… + ğ›½ ğ›¿ 
  
ğ°â†ğ°+ ğ›¼ ğ›¿ âˆ‡ğ°ğ‘ ğ‘†, ğ´, ğ° 
 
 
83 


## Page 84

84 


## Page 85

Example: Access-Control Queuing 
ï®Environment: 10 servers and a single queue of customers with 4 
priority levels 
ï®Serving a customer yields rewards 1, 2, 4, or 8 depending on priority; 
rejecting yields 0 reward 
ï®The queue never empties; servers become free with probability  
p = 0.06 each time step 
ï®The agent chooses accept or reject from states described by the 
number of free servers and the priority of the customer at the head 
of the queue 
ï®Goal: maximize long-run reward, so average reward is appropriate 
85 


## Page 86

Results 
ï®Learning uses differential semi-gradient one-step Sarsa with  
Î± = 0.01, Î² = 0.01, Ñ” = 0.1 
ï®Training for 2 million steps 
ï®Estimated long-run average reward ğ‘…  converges to ~ 2.31 units per 
time step 
ï®The learned policy accepts higher-priority customers more often and 
only accepts lower-priority customers when more servers are free 
86 


## Page 87

The drop on the right of the graph is probably 
due to insufficient data; many of these states 
were never experienced 


## Page 88

Discounted Setting vs. Average Reward  
in Continuing Tasks 
ï®With function approximation, many distinct states can share the 
same features, so the agent cannot treat them differently 
ï®In the extreme, its behavior depends only on long-run reward 
statistics of the process, not on individual state identities 
ï®If we average undiscounted rewards over time, we get the average 
reward r(Ï€) for the policy 
ï®If we average discounted returns over time, we get 
1
1 âˆ’ğ›¾ğ‘Ÿğœ‹ 
ï®The policies rank exactly the same 
88 


## Page 89

Role of Î³ in Continuing Tasks with Function 
Approximation 
ï®In theory, maximizing the discounted value over on-policy states is 
equivalent to maximizing the average reward 
ï®Î³ does not change which policy is optimal 
ï®It does not define the control problem 
ï®Control algorithms with function approximation do not truly 
optimize either discounted value or average reward 
ï®The policy improvement theorem fails, so no variant guarantees 
reliable policy improvement in practice 
ï®In continuing tasks, the average reward defines the objective (what) 
ï®The discount factor Î³ mainly controls how learning behaves 
ï®Bias-variance trade-offs, learning stability 
 
89 


## Page 90

Function Approximation 
1. On-Policy Prediction with Approximation 
 
1.1. Value-Function Approximation  
 
1.2. Stochastic-Gradient and Semi-Gradient Methods 
 
1.3. Approximation with Linear Methods  
 
1.4. Feature Construction (Polynomials, Coarse Coding, Tile Coding, RBF) 
 
1.5. Approximation with Neural Networks  
 
1.6. Approximation with Memory-Based Methods 
2. On-Policy Control with Approximation  
 
2.1. Episodic Semi-Gradient Control  
 
2.2. Average Reward for Continuing Tasks  
 
2.3. The Deadly Triad  
 
 
90 


## Page 91

The Deadly Triad 
ï®Instability and divergence in RL arise when three elements appear 
together: the deadly triad 
ï®Function approximation uses parametric value models instead of large 
tables 
ï®Bootstrapping uses targets that include current value estimates 
ï®Off-policy learning learns about one policy while following another behavior 
policy 
ï®Each ingredient is useful alone but dangerous in combination 
91 


## Page 92

Why the Combination Can Diverge 
ï®
All three together can cause value estimates to diverge even in simple 
prediction tasks 
ï®
Function approximation couples many states through shared parameters 
and updates 
ï®
Bootstrapping creates feedback by providing estimates into update 
targets 
ï®
Off-policy learning updates under a distribution that does not match that 
of the target policy 
ï®
The deadly triad definition applies to any function approximator, not 
only linear models 
ï®
Usually, function approximation and bootstrapping are important (large 
problems, continuing tasks), but so is off-policy learning in some cases 
92 


## Page 93

Off-Policy Learning 
ï®Predictive knowledge view uses many value functions for many tasks 
ï®An agent can follow one behavior policy while learning about many 
target policies 
ï®A robot in a building follows one safe behavior policy (wander safely, avoid 
collisions) yet wants to learn many â€œwhat ifâ€ predictions at once: what if I 
followed the wall, what if I walked fast, what if I went to the charger now 
ï®On-policy learning alone cannot cover many hypothetical behaviors 
efficiently 
ï®Off-policy TD reuses a single experience stream for multiple 
predictive questions 
ï®The deadly triad highlights instability risks in large-scale off-policy 
systems 
93 


## Page 94

Conclusions 
ï®Function approximation enables RL in large or continuous state 
spaces with limited memory resources 
ï®Value function approximation uses stochastic gradient and semi-
gradient methods to learn estimates from interaction 
ï®Feature construction determines representations and thus 
generalization patterns, which affect stability, sample efficiency and 
performance 
ï®Neural networks offer powerful nonlinear approximators but 
introduce optimization difficulties, instability risks and overfitting 
concerns 
ï®The deadly triad is the combination of function approximation, 
bootstrapping and off-policy learning, which often leads to instability 
 
94 


## Page 95

Main References 
ï®Sutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: 
An Introduction. 2nd edition. MIT Press, Cambridge, MA. 
http://incompleteideas.net/book/the-book-2nd.html 
 
ï®
Castellini, A. (2023). On-Policy Prediction with Approximation, 
Reinforcement learning â€“ LM Artificial Intelligence, University of Verona, 
https://profs.scienze. univr.it/~castellini/docs/reinforcementLearning22-
23/RL_L9_OnPredApprox.pdf 
ï®
Castellini, A. (2023). On-Policy Control with Approximation and Deep Q 
Networks (DQN), Reinforcement learning â€“ LM Artificial Intelligence, 
University of Verona, https://profs.scienze.univr.it/~castellini/docs/ 
reinforcementLearning22-23/RL_L10_OnControlApprox.pdf 
 
95 
