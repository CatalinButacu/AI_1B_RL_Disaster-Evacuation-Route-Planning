# RL10_ModernAC

**Source:** c:\Users\catalin.butacu\Downloads\RL\resourses\lecture\RL10_ModernAC.pdf

**Pages:** 73

---


## Page 1

Reinforcement Learning 
10. Modern Actor-Critic Methods 
 
Florin Leon 
 
â€œGheorghe Asachiâ€ Technical University of IaÈ™i, Romania 
Faculty of Automatic Control and Computer Engineering 
 
https://florinleon.byethost24.com/lect_rl.html 
 
2025, Draft 
 


## Page 2

Modern Actor-Critic Methods 
1. Proximal Policy Optimization (PPO) 
2. Soft Actor-Critic (SAC) 
3. Dreamer 
2 


## Page 3

Modern Actor-Critic Methods 
1. Proximal Policy Optimization (PPO) 
2. Soft Actor-Critic (SAC) 
3. Dreamer 
3 


## Page 4

PPO: Stabilised Actorâ€“Critic 
ï®PPO is a policy gradient method and stabilised actorâ€“critic variant 
ï®It keeps the conceptual simplicity of REINFORCE and basic actorâ€“
critic 
ï®Added mechanism explicitly limits how much the policy changes per 
update 
ï®Restricting updates keeps the new policy close to the current one 
ï®This stability helped PPO become a default deep RL algorithm 
4 


## Page 5

Where PPO Fits Among RL Methods 
ï®Monte Carlo control, Sarsa, Q-learning, DQN emphasize value 
functions and greedy policies 
ï®Policy gradient methods parameterize the policy ğœ‹ğœƒğ‘
ğ‘  directly 
ï®Objective becomes maximizing expected return by adjusting 
parameters ğœƒ 
ï®A typical gradient sample uses the pair âˆ‡ğœƒlog ğœ‹ğœƒ
ğ‘ğ‘¡
ğ‘ ğ‘¡, ğ´ğ‘¡
  
ï®ğ´ğ‘¡
  estimates the advantage of action ğ‘ğ‘¡ in state ğ‘ ğ‘¡ 
5 


## Page 6

Actorâ€“Critic Instability and PPOâ€™s Goal 
ï®Actorâ€“critic methods add a learned value function to reduce gradient 
variance 
ï®Single gradient steps may still change the policy very aggressively 
ï®Old trajectories then no longer describe behaviour of the updated 
policy well 
ï®This mismatch can create strong instability and even complete 
learning collapse 
ï®PPO modifies the update rule to keep each policy change within a 
safe region 
6 


## Page 7

Policy Ratios in PPO 
ï®PPO compares old and new policies with a probability ratio 
ï®Definition: 
ğ‘Ÿğ‘¡ğœƒ=
ğœ‹ğœƒğ‘ğ‘¡
ğ‘ ğ‘¡
ğœ‹ğœƒold ğ‘ğ‘¡
ğ‘ ğ‘¡
 
ï®ğ‘Ÿğ‘¡â‰ˆ1 means little change; ğ‘Ÿğ‘¡> 1 increases, ğ‘Ÿğ‘¡< 1 decreases action 
probability 
ï®The same ratio appears in off-policy Monte Carlo and importance 
sampling 
ï®PPO reuses ğ‘Ÿğ‘¡ to measure how aggressive each policy update is on 
sampled data 
7 


## Page 8

Surrogate Objective in Policy Gradient 
ï®Basic policy gradient objective: 
ğ¿ğ‘ƒğºğœƒ= ğ”¼t log ğœ‹ğœƒğ‘ğ‘¡
ğ‘ ğ‘¡
, ğ´ğ‘¡
  
ï®Gradient increases probability of actions with positive ğ´ğ‘¡
  
ï®Gradient decreases probability of actions with negative ğ´ğ‘¡
  
ï®Actorâ€“critic versions still follow this basic objective form 
8 


## Page 9

Conservative Policy Iteration Form 
ï®Rewrite with probability ratios: 
ğ‘Ÿğ‘¡ğœƒ= ğœ‹ğœƒğ‘ğ‘¡
ğ‘ ğ‘¡
ğœ‹ğœƒğ‘œğ‘™ğ‘‘ğ‘ğ‘¡
ğ‘ ğ‘¡
 
ï®Surrogate objective: 
ğ¿ğ¶ğ‘ƒğ¼ğœƒ= ğ”¼ğ‘¡ğ‘Ÿğ‘¡ğœƒ, ğ´ğ‘¡
  
ï®Intuition: increase ğ‘Ÿğ‘¡ when ğ´ğ‘¡
 > 0, decrease when ğ´ğ‘¡
 < 0 
ï®Large ğ‘Ÿğ‘¡ or small ğ‘Ÿğ‘¡ mean the new policy moved far from the data 
policy 
9 


## Page 10

Instability and TRPOâ€™s Trust Region Idea 
ï®Repeated gradient steps on ğ¿ğ¶ğ‘ƒğ¼ can push ğœƒ too far 
ï®Ratios ğ‘Ÿğ‘¡ can become extreme; old samples no longer match the new 
policy 
ï®Learning then becomes unreliable and can collapse completely 
ï®TRPO constrains KL divergence between old and new policies 
ï®TRPO enforces a trust region but requires complex constrained 
optimization 
10 


## Page 11

PPOâ€™s Clipped Surrogate Objective 
ï®PPO mimics a trust region with a simple clipped objective 
ï®Clipped loss: 
ğ¿ğ‘ğ‘™ğ‘–ğ‘ğœƒ= ğ”¼ğ‘¡min ğ‘Ÿğ‘¡ğœƒğ´ğ‘¡
 , clip ğ‘Ÿğ‘¡ğœƒ, 1 âˆ’ğœ–, 1 + ğœ–ğ´ğ‘¡
 
 
ï®Clip keeps ğ‘Ÿğ‘¡ğœƒ in 1 âˆ’ğœ–, 1 + ğœ– inside the loss 
ï®ğœ– is small, typically (0.1) or (0.2) 
11 


## Page 12

Clipping Behaviour for Positive and 
Negative Advantage 
ï®Case ğ´ğ‘¡
 > 0: increasing ğ‘Ÿğ‘¡ above 1 helps until 1 + ğœ– 
ï®Once ğ‘Ÿğ‘¡> 1 + ğœ–, the clipped term stops growing; extra increase 
brings no benefit 
ï®Case ğ´ğ‘¡
 < 0: decreasing ğ‘Ÿğ‘¡ below 1 helps until 1 âˆ’ğœ– 
ï®Once ğ‘Ÿğ‘¡< 1 âˆ’ğœ–, the clipped term stops decreasing; further 
reduction gives no gain 
12 


## Page 13

Soft Trust Region Intuition 
ï®PPO allows ğ‘Ÿğ‘¡ to move away from 1 inside 1 âˆ’ğœ–, 1 + ğœ– 
ï®Inside this interval, learning behaves like a standard policy gradient 
method 
ï®Outside the interval, the objective discourages further change on 
those samples 
ï®Gradients naturally pull updates back toward smaller, safer policy 
shifts 
ï®Clipping implements a soft trust region in the loss without second-
order methods 
13 


## Page 14

PPOâ€™s Combined Objective 
ï®PPO optimizes policy, value function, and entropy together 
ï®Total loss: 
 
ğ¿ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ğœƒ, ğœ™= ğ”¼t ğ¿ğ‘ğ‘™ğ‘–ğ‘ğœƒ+ ğ‘1ğ¿ğ‘‰ğ¹ğœ™+ ğ‘2ğ‘†ğœ‹ğœƒâ‹…ğ‘ ğ‘¡
 
 
ï®ğœƒ: policy parameters; ğœ™: value function parameters 
ï®ğ¿ğ‘‰ğ¹ğ‘¡: value loss for ğ‘‰ğœ™ğ‘ ğ‘¡ 
ï®ğ‘†ğœ‹ğœƒâ‹…ğ‘ ğ‘¡
: entropy of the policy at state ğ‘ ğ‘¡ 
14 


## Page 15

Value Loss and Entropy Bonus 
ï®Value loss pushes ğ‘‰ğœ™ğ‘ ğ‘¡ toward an empirical return ğ‘…ğ‘¡ 
ï®Usually a squared error: ğ‘‰ğœ™ğ‘ ğ‘¡âˆ’ğ‘…ğ‘¡
2 
ï®Learned value acts as a baseline and reduces variance of ğ´ğ‘¡
  
ï®Entropy term encourages high-entropy (more random) policies 
ï®Coefficients ğ‘1, ğ‘2  balance policy improvement, value accuracy, and 
exploration 
15 


## Page 16

Advantage Estimation with GAE 
ï®PPO uses an advantage estimate ğ´ğ‘¡
  rather than raw returns 
ï®Temporal-difference error: 
ğ›¿ğ‘¡= ğ‘Ÿğ‘¡+ ğ›¾ğ‘‰ğœ™ğ‘ ğ‘¡+1 âˆ’ğ‘‰ğœ™ğ‘ ğ‘¡ 
ï®Generalized Advantage Estimation (GAE): 
ğ´ğ‘¡
 = ğ›¿ğ‘¡+ ğ›¾ğœ†ğ›¿ğ‘¡+1 ğ›¾ğœ†2ğ›¿ğ‘¡+2 + â‹¯ 
ï®Combines TD errors over future steps with geometric weights 
ï®Extends ideas from TD(ğœ†) and eligibility traces 
16 


## Page 17

GAE: Biasâ€“Variance Trade-off and Intuition 
ï®ğœ†â‰ˆ1: long-horizon estimate, low bias, high variance 
ï®Smaller ğœ†: more reliance on short-term TD errors, higher bias, lower 
variance 
ï®GAE measures how actual continuation differs from value-function 
expectations 
ï®Positive ğ´ğ‘¡
  signals surprisingly good outcomes from ğ‘ ğ‘¡ 
ï®This surprise signal directly shapes PPOâ€™s clipped policy update 
17 


## Page 18

PPO Training Loop: Data Collection and 
Estimation 
ï®Current stochastic policy runs in the environment for several time 
steps 
ï®Often many parallel actors collect trajectories simultaneously 
ï®Collected data: states, actions, rewards, and value predictions 
ï®Returns and advantages are computed for each time step using GAE 
ï®These trajectories form a single â€œbatchâ€ for the next optimisation 
phase 
18 


## Page 19

PPO Training Loop: Multiple Optimisation 
Epochs 
ï®PPO reuses the same batch for several epochs of mini-batch SGD or 
Adam 
ï®Policy in the denominator, ğœ‹ğœƒğ‘œğ‘™ğ‘‘, stays fixed during these epochs 
ï®Only the numerator policy ğœ‹ğœƒ changes while optimizing ğ¿ğ‘ğ‘™ğ‘–ğ‘ 
ï®Clipping ensures policy ratios cannot wander too far from 1 on this 
data 
ï®Many epochs still produce conservative changes because extreme 
ratios stop improving the objective 
19 


## Page 20

PPO Training Loop: Value Updates and On-
Policy Nature 
ï®Value function parameters ğœ™ update using the same batch of 
trajectories 
ï®Value loss uses a separate gradient step, often with its own optimizer 
ï®After optimization, the algorithm sets ğœƒğ‘œğ‘™ğ‘‘â†ğœƒ 
ï®New trajectories are then collected with the updated policy 
ï®PPO remains on-policy; it does not rely on experience replay buffers 
20 


## Page 21

KL-Penalty PPO Variant 
ï®Original paper also explored a KL-penalty version of PPO 
ï®This variant adds a KL divergence term between old and new policies 
into the objective 
ï®Algorithm tracks actual KL and compares it with a target KL value 
ï®Penalty coefficient increases or decreases to steer KL toward the 
target 
ï®Behaviour mimics an adaptive trust region but proved harder to tune 
than clipping 
21 


## Page 22

Distributed PPO (DPPO) 
ï®DeepMind developed a distributed PPO implementation for large-
scale experiments 
ï®Many workers collect trajectories in parallel under a shared policy 
ï®Gradients from workers are synchronised to update a central set of 
parameters 
ï®Objective still follows PPO with regularised policy and value losses 
ï®Results on locomotion tasks show complex skills for walkers, 
quadrupeds, and humanoids 
22 


## Page 23

Why PPO Works: Stability and Sample 
Efficiency 
ï®Clipped objective ğ¿ğ‘ğ‘™ğ‘–ğ‘ discourages destructive, overly large policy 
updates 
ï®Once ratios leave the trust region, further change on those samples 
brings no benefit 
ï®Multiple epochs over each batch improve sample efficiency for an on-
policy method 
ï®Each trajectory contributes many gradient steps instead of a single 
noisy update 
ï®REINFORCE-style methods discard data after one update and 
therefore waste information 
23 


## Page 24

Why PPO Works: Simplicity and Robustness 
ï®Implementation mainly changes the loss in a standard actorâ€“critic 
network 
ï®No second-order optimisation, conjugate gradients, or line searches 
are required 
ï®Reference implementations, such as Spinning Up, closely match this 
simple structure 
ï®PPO works reliably across many tasks with reasonable default 
hyperparameters 
ï®Empirical studies show performance competitive with or better than 
TRPO, A2C, and others 
24 


## Page 25

Conceptual Summary of PPO 
ï®PPO is actorâ€“critic with a self-protecting, ratio-based surrogate loss 
ï®Policy update uses ğ‘Ÿğ‘¡ğœƒğ´ğ‘¡
  but clips benefits when ratios move too 
far 
ï®Critic provides value estimates and TD errors that feed GAE for ğ´ğ‘¡
  
ï®Entropy bonus prevents premature collapse to deterministic policies 
and maintains exploration 
ï®Trust-region intuition is encoded directly in the loss, yielding a 
simple, reliable deep RL algorithm 
25 


## Page 26

Modern Actor-Critic Methods 
1. Proximal Policy Optimization (PPO) 
2. Soft Actor-Critic (SAC) 
3. Dreamer 
26 


## Page 27

SAC: High-Level Picture 
ï®SAC is an off-policy actorâ€“critic algorithm for continuous control 
ï®Combines value-based off-policy learning with stochastic policy 
gradients 
ï®Uses a maximum-entropy objective that explicitly rewards 
â€œpurposeful randomnessâ€ 
ï®Aims for stability, sample efficiency, and robustness across tasks 
ï®Replaces brittle deterministic behavior with deliberately noisy 
policies 
27 


## Page 28

From DDPG/TD3 to SAC 
ï®DDPG and TD3 optimise deterministic policies for expected return 
only 
ï®They achieve strong sample efficiency but often explore poorly 
ï®Deterministic policies over-trust noisy Q-values and get stuck in bad 
optima 
ï®SAC keeps off-policy efficiency but changes the exploration story 
ï®Policy remains noisy as long as entropy does not hurt reward too 
much 
28 


## Page 29

Classic RL Objective 
ï®Standard episodic RL maximizes expected return: 
 
ğ½ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘ğœ‹= ğ”¼ ğ‘Ÿğ‘ ğ‘¡, ğ‘ğ‘¡
ğ‘¡
 
 
ï®Optimal policy in fully observed MDPs tends toward determinism 
ï®Policy eventually commits to the single best action per state 
ï®Determinism is risky with noisy, approximate value functions 
ï®Small estimation errors can produce fragile, over-committed 
behavior 
29 


## Page 30

Maximum Entropy Objective 
ï®Maximum entropy RL augments the reward with an entropy bonus 
ï®Objective: 
ğ½ğ‘šğ‘ğ‘¥âˆ’ğ‘’ğ‘›ğ‘¡ğœ‹= ğ¸ ğ‘Ÿğ‘ ğ‘¡, ğ‘ğ‘¡+ ğ›¼, â„‹ğœ‹â‹…ğ‘ ğ‘¡
ğ‘¡
 
ï®Policy entropy: 
 
ï® trades off reward versus entropy 
ï®As ğ›¼â†’0, objective reduces to standard RL 
30 


## Page 31

Consequences of the Max-Entropy View 
ï®High entropy keeps multiple promising actions alive per state 
ï®Exploration improves because randomness is part of the objective 
ï®Robustness increases; policy avoids over-commitment to narrow 
strategies 
ï®Multiple behavioral modes can coexist when actions are similarly 
good 
ï®For finite ğ›¼, the optimal policy is intentionally stochastic 
31 


## Page 32

SAC as â€œSoftâ€ Actorâ€“Critic 
ï®SAC follows the usual actorâ€“critic pattern with modifications 
ï®Uses off-policy data and replay-style updates like DDPG/TD3 
ï®Policy update targets high soft Q-values, not only plain Q-values 
ï®Soft Q-values incorporate both reward and future entropy 
ï®Framework formalises the trade-off between performance and 
purposeful randomness 
32 


## Page 33

Main Function Approximators in SAC 
ï®Stochastic policy ğœ‹ğœ™ğ‘
ğ‘  serves as the actor 
ï®Policy often modeled as a Gaussian with neural-network mean and 
standard deviation 
ï®Two Q-networks ğ‘„ğœƒ1 ğ‘ , ğ‘ and ğ‘„ğœƒ2 ğ‘ , ğ‘ serve as critics 
ï®Dual critics help control overestimation bias, echoing Double Q ideas 
ï®A pair of target Q-networks provides delayed, stable bootstrapping 
targets 
33 


## Page 34

Value Network and Standard SAC Variant 
ï®Original SAC formulation used a separate value network V(s) 
ï®Later simplification removed V(s) and relied directly on twin Q-
functions 
ï®Modern implementations usually follow this streamlined â€œstandard 
variant.â€ 
ï®Overall flow still mirrors DDPG/TD3 structurally: actor proposes, 
critics evaluate 
ï®Key change: actor is trained to maximize soft Q, not solely expected 
reward 
34 


## Page 35

Soft Q-Values and the Soft Bellman Backup 
ï®Standard Q-learning backup assumes greedy next actions: 
ğ‘„ğ‘ , ğ‘â‰ˆğ‘Ÿğ‘ , ğ‘+ ğ›¾ğ”¼ğ‘ â€² max ğ‘â€² ğ‘„ğ‘ â€², ğ‘â€²
 
ï®Maximum entropy RL replaces greedy choice with sampling from an 
optimal stochastic policy 
ï®Soft backup uses a soft value: 
 
 
ï®The term âˆ’ğ›¼log ğœ‹ğ‘â€² ğ‘ â€²  injects the entropy bonus at the next step 
ï®High soft Q(s,a) indicates actions with good reward and beneficial 
future stochasticity 
35 


## Page 36

Double Critics and Soft Targets in SAC 
ï®Deep methods often overestimate Q-values when maximization 
appears inside updates 
ï®SAC maintains two critics and uses the minimum of their target 
predictions 
ï®Soft target for transition (s, a, r, s'): 
ğ‘¦= ğ‘Ÿ+ ğ›¾min
ğ‘–=1,2 ğ‘„ğœƒğ‘–ğ‘ â€², ğ‘â€² âˆ’ğ›¼log ğœ‹ğœ™ğ‘â€² ğ‘ â€²
 
ï®with ğ‘â€² âˆ¼ğœ‹ğœ™â‹…ğ‘ â€²  
ï®Target networks ğ‘„ğœƒğ‘– change slowly and keep bootstrapping stable 
ï®Critics learn expected future reward plus entropy under the current 
stochastic policy, using conservative targets 
36 


## Page 37

Soft Policy Improvement in SAC 
ï®Standard actorâ€“critic increases expected Q(s,a) under the policy 
ï®SAC still prefers high-Q actions but also values entropy 
ï®Good actions balance large Q with sufficient randomness 
ï®Policy improvement trades reward against uncertainty, not reward 
alone 
ï®Objective explicitly encodes this Qâ€“entropy trade-off 
37 


## Page 38

KL View and Ideal Max-Entropy Policy 
ï®Ideal maximum-entropy policy for a given Q: 
ğœ‹â‹†ğ‘
ğ‘ 
âˆexp 1
ğ›¼ğ‘„ğ‘ , ğ‘
 
ï®High-Q actions receive high probability but other actions never 
vanish completely 
ï®Temperature ğ›¼ controls how sharp or diffuse preferences are 
ï®SAC cannot represent ğœ‹â‹† exactly within restricted policy classes 
ï®Actor instead moves toward ğœ‹â‹† by minimizing a KL divergence 
38 


## Page 39

Practical Actor Objective and Interpretation 
ï®SAC actor objective: 
ğ½ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿğœ™= ğ”¼ğ‘ âˆ¼ğ’Ÿ,ğ‘âˆ¼ğœ‹ğœ™ğ›¼log ğœ‹ğœ™
ğ‘
ğ‘ 
âˆ’ğ‘„ğœƒğ‘ , ğ‘ 
ï®The âˆ’ğ‘„ğœƒğ‘ , ğ‘ term discourages low-value actions 
ï®The ğ›¼log ğœ‹ğœ™
ğ‘
ğ‘  term penalizes overly peaked distributions 
ï®Minimizing ğ½ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ puts mass on high-Q actions while maintaining 
entropy 
ï®Actor approximates the exponentiated-Q distribution within its 
parameterization 
39 


## Page 40

Gaussian Actor and Reparameterisation 
ï®Policy usually Gaussian with neural-network mean and log standard 
deviation 
ï®Actions sampled via reparameterization: 
ğ‘= tanh ğœ‡ğœ™ğ‘ + ğœğœ™ğ‘ , ğœ‰, â€ƒğœ‰âˆ¼ğ’©0, ğ¼ 
ï®Reparameterization keeps sampling differentiable for 
backpropagation 
ï®Tanh squashing enforces bounded actions matching environment 
constraints 
ï®Actor network learns both mean behavior and exploration scale 
40 


## Page 41

Temperature and Reward Scaling 
ï®Temperature ğ›¼ sets trade-off between reward and entropy 
ï®Large ğ›¼ : strong entropy bonus, very random policies 
ï®Small ğ›¼: entropy suppressed, policy approaches deterministic 
behavior 
ï®In max-entropy RL, reward rescaling changes the optimal policy 
unless ğ›¼ adjusts 
ï®Choosing ğ›¼ well is crucial for good performance 
41 


## Page 42

Automatic Temperature Tuning in SAC 
ï®SAC treats ğ›¼ as a learnable parameter, not a fixed hyperparameter 
ï®Conceptually solves: maximize return subject to entropy above a 
target 
ï®Lagrangian introduces ğ›¼ as dual variable for the entropy constraint 
ï®Practical loss: 
ğ½ğ›¼= ğ¸ğ‘ ,ğ‘âˆ¼ğœ‹ğœ™âˆ’ğ›¼log ğœ‹ğœ™
ğ‘
ğ‘ 
+ â„‹ğ“‰ğ’¶ğ“‡â„Šâ„¯ğ“‰
 
ï®Low entropy (below target) pushes ğ›¼ up; high entropy pushes ğ›¼ 
down, adapting exploration automatically 
42 


## Page 43

SAC Training Loop: Interaction and Replay 
ï®SAC stores transitions (s, a, r, s') from the current stochastic policy in 
a replay buffer 
ï®Environment interaction may use a single agent or many parallel 
actors 
ï®Replay buffer keeps experience from recent policies, not only the 
latest one 
ï®Off-policy design permits learning from older data and varied 
behavior 
ï®Robotics settings benefit because physical samples arrive slowly and 
expensively 
43 


## Page 44

SAC Training Loop: Gradient Updates 
ï®Mini-batches sampled from replay drive several gradient steps per 
environment phase 
ï®Critics minimise soft Bellman error using the conservative double-Q 
target 
ï®Actor minimises the soft objective mixing Q-values and log-
probabilities 
ï®Temperature parameter updates toward a target entropy through its 
own loss 
ï®Target Q-networks track critics with an exponential moving average 
update 
44 


## Page 45

Off-Policy Efficiency and Real-World 
Example 
ï®Off-policy replay makes old experience useful for many updates 
ï®Sample efficiency significantly exceeds that of on-policy methods like 
PPO 
ï®Efficiency becomes crucial in real robots with wear-and-tear and 
reset costs 
ï®Minitaur experiments show SAC learning robust quadruped gaits on 
hardware within hours 
ï®Learned policies tolerate perturbations and terrain variations 
without catastrophic failure 
45 


## Page 46

Why SAC Works Well in Practice 
ï®Maximum-entropy objective encourages broad, persistent 
exploration 
ï®Stochastic policies remain less brittle to modelling errors and 
dynamics shifts 
ï®Twin critics with conservative targets prevent severe overestimation 
and instability 
ï®Off-policy formulation reuses data heavily while retaining a flexible 
stochastic actor 
ï®Automatic temperature tuning adapts exploration level across 
training stages 
46 


## Page 47

Practical Advantages and Conceptual 
Summary 
ï®SAC relies on first-order gradients and standard neural network 
components 
ï®Implementation complexity matches DDPG or TD3, simpler than 
trust-region methods 
ï®Single hyperparameter set often performs well across diverse 
continuous-control tasks 
ï®SAC resembles Q-learning in a maximum-entropy world plus an 
exponentiated-Q actor 
ï®Compared with PPO, SAC suits continuous actions and offers 
stronger sample efficiency 
47 


## Page 48

Modern Actor-Critic Methods 
1. Proximal Policy Optimization (PPO) 
2. Soft Actor-Critic (SAC) 
3. Dreamer 
48 


## Page 49

Dreamer: Model-Based â€œThinking Aheadâ€ 
ï®Dreamer is a model-based deep RL algorithm with a learned world 
model 
ï®DreamerV3 uses one configuration across hundreds of tasks without 
per-domain tuning 
ï®Control problem: act by â€œthinking aheadâ€ in the learned model, not 
just reacting 
ï®PPO and SAC learn directly from real transitions; Dreamer first 
learns an internal simulator 
ï®Separation: â€œunderstand the worldâ€ first, then â€œdecide what to doâ€ 
via imagined rollouts 
49 


## Page 50

From Model-Free RL to Latent World 
Models 
ï®Model-free methods (DQN, PPO, SAC) use the real environment for 
all next states 
ï®Every gradient step needs fresh experience, which leads to sample 
hunger 
ï®No way to ask â€œwhat if I tried this action elsewhere?â€ without 
executing it 
ï®Dreamer follows Dyna: learn a model, then use it for planning and 
policy learning 
ï®Model predicts latent dynamics instead of raw pixels or sensor 
streams 
50 


## Page 51

Latent Dynamics Model: Core Components 
ï®Encoder compresses each observation into a compact latent state 
ï®Recurrent state-space model predicts how the latent state evolves 
under actions 
ï®Decoders map latent states back to observations 
ï®Decoders also reconstruct rewards and continuation (episode) flags 
ï®Planning and imagination occur entirely in this low-dimensional 
latent space 
51 


## Page 52

Why Plan in Latent Space? 
ï®Agent never imagines raw images, only abstract latent states 
ï®Latent states summarise the aspects of the observation that matter 
for control 
ï®Latent planning avoids the curse of dimensionality of pixel-level 
prediction 
ï®Imagined trajectories become cheap to generate once the model is 
trained 
ï®Real experience mainly serves to refine this latent dynamics model 
52 


## Page 53

World Model Structure in Dreamer 
ï®Architecture: world model at the bottom, actorâ€“critic module on top 
ï®The latent state ğ‘ ğ‘¡ splits into deterministic recurrent state â„ğ‘¡ and 
stochastic ğ‘§ğ‘¡ 
ï®The encoder maps observation ğ‘¥ğ‘¡ into stochastic latent variable ğ‘§ğ‘¡ 
ï®The recurrent part updates hidden â„ğ‘¡ from previous hidden state and 
action 
ï®The pair â„ğ‘¡, ğ‘§ğ‘¡ defines the full model state ğ‘ ğ‘¡ 
53 


## Page 54

Predictions from the Model State 
ï®From ğ‘ ğ‘¡, the model predicts the reward at time t 
ï®Model predicts whether the episode continues or terminates 
ï®Model reconstructs the original observation ğ‘¥ğ‘¡ 
ï®Dynamics predictor forecasts next latent ğ‘§ğ‘¡+1 from current hidden 
state and action 
ï®Recurrent state-space model rolls forward without seeing new 
observations 
54 


## Page 55

Discrete Latents and Straight-Through 
Training 
ï®DreamerV3 represents each coordinate of ğ‘§ğ‘¡ as a small categorical 
distribution 
ï®Joint latent forms a vector of categorical variables rather than a 
Gaussian 
ï®Straight-through gradients make sampling appear discrete while 
gradients remain continuous 
ï®Backpropagation treats discrete choices as differentiable 
approximations 
ï®Discrete latents provide greater stability than fully continuous 
Gaussian latents 
55 


## Page 56

Three Losses for Learning the World Model 
ï®World model trains on replayed experience using multiple loss terms 
ï®Prediction loss encourages accurate reconstructions of observations, 
rewards, and continuation 
ï®Dynamics loss encourages forecasting of ğ‘§ğ‘¡+1 from current hidden 
state 
ï®Representation loss encourages latents that dynamics can predict 
reliably 
ï®Combined objective yields informative and predictable latent 
representations 
56 


## Page 57

Posteriorâ€“Prior KL and Free Bits 
ï®Dynamics and representation losses use KL divergence between 
posterior and prior 
ï®Posterior: encoder distribution conditioned on the current 
observation 
ï®Prior: dynamics predictor distribution conditioned only on past states 
and actions 
ï®DreamerV3 applies â€œfree bitsâ€, clipping KL terms below a threshold 
ï®Free bits prevent collapse into trivial, low-information latents that 
ignore inputs 
57 


## Page 58

Regularised Discrete Latents and 
Imagination 
ï®Small amount of uniform noise mixes into categorical distributions 
for ğ‘§ğ‘¡ 
ï®Noise stops latents from becoming perfectly deterministic and avoids 
KL spikes 
ï®Trained world model can roll forward from an initial latent plus an 
action sequence 
ï®Imagined trajectories evolve entirely in latent space without new 
visual input 
ï®Long-range video predictions (mazes, walking robots) illustrate this 
internal world model 
58 


## Page 59

Handling Scale: Symlog and Symexp 
ï®DreamerV3 must handle tiny and huge rewards, short and long 
horizons 
ï®Raw squared losses misbehave when reward scales differ by orders 
of magnitude 
ï®Symlog transform: 
ğ‘ ğ‘¦ğ‘šğ‘™ğ‘œğ‘”ğ‘¥= ğ‘ ğ‘–ğ‘”ğ‘›ğ‘¥ log ğ‘¥+ 1  
ï®Large magnitudes compress; small values near zero stay almost 
unchanged 
ï®Rewards, returns, even observations pass through symlog to keep 
gradients bounded 
59 


## Page 60

Inverting the Transform and Two-Hot 
Regression 
ï®Symexp is the inverse mapping back to original scale: 
ğ‘ ğ‘¦ğ‘šğ‘’ğ‘¥ğ‘ğ‘¦= ğ‘ ğ‘–ğ‘”ğ‘›ğ‘¦
exp ğ‘¦
âˆ’1  
ï®Dreamer predicts noisy scalars via distributions, not direct 
regression 
ï®Network outputs logits over exponentially spaced scalar bins 
ï®Two-hot targets split weight between the two nearest bins with 
interpolation 
ï®Cross-entropy compares soft targets and predictions, focusing 
gradients on probability mass shifts 
60 


## Page 61

Imagination Rollouts in Latent Space 
ï®After training, the world model compresses and predicts real 
experience 
ï®Actor and model start from latent states linked to replayed 
observations 
ï®Actor samples an action from its policy given the current latent state 
ï®World model predicts next latent state, reward, and continuation flag 
ï®Repeated application yields imagined trajectories of roughly 16 latent 
steps 
61 


## Page 62

Latent Distributional Critic and Î»-Returns 
ï®Critic consumes imagined rewards and latent states from these 
rollouts 
ï®It predicts a return distribution using categorical bins, spaced 
exponentially 
ï®Dreamer applies distributional value prediction instead of scalar 
values 
ï®Î»-returns mix multi-step returns with bootstrapping from later value 
estimates 
ï®This combination stabilizes training and propagates information 
across imagined horizons 
62 


## Page 63

Latent Actor Objective and Return 
Normalisation 
ï®Actor seeks actions maximizing return while preserving policy 
entropy 
ï®Actor gradient resembles REINFORCE: log-probabilities weighted by 
return-like signals 
ï®Returns are normalized using within-batch percentiles, not standard 
advantage normalization 
ï®Exponential moving average tracks return range, keeping effective 
scale near [0, 1] 
ï®Single entropy coefficient then works across sparse and dense 
reward domains 
63 


## Page 64

World Model as the Actorâ€“Criticâ€™s 
Environment 
ï®Actor and critic both operate purely on latent states ğ‘ ğ‘¡= â„ğ‘¡, ğ‘§ğ‘¡ 
ï®From their perspective, the world model is the environment 
ï®Real environment intervenes only during fresh data collection 
ï®Imagined rollouts supply most gradients for policy and value 
learning 
ï®Separation of modelling and control enables broad generalization 
across tasks 
64 


## Page 65

DreamerV3 Training Loop: Real Interaction 
ï®Current actor interacts with the real environment for several steps 
ï®Observations arrive and pass through the encoder into latent states 
ï®Actor samples actions based on these latent states, not raw pixels 
ï®True rewards and next observations are stored in a replay buffer 
ï®Replay buffer holds sequences that later train both world model and 
critic 
65 


## Page 66

Training the World Model from Replay 
ï®Algorithm samples sequences of experience from the replay buffer 
ï®Observations are encoded into latents for each time step in the 
sequence 
ï®Recurrent model rolls forward using logged actions and latent states 
ï®Model reconstructs observations, predicts rewards, and predicts 
continuation flags 
ï®Encoder, recurrent core, decoders, and reward/continue heads 
update via combined losses 
66 


## Page 67

Imagination Rollouts and Latent Actorâ€“
Critic 
ï®Dreamer selects latent states at sequence ends as starting points for 
imagination 
ï®World model and actor simulate future steps entirely in latent space 
ï®Reward head provides imagined rewards that define imagined return 
sequences 
ï®Critic receives Î»-returns from imagined rollouts as training targets 
ï®Actor receives policy gradients based on normalised returns and an 
entropy bonus 
67 


## Page 68

Critic Anchoring and Self-Consistent 
Universe 
ï®Critic sometimes also trains on real latent trajectories from replay 
ï®Real trajectories anchor value estimates directly to actual rewards 
ï®Critic parameters move toward an exponential moving average of 
themselves 
ï®This regularisation acts as a soft target-network mechanism 
ï®World model predicts, critic evaluates, and actor chooses, forming a 
self-consistent universe 
68 


## Page 69

DreamerV3 Robustness Across Domains 
ï®Single hyperparameter setting works across more than 150 tasks 
ï®Benchmarks include Atari, DeepMind Lab, ProcGen, control suites, 
BSuite, and Minecraft 
ï®KL balancing with free bits prevents model collapse in simple and 
complex environments 
ï®Symlog, symexp, and two-hot losses normalise signal scales 
consistently 
ï®Percentile-based return normalisation supports one entropy scale for 
sparse and dense rewards 
69 


## Page 70

Further Robustness Mechanisms and 
Minecraft Example 
ï®Distributional critic decouples gradient scale from raw return 
magnitude 
ï®Replay ratio and model size scale performance smoothly without 
retuning 
ï®Ablations show monotonic gains as model size and replay ratio 
increase 
ï®Same configuration learns locomotion, navigation, and visual tasks 
without domain-specific tweaks 
ï®In Minecraft, DreamerV3 discovers diamond tools, outperforming 
competitors stuck at iron tools 
70 


## Page 71

Relation to PPO and SAC 
ï®Compared with PPO, Dreamer uses a model-based path instead of 
clipped on-policy gradients 
ï®PPO relies on large batches of fresh data and discards them quickly 
ï®Dreamer reuses data heavily to train both world model and latent 
actorâ€“critic 
ï®Compared with SAC, Dreamer trades direct soft Q-learning for latent 
imagination rollouts 
71 


## Page 72

Conceptual Summary of Dreamer 
ï®Dreamer promotes â€œunderstand the world first, then dream before 
actingâ€ 
ï®Recurrent state-space model compresses experience into latents 
predicting observations, rewards, and continuation 
ï®Symlog, symexp, and two-hot encodings stabilize learning across 
diverse reward scales 
ï®Actorâ€“critic learns from imagined trajectories with normalized 
returns and a fixed entropy bonus 
ï®World models plus careful normalization emerge as a promising 
route toward general deep RL 
72 


## Page 73

Conclusions 
ï®PPO offers stable on-policy learning with clipped updates, strong 
robustness and reliable performance across diverse continuous 
control tasks 
ï®SAC achieves high sample efficiency and strong exploration through 
entropy maximization, excelling on challenging, stochastic 
environments and high-dimensional action spaces 
ï®DreamerV3 uses world models for long-horizon planning, gains 
strong sample efficiency and generalization in complex, partially 
observable domains 
73 
