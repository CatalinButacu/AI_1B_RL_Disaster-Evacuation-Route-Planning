# RL07_Eligibility

**Source:** c:\Users\catalin.butacu\Downloads\RL\resourses\lecture\RL07_Eligibility.pdf

**Pages:** 37

---


## Page 1

Reinforcement Learning 
7. Eligibility Traces 
 
Florin Leon 
 
â€œGheorghe Asachiâ€ Technical University of IaÈ™i, Romania 
Faculty of Automatic Control and Computer Engineering 
 
https://florinleon.byethost24.com/lect_rl.html 
 
2025 
 


## Page 2

Eligibility Traces 
1. The Î»-return 
2. TD(Î») 
3. Sarsa(Î») 
2 


## Page 3

Eligibility Traces 
1. The Î»-return 
2. TD(Î») 
3. Sarsa(Î») 
 
 
3 


## Page 4

Eligibility Traces 
ï®Eligibility traces form a widely used mechanism in RL algorithms 
ï®They unify and generalize TD and MC methods 
ï®TD(Î») and many temporal-difference methods, e.g., Q-learning or 
Sarsa, use eligibility traces 
ï®Parameter Î» extends methods from MC (Î» = 1) to one-step TD (Î» = 0) 
4 


## Page 5

n-step Returns and Compound Updates 
ï®n-step return uses the first n rewards plus a discounted value 
estimate at step t + n 
  
ğºğ‘¡:ğ‘¡+ğ‘›= ğ‘…ğ‘¡+1 + ğ›¾ğ‘…ğ‘¡+2 + â‹¯+ ğ›¾ğ‘›âˆ’1ğ‘…ğ‘¡+ğ‘›+ ğ›¾ğ‘›ğ‘£ ğ‘†ğ‘¡+ğ‘›, ğ‘¤ğ‘¡+ğ‘›âˆ’1  
 
0 â‰¤ğ‘¡â‰¤ğ‘‡âˆ’ğ‘› 
  
ï®Any n-step return forms a valid update target for tabular or 
approximate value learning 
ï®We may update toward a weighted average of several n-step returns 
ï®The weights are positive and sum to 1, e.g.: 
  
1
2 ğºğ‘¡:ğ‘¡+2 + 1
2 ğºğ‘¡:ğ‘¡+4 
5 
optional 


## Page 6

The Î»-return 
ï®The Î»-return is defined as: 
ğºğ‘¡
ğœ†= 1 âˆ’ğœ† ğœ†ğ‘›âˆ’1ğºğ‘¡:ğ‘¡+ğ‘›
âˆ
ğ‘›=1
 
ï®Each ğºğ‘¡:ğ‘¡+ğ‘› is an n-step return  
ï®The factor 1 âˆ’ğœ†ğœ†ğ‘›âˆ’1 is the weight of that n-step return 
ï®These weights are all positive and sum to 1 
ï®Small Î»: mostly short n-step returns 
ï®More bootstrapping, more bias, less variance 
ï®Large Î»: mostly long n-step, MC-like returns 
ï®Less bias, more variance 
6 


## Page 7

Example 
ï®Time steps: t = 0, 1, 2, 3 with S3 terminal 
ï®Discount factor: Î³ = 1 
ï®Rewards: R1 = 0, R2 = 0, R3 = 1 
ï®Current value estimates: ğ‘£ ğ‘†1 = 0.3, ğ‘£ ğ‘†2 = 0.4, ğ‘£ ğ‘†3 = 0 (terminal) 
ï®We want the Î»-return ğº0
ğœ† at time t = 0 
7 


## Page 8

Computing the n-step Returns from t = 0 
ï®
ğºğ‘¡:ğ‘¡+ğ‘›= ğ‘…ğ‘¡+1 + ğ›¾ğ‘…ğ‘¡+2 + â‹¯+ ğ›¾ğ‘›âˆ’1ğ‘…ğ‘¡+ğ‘›+ ğ›¾ğ‘›ğ‘£ ğ‘†ğ‘¡+ğ‘› 
ï®1-step return: ğº0:1 = ğ‘…1 + ğ‘£ ğ‘†1 = 0 + 0.3 = 0.3 
ï®2-step return: ğº0:2 = ğ‘…1 + ğ‘…2 + ğ‘£ ğ‘†2 = 0 + 0 + 0.4 = 0.4 
ï®3-step return: the episode ends at step 3, so no bootstrap term: 
  
ğº0:3 = ğ‘…1 + ğ‘…2 + ğ‘…3 = 0 + 0 + 1 = 1.0 
ï®
ğº0:3 is just the full MC return ğº0 in this episode 
8 


## Page 9

The Finite-Horizon Î»-return 
ï®For episodic tasks, the forward-view Î»-return at time t with terminal 
time T can be written as: 
  
ğºğ‘¡
ğœ†= 1 âˆ’ğœ†
 ğœ†ğ‘›âˆ’1ğºğ‘¡:ğ‘¡+ğ‘›
ğ‘‡âˆ’ğ‘¡âˆ’1
ğ‘›=1
+ ğœ†ğ‘‡âˆ’ğ‘¡âˆ’1ğºğ‘¡ 
ï®Here, T = 3, t = 0 â‡’ T â€“ t â€“ 1 = 2 
ï®
â‡’ğº0
ğœ†= 1 âˆ’ğœ†
ğº0:1 + ğœ†ğº0:2 + ğœ†2ğº0 
ï®and ğº0 = ğº0:3 = 1.0 
ï®We combine: 
ï®Weight 1 âˆ’ğœ† on the 1-step return 
ï®Weight 1 âˆ’ğœ†ğœ† on the 2-step return 
ï®Weight ğœ†2 on the full MC return 
 
9 


## Page 10

Computing the Î»-return 
ï®Let Î» = 0.5 
ï®Weights: 
ï®For ğº0:1: 1 âˆ’ğœ†=  0.5 
ï®For ğº0:2: 1 âˆ’ğœ†ğœ†= 0.5 â‹…0.5 = 0.25 
ï®For ğº0: ğœ†2 = 0.25 
ï®
â‡’ğº0
0.5= 0.5 â‹…ğº0:1 + 0.25 â‹…ğº0:2 + 0.25 â‹…ğº0  
ï®
ğº0
0.5 = 0.5 â‹…0.3 + 0.25 â‹…0.4 + 0.25 â‹…1 = 0.5 
ï®The extremes: 
ï®Î» = 0 â†’ ğº0
0 = ğº0:1 = 0.3 (pure one-step TD) 
ï®Î» = 1 â†’ ğº0
1 = ğº0 = 1 (pure MC) 
10 


## Page 11

Backup Diagram for TD(Î») 
11 


## Page 12

The Î»-return as a Geometric Average 
ï®TD(Î») defines a compound update averaging all n-step returns using 
ğœ†âˆˆ0,1  
ï®Each ğºğ‘¡:ğ‘¡+ğ‘› receives a weight proportional to ğœ†ğ‘›âˆ’1; the factor 1 âˆ’ğœ† 
normalizes the weights (ğºğ‘¡
ğœ†= 1 âˆ’ğœ† 
ğœ†ğ‘›âˆ’1ğºğ‘¡:ğ‘¡+ğ‘›
âˆ
ğ‘›=1
) 
ï®Weights form a geometric sequence: 1 âˆ’ğœ†, 1 âˆ’ğœ†ğœ†, 1 âˆ’ğœ†ğœ†2, â€¦ 
ï®After termination, all later n-step returns equal ğºğ‘¡, yielding an 
equivalent finite-sum form: ğºğ‘¡
ğœ†= 1 âˆ’ğœ† 
ğœ†ğ‘›âˆ’1ğºğ‘¡:ğ‘¡+ğ‘›
ğ‘‡âˆ’ğ‘¡âˆ’1
ğ‘›=1
+ ğœ†ğ‘‡âˆ’ğ‘¡âˆ’1ğºğ‘¡ 
ï®At Î» = 1, Î»-return gives an MC update; at Î» = 0, one-step TD 
12 


## Page 13

Return Weights 
ï®Weighting given in the Î»-return to each of the n-step returns 
13 


## Page 14

The Off-line Î»-return Algorithm 
ï®The off-line Î»-return algorithm keeps the weight vector unchanged 
during each episode 
ï®After the episode ends, semi-gradient updates are applied for all time 
steps ğ‘¡= 0, â€¦ , ğ‘‡âˆ’1: 
ğ‘¤ğ‘¡+1 = ğ‘¤ğ‘¡+ ğ›¼ğºğ‘¡
ğœ†âˆ’ğ‘£ ğ‘†ğ‘¡, ğ‘¤ğ‘¡
âˆ‡ğ‘£ ğ‘†ğ‘¡, ğ‘¤ğ‘¡ 
14 


## Page 15

Example: 19-State Random Walk 
ï®
In both cases, intermediate values of the bootstrapping parameter (Î» or n) 
performed best 
ï®
The results with the off-line Î»-return algorithm are slightly better at the best 
values of Î± and Î», and at high Î± 


## Page 16

Forward (Theoretical) View of Î»-return 
Methods 
ï®
The forward view defines each update using future rewards and states following the updated 
state 
ï®
Intuition: ride along the state sequence, updating each state once from its own vantage point 
ï®
After updating a state from its vantage point, the algorithm never revisits that state again 
ï®
Future states appear in many updates, each time viewed from an earlier preceding state 
ï®
The forward view is theoretical; equivalent, more efficient implementations exist 
16 


## Page 17

Eligibility Traces 
1. The Î»-return 
2. TD(Î») 
3. n-step Truncated Î»-return Methods 
4. Sarsa(Î») 
 
 
17 


## Page 18

Forward and Backward Views 
ï®
In the forward view, the value of a state is updated using one or multiple future 
time steps 
ï®
It is mathematically elegant but inconvenient for online, step-by-step learning 
ï®
At time t, the agent does not yet observe the rewards that are many steps in the 
future 
ï®
Waiting until episode termination to update all visited states wastes data and 
computation 
ï®
The backward view replaces â€œlook forward from past statesâ€ with local TD 
errors and credit assignment backward in time 
ï®
Example of a credit assignment problem: did the bell or the light cause the shock? 
 
ï®
Forward and backward views can produce identical or nearly identical learning 
updates 
ï®
Eligibility traces (in the backward view) allow more efficient implementations 
18 


## Page 19

The Backward View 
ï®
At each time step t, we compute a TD error Î´t from the most recent transition 
ï®
We propagate this TD error backward to earlier states with a decaying influence 
over time 
ï®
Picture a TD error Î´t shouted backward through time, increasingly muffled for 
older states 
ï®
The backward view implements Î»-returns approximately using only online 
information and local updates 
19 


## Page 20

Eligibility Traces with Function 
Approximation 
ï®With function approximation, we use a weight vector ğ‘¤ğ‘¡âˆˆâ„ğ‘‘ 
ï®The eligibility trace is another vector ğ‘§ğ‘¡âˆˆâ„ğ‘‘ 
ï®The weight vector acts as long-term memory, which accumulates 
knowledge over the whole learning process 
ï®The eligibility trace acts as short-term memory, typically shorter than 
the episode duration 
ï®The trace only affects learning indirectly through its influence on 
later weight updates 
20 


## Page 21

TD(Î») 
ï®TD(Î») is an early, widely used RL algorithm, the first algorithm with 
a formal forward-backward equivalence using eligibility traces 
ï®It empirically approximates the off-line Î»-return algorithm 
ï®It improves over off-line Î»-return by updating weights on every time 
step 
ï®It also applies naturally to continuing tasks 
21 


## Page 22

Eligibility Traces in TD(Î») 
ï®
TD(Î») maintains an eligibility trace for each component of the weight 
vector wt 
ï®The trace vector zt has the same dimension as the weight vector wt  
ï®
When a feature is active at time t, the corresponding component of zt is 
increased 
ï®
At every time step, all eligibility components decay by a factor Î³ Â· Î» 
ï®
The eligibility trace captures which parameters recently influenced the 
current value estimate and deserve more credit or blame 
ï®
Eligibility traces are short-term memory variables that decay over time 
and control how strongly current TD errors update the learned values 
ï®Tabular case: one trace per state (or state-action pair) 
ï®Function approximation: one trace per parameter (or feature) 
22 


## Page 23

Update Equations 
ï®The eligibility trace update: 
  
ğ³ğ‘¡= ğ›¾ ğœ† ğ³ğ‘¡âˆ’1 + âˆ‡ğ‘£ ğ‘†ğ‘¡, ğ°ğ‘¡ 
ï®The term âˆ‡ğ‘£ ğ‘†ğ‘¡, ğ°ğ‘¡ marks the weights that helped produce the 
current estimate as eligible 
ï®The factor Î³Î» ensures that older contributions gradually fade from 
the eligibility trace 
23 
accumulating eligibility trace 
times of visits to a state 


## Page 24

Update Equations 
ï®One-step TD error: 
  
ğ›¿ğ‘¡= ğ‘…ğ‘¡+1 + ğ›¾ ğ‘£ ğ‘†ğ‘¡+1, ğ°ğ‘¡âˆ’ğ‘£ ğ‘†ğ‘¡, ğ°ğ‘¡ 
ï®Weight update with eligibility traces: 
  
ğ°ğ‘¡+1 = ğ°ğ‘¡+ ğ›¼ ğ›¿ğ‘¡ ğ³ğ‘¡ 
vs.  ğ°ğ‘¡+1 = ğ°ğ‘¡+ ğ›¼ ğ›¿ğ‘¡ âˆ‡ğ‘£ ğ‘†ğ‘¡, ğ°ğ‘¡ 
24 


## Page 25

TD(Î») Behavior 
ï®TD(Î») closely matches the ideal offline Î»-return algorithm when step 
size Î± is small enough 
ï®Performance is often best at an intermediate Î» value 
ï®Extreme values can decrease efficiency or increase bias 
 
ï®Metaphor: each visited state or feature â€œglowsâ€ after visitation 
ï®The glow intensity fades at rate Î³Î» as time passes 
ï®The current reward prediction error distributes onto all glowing 
states in proportion to their remaining brightness 
25 


## Page 26

Forward-Backward Equivalence 
ï®Each time step, we update the trace zt by adding current features and 
decaying previous entries 
ï®We compute the current TD error Î´t from the observed reward and 
the bootstrapped next-state value 
ï®We update all weights in proportion to Î´t and their current eligibility 
zt 
ï®If w is held fixed during an episode, the backward-view TD(Î») 
matches the forward Î»-return updates 
ï®The total parameter change then equals that from using forward-
view Î»-returns at each time step of the episode 
26 


## Page 27

Î» as Temporal Credit-Assignment Memory 
ï®In the backward view, Î» controls how long past states remain eligible 
ï®Î» = 0: the traces reset immediately; only the current state has 
nonzero eligibility, i.e., TD(0) 
ï®0 < Î» < 1: eligibility decays geometrically 
ï®A state k steps back has a trace (Î³Î»)k 
ï®Such states still receive credit from current updates, but less than more 
recent states 
ï®Î» = 1: the traces decay only with Î³; in episodic tasks every state 
shares credit equally, i.e., TD(1), Monte Carlo 
27 


## Page 28

Eligibility Traces 
1. The Î»-return 
2. TD(Î») 
3. Sarsa(Î») 
 
 
28 


## Page 29

From State Values to Action Values: 
Sarsa(Î») 
ï®Control usually needs action values q(s, a), not only state values v(s) 
ï®We approximate action values by ğ‘ ğ‘ , ğ‘, ğ‘¤ 
ï®Eligibility trace ideas from TD(Î») transfer almost directly to action 
values 
ï®The forward view uses action-value Î»-returns; the backward view 
gives Sarsa(Î») 
29 


## Page 30

Action-Value n-step Returns 
ï®Action-value n-step return for ğ‘¡+ ğ‘› <  ğ‘‡: 
  
ğºğ‘¡:ğ‘¡+ğ‘›= ğ‘…ğ‘¡+1 + â‹¯+ ğ›¾ğ‘›âˆ’1ğ‘…ğ‘¡+ğ‘›+ ğ›¾ğ‘›ğ‘ ğ‘†ğ‘¡+ğ‘›, ğ´ğ‘¡+ğ‘›, ğ°ğ‘¡+ğ‘›âˆ’1  
ï®For ğ‘¡+ ğ‘› â‰¥ğ‘‡, ğºğ‘¡:ğ‘¡+ğ‘›= ğºğ‘¡ 
ï®These returns bootstrap from later action-value estimates 
ï®They provide the building blocks for the action-value Î»-return 
30 


## Page 31

Forward View Î»-return for Action Values 
ï®Combine action-value n-step returns into a Î»-return ğºğ‘¡
ğœ† 
ï®Definition matches the state-value Î»-return, now using ğºğ‘¡:ğ‘¡+ğ‘› for 
action values 
ï®Off-line action-value Î»-return algorithm: 
  
ğ°ğ‘¡+1 = ğ°ğ‘¡+ ğ›¼ğºğ‘¡
ğœ†âˆ’ğ‘ ğ‘†ğ‘¡, ğ´ğ‘¡, ğ°ğ‘¡
âˆ‡ğ‘ ğ‘†ğ‘¡, ğ´ğ‘¡, ğ°ğ‘¡, ğ‘¡= 0, â€¦ , ğ‘‡âˆ’1 
ï®and ğºğ‘¡
ğœ†= ğºğ‘¡:âˆ
ğœ† in long episodic or continuing tasks 
31 


## Page 32

Sarsa(Î») Update Rule 
ï®Sarsa(Î») approximates the action-value Î»-return algorithm by TD 
learning 
ï®Parameter update keeps the TD(Î») form: 
  
ğ°ğ‘¡+1 = ğ°ğ‘¡+ ğ›¼ ğ›¿ğ‘¡ ğ³ğ‘¡ 
ï®Action-value TD error: 
  
ğ›¿ğ‘¡= ğ‘…ğ‘¡+1 + ğ›¾ğ‘ ğ‘†ğ‘¡+1, ğ´ğ‘¡+1, ğ°ğ‘¡âˆ’ğ‘ ğ‘†ğ‘¡, ğ´ğ‘¡, ğ°ğ‘¡ 
ï®We replace ğ‘£ ğ‘†ğ‘¡, ğ°ğ‘¡ with ğ‘ ğ‘†ğ‘¡, ğ´ğ‘¡, ğ°ğ‘¡ in all TD(Î») formulas 
32 


## Page 33

Eligibility Traces for Action Values 
ï®Sarsa(Î») maintains an eligibility trace vector zt over parameters 
ï®The trace is initialized at the start of an episode: ğ³âˆ’1 = 0 
ï®For 0 â‰¤ğ‘¡ â‰¤ğ‘‡: 
ğ³ğ‘¡= ğ›¾ ğœ† ğ³ğ‘¡âˆ’1 + âˆ‡ğ‘ ğ‘†ğ‘¡, ğ´ğ‘¡, ğ°ğ‘¡ 
ï®Some optimizations are possible in the special case of binary features 
33 


## Page 35

Example: Gridworld 
ï®
Initial action values are 0, a positive reward is received only at goal state G 
ï®
One-step Sarsa increases only the value of the final action in the episode 
ï®
n-step Sarsa increases equally the last n action-values when Î³ = 1 
ï®
Sarsa(Î») with Î» = 0.9 updates all actions along the path, fading with recency 
ï®
Fading credit suits tasks where early actions matter, but are less trusted than later actions 
35 


## Page 36

Conclusions 
ï®Î»-return is a geometric average of n-step returns, which interpolates 
between TD(0) and MC bias-variance extremes 
ï®The forward view offers analytical clarity, while the backward view 
with eligibility traces yields equivalent online updates from local TD 
errors 
ï®TD(Î») and Sarsa(Î») share fading-memory credit assignment 
controlled by Î³Î» and step-size Î± 
ï®Eligibility traces often improve data efficiency dramatically in control 
problems 
36 


## Page 37

Main Reference 
ï®Sutton, R.S. and Barto, A.G. (2018). Reinforcement Learning: An 
Introduction. 2nd edition. MIT Press, Cambridge, MA. 
http://incompleteideas.net/book/the-book-2nd.html 
37 
