# RL11_AlphaGo

**Source:** c:\Users\catalin.butacu\Downloads\RL\resourses\lecture\RL11_AlphaGo.pdf

**Pages:** 48

---


## Page 1

Reinforcement Learning 
11. The AlphaGo Family 
 
Florin Leon 
 
â€œGheorghe Asachiâ€ Technical University of IaÈ™i, Romania 
Faculty of Automatic Control and Computer Engineering 
 
https://florinleon.byethost24.com/lect_rl.html 
 
2025 
 


## Page 2

The AlphaGo Family 
1. AlphaGo 
2. AlphaGo Zero 
3. AlphaZero 
4. MuZero 
2 


## Page 3

The AlphaGo Family 
1. AlphaGo 
2. AlphaGo Zero 
3. AlphaZero 
4. MuZero 
3 


## Page 4

Go 
ï®
19Ã—19 board with 361 intersections 
ï®
Size of state space: 10170 
ï®
Early and middle game states often offer 100â€“200 legal moves, far above chess 
ï®
Games typically last several hundred moves 
ï®
Reward is almost entirely delayed: a single terminal signal, +1 win or â€“1 loss 
Atari 
4 


## Page 5

Why Go Is Difficult to Learn 
ï®Random or near-random games teach almost nothing about good 
local tactics or global strategy 
ï®Credit assignment horizons are very long (hundreds of moves); 
crucial early exchanges can determine results far later 
ï®TD learning can in principle backpropagate value, yet noisy 
intermediate decisions introduce massive variance 
ï®The value function should unify local capture patterns with global 
territory, influence, and balance; small local changes can flip results 
5 


## Page 6

Monte Carlo Tree Search 
ï®The difficulty of Go motivates explicit lookahead: players imagine 
concrete futures instead of relying on value approximation 
ï®Monte Carlo Tree Search (MCTS) concentrates simulations on 
promising or uncertain actions, guided by statistics from previous 
rollouts 
ï®Early Go engines paired MCTS with hand-crafted pattern rules, 
heuristic rollouts, and static territory or influence evaluations 
ï®These systems reached strong amateur level but missed subtle long-
term sacrifices and global trade-offs 
6 


## Page 7

AlphaGo: Overall Architecture 
ï®AlphaGo combines deep policy networks, a value network, and MCTS 
ï®It uses Go-specific input features and learned representations to 
guide search efficiently 
ï®It separates what to play (policy) from who is winning (value) 
ï®It relies on both human expert data and self-play  
ï®It is a solution between early hand-crafted Go programs and later, 
more general successors 
7 


## Page 8

Policy and Value Networks 
8 


## Page 9

Input Features 
ï®The Go board is seen as a 19Ã—19 grid with multiple feature planes 
ï®The feature planes include black stones, white stones, liberties, 
captures, move history, etc. 
ï®For each position on the game board there are 48 binary or integer 
features, for example: 
ï®
Whether the position is occupied by AlphaGo, by the opponent, or is empty 
ï®
The number of adjacent empty positions 
ï®
The number of the opponentâ€™s stones that would be captured by placing a stone 
on that position 
ï®
How many moves ago a stone was placed there, etc. 
ï®Stacked planes form an image-like tensor input to convolutional 
layers 
9 


## Page 10

Training 
10 


## Page 11

1. Supervised Policy Network 
ï®The first component is a supervised policy network pÏƒ â€‹(a âˆ£ s) trained 
to imitate expert human moves 
ï®Input: encoded Go position s 
ï®Output: probabilities over legal moves a 
ï®Trained on professional game records using cross-entropy loss (with 
a one-hot expert target): 
 
  
ï®Learns a strong human-like prior over plausible moves in many 
positions 
ï®Can play at strong amateur level without additional search 
11 


## Page 12

2. RL Policy Network 
ï®The second component is a reinforcement learning policy network 
pÏ â€‹(a âˆ£ s) initialized with the supervised policy parameters pÏƒ 
ï®The ğ‘ğœŒ is improved through self-play: two copies of the current 
policy network play full games against each other (no MCTS) 
ï®Each game receives a return z = +1 for win and z = â€“1 for loss 
ï®The policy is updated with REINFORCE-style loss: 
 
 
ï®The moves from winning games become more probable, and vice 
versa. The shared weights are updated (no separate teacher-student) 
ï®This helps the network to surpass human performance 
12 


## Page 13

3. Value Network 
ï®The third component is a value network that predicts a winning 
probability (for the game) outcome from a position s: ğ‘£ğœƒğ‘ âˆˆâˆ’1,1  
ï®It is trained on self-play positions with the known final result  
ğ‘§âˆˆ{âˆ’1, +1}  
ï®It uses MSE regression loss: 
  
ğ¿ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’= ğ‘£ğœƒğ‘ âˆ’ğ‘§2 
13 


## Page 14

4. Rollout Policy Network 
ï®The fourth component is the rollout policy network: a small, shallow 
convolutional network trained from expert games, used only inside 
MCTS rollouts during search 
ï®It generates fast, plausible (but not necessarily smart) moves to 
replace random play; this reduces rollout variance and improves value 
estimates 
 
ï®The final leaf evaluation uses both rollout and value: 
  
ğ‘‰ğ‘™ğ‘’ğ‘ğ‘“= 1 âˆ’ğœ† ğ‘§ğ‘Ÿğ‘œğ‘™ğ‘™ğ‘œğ‘¢ğ‘¡+ ğœ† ğ‘£ğœƒğ‘  
14 


## Page 15

5. MCTS 
ï®During gameplay, AlphaGo uses MCTS augmented with policy and 
value networks 
ï®At each node, the policy network initializes the priors ğ‘ƒğ‘ğ‘ ) over 
child moves 
ï®The leaf nodes are evaluated using the blended estimate Vleaf 
ï®Action selection uses PUCT (Policy-UCB applied to Trees): 
15 


## Page 16

Alpha Go 
ï®
The supervised policy network ğ‘ğœ is trained on expert games 
ï®
The RL policy ğ‘ğœŒ is initialized from ğ‘ğœ and improves via self-play 
ï®
Self-play games are generated using ğ‘ğœŒ and the value network ğ‘£ğœƒ is 
trained on the game results 
ï®
A fast rollout policy is trained from human games for efficient playouts 
ï®
The networks and MCTS are combined into the final AlphaGo system 
 
ï®
Alpha Go demonstrates that deep neural networks can provide strong 
priors and evaluations for search in complex games 
ï®
It still depends on human data and Go-specific features 
 
ï®
Google DeepMind: AlphaGo - The Movie 
https://www.youtube.com/watch?v=WXuK6gekU1Y 
16 


## Page 17

The AlphaGo Family 
1. AlphaGo 
2. AlphaGo Zero 
3. AlphaZero 
4. MuZero 
17 


## Page 18

AlphaGo Zero 
ï®It learns Go from scratch using only the game rules, without human 
games or features 
ï®It maintains the AlphaGo skeleton: deep networks + MCTS 
ï®It uses a single network for both move selection and position 
evaluation 
ï®Self-play games provide all training data 
ï®It can be viewed as approximate policy iteration:  
network â†’ MCTS â†’ self-play data â†’ network update â†’ repeat 
18 


## Page 19

Unified Policy-Value Network 
ï®The network maps a Go state s to both policy and value: 
   
ğ‘“ğœƒğ‘ = ğ‘, ğ‘£ 
ï®p: the probability distribution over all moves, including pass 
ï®
ğ‘£âˆˆâˆ’1,1 : the predicted win probability for the current player 
ï®AlphaGo Zero uses a residual network (ResNet) instead of a simple 
convolutional network 
 
 
 
 
 
ï®The network has a shared part and two heads (for p and v) 
 
ResNet illustration, not AlphaGo Zero 
19 


## Page 20

Input Encoding 
ï®The board is represented as a 19Ã—19Ã—17 tensor of binary feature 
planes 
ï®8 planes for current playerâ€™s stones over the last 8 time steps 
ï®8 planes for opponentâ€™s stones over the last 8 time steps 
ï®1 plane indicating which color is to move 
ï®No explicit domain-specific tactical features 
20 


## Page 21

Architecture 


## Page 22

MCTS Steps 
ï®At a node, AlphaGo Zero calls the network ğ‘“ğœƒ for p and v 
ï®There is no rollout policy; the value head v replaces the random 
playouts entirely 
ï®The priors p initialize new child edges in the tree 
ï®The value v backs up through the path to update the Q estimates 
ï®The PUCT action selection rule is used 
ï®The root policy is updated from the visit counts: 
  
ğœ‹ğ‘
ğ‘ 0
âˆğ‘ğ‘ 0, ğ‘1/ğœ 
ï®Ï„ is the exploration temperature 
ï®Ï„ is higher early in training to encourage exploration, and near 0 later so the 
agent picks the most visited move 
22 


## Page 23

Self-Play Data Generation 
ï®AlphaGo Zero plays games against itself using MCTS and the current 
network 
ï®At each move, it samples the action from root policy ğœ‹â‹…ğ‘ ğ‘¡ 
ï®The game ends with an outcome ğ‘§âˆˆ{âˆ’1, +1} 
ï®For each timestep t, training tuple ğ‘ ğ‘¡, ğœ‹ğ‘¡, ğ‘§ğ‘¡ are stored 
ï®The data encode both search behavior and true long-term outcomes 
ï®A replay buffer accumulates many such triplets from many games 
23 


## Page 24

Training Targets 
ï®st is the encoded position seen by the network during play 
ï®Ï€t is the improved policy from MCTS visit counts at state st  
ï®zt is the final game result from the perspective of player to move at t 
ï®Ï€t acts as a stronger teacher than raw network policy 
ï®zt provides a ground truth signal for long-horizon value prediction 
24 


## Page 25

Loss Function 
ï®For one position, the network outputs ğ‘ğ‘¡, ğ‘£ğ‘¡= ğ‘“ğœƒğ‘ ğ‘¡ 
ï®AlphaGo Zero uses a combined loss: 
  
ğ¿ğœƒ= ğ‘§âˆ’ğ‘£2 âˆ’ ğœ‹(ğ‘|ğ‘ )
ğ‘
â‹…log ğ‘(ğ‘|ğ‘ ) + ğ‘ğœƒ2 
ï®The first term: squared error that drives the value vt toward the 
game outcome zt 
ï®The second term: cross-entropy that pushes policy pt toward the 
search policy Ï€t 
ï®The regularization term ğ‘ğœƒ2 stabilizes training and discourages 
overfitting 
25 


## Page 26

Outer Training Loop 
ï®Step 1: Generate self-play games using the current best network Î¸best 
and MCTS 
ï®Step 2: Collect ğ‘ ğ‘¡, ğœ‹ğ‘¡, ğ‘§ğ‘¡ into the replay buffer 
ï®Step 3: Train the network with gradient descent on batches from the 
buffer and obtain an updated network Î¸newâ€‹ 
ï®Step 4: Evaluate Î¸new  versus the current best Î¸best in direct matches 
ï®If the new network wins more than some threshold (e.g., 55%), promote it 
to be the new best 
ï®Otherwise keep the old best network and continue training 
26 


## Page 27

AlphaGo Zero vs. AlphaGo 
ï®AlphaGo Zero removes the need for human game data; learning 
relies entirely on self-play 
ï®It discards handcrafted Go features and uses only stone histories and 
side-to-move 
ï®It eliminates the rollout policy; the value head provides all leaf 
evaluations 
ï®It uses a single network for both policy and value, which simplifies 
the architecture 
ï®It represents a cleaner, more general template for search-guided 
reinforcement learning 
27 


## Page 28

Performance 
28 


## Page 29

The AlphaGo Family 
1. AlphaGo 
2. AlphaGo Zero 
3. AlphaZero 
4. MuZero 
29 


## Page 30

AlphaZero 
ï®AlphaZero is based on the AlphaGo Zero architecture 
ï®It mastered Go, chess, and shogi (Japanese chess) from scratch using 
only self-play and the game rules, with no human examples 
ï®AlphaZero uses the same architecture and learning algorithm for all 
three games:  
ï®A deep residual neural network with 19 blocks  
ï®Shared weights for both policy and value outputs  
ï®Monte Carlo Tree Search 
ï®The input encoding depends on the game 
ï®For Go, AlphaZero uses the same 17-plane encoding as AlphaGo Zero 
30 


## Page 31

Input Encoding: Chess 
ï®
119 binary planes: 
ï®
Planes for each piece type and color over a short history, e.g., where white pawns 
were for several recent moves, where the black queen was, etc. 
ï®
Extra planes for castling rights, side to move, move counters (like the fifty-move 
rule), and similar rule-related information 
31 
8Ã—8 board 


## Page 32

Input Encoding: Shogi 
ï®
Different rules: captured pieces can be reinserted into the game (by the 
captor), promotions differ from chess, piece moves are also slightly different 
ï®
The encoding expands to 362 planes, including piece-in-hand information 
and promotion-related flags 
32 
9Ã—9 board 


## Page 33

Handling Draws 
ï®Go training in AlphaGo Zero considers only win or loss outcomes 
ï®Chess and, to a lesser extent, shogi require incorporating draws and 
repetition-related termination rules 
ï®The value head must represent {â€“1, 0, +1} as outcomes 
ï®Self-play can naturally include draws; no special-case heuristic is 
needed 
33 


## Page 34

Performance 
ï®AlphaZero defeated Stockfish, the strongest traditional chess engine, 
after 4 hours of self-play training 
ï®It beat Elmo, a top shogi engine, after 2 hours of training  
ï®AlphaZero searched far fewer nodes than Stockfish or Elmo in chess 
and shogi, because it can focus search on promising paths 
 
ï®Its playstyle is often aggressive and unconventional compared to 
traditional engines 
34 


## Page 35

AlphaZero vs. AlphaGo Zero 
ï®
In Go, AlphaZero surpassed AlphaGo Zero after 24 hours of training, 
despite reusing the same architecture 
ï®
AlphaZero no longer used the evaluation phase (Î¸new  vs. Î¸best); it 
continuously updated the same network with the latest self-play data 
ï®
It used a batch size of 4096, double the 2048 used by AlphaGo Zero 
ï®
DeepMind used 5000 first-generation TPUs for generating self-play 
games and 64 second-generation TPUs for training 
ï®
In its training run, AlphaZero processed ~21 million games, compared to 
the ~5 million games processed by AlphaGo Zero 
35 


## Page 36

Significance  
ï®AlphaZero demonstrates that the architecture is not tied to Go 
ï®It establishes a reusable template for deterministic, perfect-
information board games 
ï®Only the encodings and rules simulators are hand-designed 
ï®Everything else (strategies, evaluations, style of play) emerges from 
self-play with a single algorithm 
36 


## Page 37

The AlphaGo Family 
1. AlphaGo 
2. AlphaGo Zero 
3. AlphaZero 
4. MuZero 
37 


## Page 38

MuZero: Unknown Transition Model 
ï®Alpha* models assume that a perfect simulator exists for every move 
ï®Many real domains lack explicit, hand-coded transition functions 
ï®MuZero keeps the Alpha* structure: policy-value network, MCTS, 
and self-play, but no longer needs a known transition model 
ï®The search runs inside a learned model instead of a hand-written 
engine 
38 


## Page 39

Predicting only the Essentials for Decisions 
ï®Classic world models often reconstruct next full observations, e.g., 
images 
ï®This reconstruction forces care about many details irrelevant to 
decisions 
ï®MuZero focuses only on predicting rewards, values, and good policies 
ï®The model may be wrong about irrelevant future aspects without 
penalty 
ï®The main criterion is to preserve decision quality, not observation 
fidelity 
39 


## Page 40

Three-Component Model 
ï®The representation network â„ğœƒ: ğ‘‚âˆ—â†’ğ‘† maps an observation history 
to the initial latent state s0 
ï®The dynamics network ğ‘”ğœƒ: ğ‘†Ã— ğ´â†’ğ‘†Ã— â„ maps (sk , ak) to the next 
latent state and reward 
ï®The prediction network ğ‘“ğœƒ: ğ‘†â†’Î”(ğ´) Ã— â„ maps a latent state sk to 
policy and value 
ï®All three networks share parameters Î¸ and train jointly 
ï®Together they form an internal â€œenvironmentâ€ where planning 
occurs 
 
40 


## Page 41

Representation Network 
ï®The representation network is responsible for the construction of 
hidden states 
ï®The input is the recent observation history ğ‘œ1:ğ‘¡ from the real 
environment 
ï®The representation network computes the root latent state: 
ğ‘ 0 = â„ğœƒğ‘œ1:ğ‘¡ 
ï®s0 does not need to resemble images or boards directly 
ï®MuZero only needs information sufficient for predicting rewards, 
values, and policies 
ï®The model learns its own abstractions, like threats or configurations, 
inside this latent space 
41 


## Page 42

Dynamics Network 
ï®The dynamics network is responsible for the construction of latent 
transitions and rewards 
ï®It takes a latent state and action ğ‘ ğ‘˜, ğ‘ğ‘˜ and outputs the next latent 
state and the predicted immediate reward: 
  
ğ‘ ğ‘˜+1, ğ‘Ÿğ‘˜= ğ‘”ğœƒğ‘ ğ‘˜, ğ‘ğ‘˜ 
ï®Repeated applications unroll imagined trajectories entirely in the 
latent space 
ï®There are no calls to the real environment during the search; only 
ğ‘”ğœƒ generates the â€œfuturesâ€ 
ï®Reward prediction trains the model to represent decision-relevant 
consequences of actions 
42 


## Page 43

Prediction Network 
ï®The prediction network assesses the policies and values 
ï®For any latent state sk , the prediction network outputs 
  
ğ‘ğ‘˜, ğ‘£ğ‘˜= ğ‘“ğœƒğ‘ ğ‘˜ 
ï®
pk : policy distribution over actions for that imagined state 
ï®
vk : value estimate of long-term return from that state 
ï®It provides priors and leaf evaluations for tree search inside the 
model 
43 


## Page 44

Collecting Training Data from Real Episodes 
ï®MuZero interacts with the real environment to generate trajectories 
ï®Each episode yields observations ot , actions at , and actual rewards 
from the real environment ut 
ï®Self-play for games; standard RL interaction for Atari-style tasks 
ï®It stores full sequences ğ‘œ1, ğ‘1, ğ‘¢1, ğ‘œ2, â€¦ , ğ‘œğ‘‡ in a replay buffer 
ï®Later it picks random time indices t as roots for training unrolls 
44 


## Page 45

Learning by Unrolling the Model 
ï®For a chosen root time t, compute ğ‘ 0 = â„ğœƒğ‘œ1:ğ‘¡ 
ï®Unroll model K steps using recorded actions: ğ‘ ğ‘˜+1, ğ‘Ÿğ‘˜= ğ‘”ğœƒğ‘ ğ‘˜, ğ‘ğ‘¡+ğ‘˜ 
ï®At each step k, apply ğ‘“ğœƒğ‘ ğ‘˜ to obtain ğ‘ğ‘˜, ğ‘£ğ‘˜ 
ï®Reward rk  target: actual reward ğ‘¢ğ‘¡+ğ‘˜ from the environment 
ï®Value vk target: the truncated return 
  
ğ‘§ğ‘¡+ğ‘˜â‰ˆğ‘¢ğ‘¡+ğ‘˜+ ğ›¾âˆ™ğ‘¢ğ‘¡+ğ‘˜+1 + â‹¯+ ğ›¾ğ‘›âˆ™ğ‘¢ğ‘¡+ğ‘˜+ğ‘›+ ğ›¾ğ‘›+1ğ‘£  
45 


## Page 46

MuZero Operations 
46 


## Page 47

Conclusions 
ï®The AlphaGo family moved RL from toy worlds and classic Atari 
games into world-class decision making 
ï®AlphaGo succeeded in beating a human world champion in Go using 
human data, deep networks, and MCTS 
ï®AlphaGo Zero learned Go from scratch, without human games 
ï®AlphaZero generalized for different games 
ï®MuZero generalized further for unknown environment dynamics 
47 


## Page 48

Main References 
ï®Silver, D., et al. (2016). Mastering the Game of Go with Deep Neural 
Networks and Tree Search. Nature 
ï®Silver, D., et al. (2017). Mastering the Game of Go without Human 
Knowledge. Nature 
ï®Silver, D., et al. (2018). A General Reinforcement Learning Algorithm 
that Masters Chess, Shogi, and Go through Self-Play. Science 
ï®Schrittwieser, J., et al. (2020). Mastering Atari, Go, Chess and Shogi 
by Planning with a Learned Model. Nature 
48 
