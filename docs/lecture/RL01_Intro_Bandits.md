# RL01_Intro_Bandits

**Source:** c:\Users\catalin.butacu\Downloads\RL\resourses\lecture\RL01_Intro_Bandits.pdf

**Pages:** 89

---


## Page 1

Reinforcement Learning 
1. Introduction. Multi-Armed Bandits 
 
Florin Leon 
 
â€œGheorghe Asachiâ€ Technical University of IaÈ™i, Romania 
Faculty of Automatic Control and Computer Engineering 
 
https://florinleon.byethost24.com/lect_rl.html 
 
2025 
 


## Page 2

Introduction. Multi-Armed Bandits 
1. Introduction to Reinforcement Learning 
 
1.1. RL Among Machine Learning Paradigms 
 
1.2. Key Elements of RL 
 
1.3. Related Fields 
2. Multi-Armed Bandit Problems 
 
2.1. Sample-Average Estimation of Action Values 
 
2.2. Exponential Recency-Weighted Average 
 
2.3. Optimistic Initial Estimates 
 
2.4. Upper-Confidence-Bound Action Selection 
 
 
2 


## Page 3

Introduction. Multi-Armed Bandits 
1. Introduction to Reinforcement Learning 
 
1.1. RL Among Machine Learning Paradigms 
 
1.2. Key Elements of RL 
 
1.3. Related Fields 
2. Multi-Armed Bandit Problems 
 
2.1. Sample-Average Estimation of Action Values 
 
2.2. Exponential Recency-Weighted Average 
 
2.3. Optimistic Initial Estimates 
 
2.4. Upper-Confidence-Bound Action Selection 
 
 
3 


## Page 4

Introduction 
ï®Reinforcement learning (RL) focuses on learning through interaction 
with the environment, with no explicit teacher 
ï®The agent learns by trial and error 
ï®Natural learning examples: riding a bicycle, language acquisition, 
driving. Here, there is some kind of supervision, but one cannot 
succeed without direct practice 
4 


## Page 5

Machine Learning Paradigms 
ï®Machine learning (ML) has three main paradigms:  
ï®Supervised learning 
ï®Unsupervised learning 
ï®Reinforcement learning 
ï®Each paradigm has different learning methods, based on 
available information 
ï®RL is a sequential decision-making approach 
5 


## Page 6

Machine Learning Paradigms 
ï®Supervised learning: learn a function from labeled data 
ï®Unsupervised learning: find patterns without labels 
ï®Reinforcement learning: learn by interacting with the 
environment to maximize rewards 
6 


## Page 7

Learned Functions 
ï®Some functions are known exactly, e.g., Newtonâ€™s second law of 
motion: F = m a 
ï®But they were previously induced and confirmed experimentally 
ï®Many functions need to be learned from data, e.g., predicting 
shampoo purchases from age 
ï®Given age, predict 0 or 1 
ï®They may have no known analytical expressions 
ï®E.g., complex polymerization reactions 
ï®In such cases, ML methods are used to approximate the functions 
7 


## Page 8

Supervised Learning 
ï®Classification (discrete output) 
 
 
 
 
ï®Regression (numerical output) 
 
 
 
 
8 


## Page 9

Supervised Learning 
ï®In supervised learning, the algorithm learns from individual 
examples given as labeled data: pairs of inputs x and outputs  
y = f(x) 
ï®The algorithm learns an approximation ğ‘“  of the (possibly very 
complex) function  
ï®There are many types of algorithms for supervised learning, e.g., 
decision trees, probabilistic, instance-based, neural networks (NNs) 
ï®When using NNs, common loss functions include cross-entropy (for 
classification) and mean squared error (for regression) 
9 


## Page 10

Unsupervised Learning 
ï®Unsupervised learning works with data that has no labels 
ï®The goal is to discover patterns in the data, such as clusters or 
subgroups 
ï®Techniques include k-means, Expectation-Maximization (EM), DBSCAN 
10 


## Page 11

Unsupervised Learning Directions 
ï®Clustering 
ï®Grouping similar data points, e.g., customer segmentation 
ï®Dimensionality reduction 
ï®Reducing feature space for easier analysis, e.g., PCA or t-SNE for 
visualization 
ï®Representation learning 
ï®Learning efficient, lower-dimensional representations of data that 
retain essential information., e.g., autoencoders 
11 


## Page 12

Clustering Applications 
ï®
Customer segmentation 
ï®
Used in marketing and sales to group customers based on purchasing behavior, 
demographics, or engagement 
ï®
Helps with targeted advertising, product recommendations, and personalized 
experiences 
ï®
Document clustering 
ï®
Organizes large volumes of unstructured text, e.g., news articles, research papers, legal 
documents 
ï®
Used in search engines, digital libraries, and topic discovery 
ï®
Social network analysis 
ï®
Clusters people or nodes based on connection patterns, e.g., communities, influence 
groups 
ï®
Applied in marketing, misinformation detection, and online behavior analysis 
ï®
Urban planning and geospatial analysis 
ï®
Clusters locations based on factors like traffic patterns, population density, or land use  
ï®
Used for zoning, resource allocation, and emergency planning 
 
12 


## Page 13

Reinforcement Learning 
ï®RL agents learn by interacting with the environment, rather than 
from a static dataset 
ï®The goal is to find an optimal policy that maps states to actions to 
maximize long-term cumulative rewards 
ï®In supervised and unsupervised learning, the full dataset is usually 
given 
ï®But continual, online learning methods, exist as well 
ï®RL learns step by step, as the agent receives feedback (rewards) from 
the environment 
13 


## Page 14

Reinforcement Learning 
ï®The agent adjusts its behavior (actions) to maximize total rewards 
over time 
ï®Actions affect both immediate and future rewards 
ï®Delayed consequences play an essential role in determining optimal 
actions 
ï®Environments are often complex, non-deterministic, and dynamic 
14 


## Page 15

RL Applications 
ï®Operating adaptive controllers in refineries 
ï®Optimizing energy use in power grids 
ï®Autonomous driving 
ï®Suggesting a medical treatment plan 
ï®Making hold-buy-sell decisions in trading 
15 


## Page 16

Games 
ï®
Games have long been used to study 
intelligent decision making in simple, 
controlled environments 
ï®
Board games: Backgammon, Go 
ï®
Video games: Atari suite, Pac-Man, 
StarCraft (multi-agent) 
ï®
RL strategies improve through repeated 
gameplay 


## Page 17

Robotics 
ï®Robots can be pre-programmed for specific tasks, but are 
limited in adaptability 
ï®Developers may struggle to describe operational knowledge, 
e.g., how â€œmusclesâ€ move when picking up an object 
ï®RL allows robots to learn from experience and adapt to new or 
changing environments 
ï®Tasks like navigation, manipulation, and locomotion can be 
learned through RL, not pre-programmed 
17 


## Page 18

Examples 
ï®Robot flipping pancakes 
ï®Autonomous model helicopter  
 
18 


## Page 19

Reinforcement Learning vs.  
Supervised/Unsupervised Learning 
ï®In supervised learning, the correct output value is provided 
ï®In unsupervised learning, there is no given value 
ï®In RL, the agent must explore to discover optimal actions based 
on rewards (~â€œgoodâ€ or â€œbadâ€) 
ï®An intermediate case in terms of available information 
ï®RL requires making decisions without knowing the exact 
outcomes, whereas supervised learning assumes fixed data 
ï®Rewards may be sparse, e.g., a game was won or lost 
19 


## Page 20

Exploration and Exploitation 
ï®In RL, an agent must decide between exploring new actions and 
exploiting known actions with high returns (total rewards) 
ï®Exploration involves trying new actions to gather more information 
about the environment 
ï®Exploitation focuses on taking actions that have previously resulted 
in high returns 
ï®The challenge is to balance these two strategies to avoid either over-
exploring or sticking too much to known actions 
ï®Too much exploration: low returns  
ï®Too much exploitation: suboptimal results 
20 


## Page 21

The Exploration-Exploitation Dilemma: 
Examples 
ï®Do you go to the restaurant youâ€™ve known and liked for a long time, 
or try the newly opened one?  
ï®Does it make a difference if youâ€™re in your hometown or in another city 
where youâ€™re staying for only two nights? 
ï®Do you go with your best friend or with someone youâ€™d like to get to 
know better? 
ï®Do you order a familiar dish or try something new? 
 
ï®Companies invest money in research and development to invent new 
products (e.g., medications), but they also want to profit from 
existing production lines that are already successful. How much 
money should they invest in research? 
21 


## Page 22

Introduction. Multi-Armed Bandits 
1. Introduction to Reinforcement Learning 
 
1.1. RL Among Machine Learning Paradigms 
 
1.2. Key Elements of RL 
 
1.3. Related Fields 
2. Multi-Armed Bandit Problems 
 
2.1. Sample-Average Estimation of Action Values 
 
2.2. Exponential Recency-Weighted Average 
 
2.3. Optimistic Initial Estimates 
 
2.4. Upper-Confidence-Bound Action Selection 
 
 
22 


## Page 23

Rewards 
ï®A reward is the immediate feedback that defines what is 
desirable or undesirable in the environment 
ï®Rewards are the indicators of success or failure in achieving the 
goal 
ï®The agentâ€™s objective is to maximize the total reward over time, 
not just immediate gains 
ï®Rewards can be stochastic, influenced by both the agentâ€™s 
actions and the environmentâ€™s state 
ï®The sum of rewards over time is called the return 
23 


## Page 24

Value Function 
ï®The value function estimates the expected return that can be 
obtained starting from a given state 
ï®It helps agents evaluate which states are worth pursuing based 
on their future rewards 
ï®Agents make decisions based on long-term value rather than 
immediate reward 
ï®Estimating values is difficult because future states and rewards 
are uncertain 
24 


## Page 25

Policy 
ï®The policy determines how the agent behaves by mapping 
states to actions 
ï®A policy can be deterministic or stochastic, where action 
probabilities are assigned to states 
ï®Deterministic: ğœ‹: ğ‘†â†’ğ´, ğœ‹ğ‘ = ğ‘ (a is the action taken in state s) 
ï®Stochastic: ğœ‹: ğ‘†Ã— ğ´â†’0, 1 , ğœ‹ğ‘|ğ‘  is the probability of taking a in s  
ï®The agent updates the policy over time, seeking actions that 
yield the highest cumulative rewards 
 
ï®This is the goal of an RL problem â€“ the policy encapsulates the 
agentâ€™s learned behavior 
 
25 


## Page 26

Environment Model 
ï®The model represents the transition probability of the 
environment between successive states, given an action 
ï®Model-free methods rely solely on trial and error 
ï®Some RL systems use a model to predict the environmentâ€™s 
response to actions 
ï®Model-based methods incorporate planning into learning by 
simulating potential future states before taking actions 
26 


## Page 27

Introduction. Multi-Armed Bandits 
1. Introduction to Reinforcement Learning 
 
1.1. RL Among Machine Learning Paradigms 
 
1.2. Key Elements of RL 
 
1.3. Related Fields 
2. Multi-Armed Bandit Problems 
 
2.1. Sample-Average Estimation of Action Values 
 
2.2. Exponential Recency-Weighted Average 
 
2.3. Optimistic Initial Estimates 
 
2.4. Upper-Confidence-Bound Action Selection 
 
 
27 


## Page 28

Psychology 
ï®The term reinforcement learning was first used in the 
psychology literature, not in computer science 
ï®E. Thorndike (1898) proposed the Law of Effect: actions followed by 
rewards become more likely 
ï®B. F. Skinner (1930sâ€“1950s) introduced operant conditioning: 
behavior changes through rewards and punishments 
ï®C. Hull (1943) built a mathematical theory of learning: reinforcement 
is a quantifiable variable that increases the likelihood of a behavior 
in response to a stimulus 
28 


## Page 29

Conditioning: Pavlovâ€™s Dog 
ï®
(1) a dog salivates when seeing food, (2) but initially not when hearing a bell,  
(3) when the sound rings often enough together when food is served, the dog starts 
to associate the bell with food, and (4) also salivates when only the bell rings  
29 


## Page 30

Conditioning ïŠ 
30 


## Page 31

Neuroscience 
ï®The brain learns from surprises 
ï®Dopamine signals go up when things are better than expected, and 
down when worse, like reward prediction errors in RL 
ï®The brain ignores cues that add no new information 
ï®Blocking: no learning happens if one signal already predicts reward, 
like in temporal-difference learning 
ï®The brain also learns from predictors of predictors 
ï®Higher-order conditioning: a cue gains meaning by predicting 
another cue, like bootstrapped learning 
31 


## Page 32

Neuroscience 
ï®The brain updates expectations gradually over time 
ï®Dopamine changes track small differences between expected and 
actual outcomes, like temporal-difference learning adjusts 
predictions step by step in RL 
ï®The brain separates evaluation from decision-making 
ï®One system estimates how good things are (critic), another decides 
what to do (actor), like actor-critic methods in RL 
ï®The brain keeps short-term memory of recent actions 
ï®When a reward arrives, it strengthens recent brain activity, like 
eligibility traces reinforce helpful past steps in RL 
32 


## Page 33

Mathematics  
ï®Probabilities are an integral part of the formalization of 
reinforcement learning problems 
ï®Markov Decision Processes (MDPs) define the framework for 
modeling RL environments 
ï®Expected values are used to evaluate and compare actions or policies 
under uncertainty 
ï®Continuous optimization methods, such as gradient descent, 
are essential in deep reinforcement learning for training neural 
networks 
33 


## Page 34

Optimal Control 
ï®Both RL and optimal control focus on controlling dynamical 
systems 
ï®Key concepts, such as Bellmanâ€™s equations, were proposed in 
the context of optimal control theory within the field of 
automatic control 
ï®RL and optimal control have different terminologies but solve 
similar sequential decision problems 
34 


## Page 35

Introduction. Multi-Armed Bandits 
1. Introduction to Reinforcement Learning 
 
1.1. RL Among Machine Learning Paradigms 
 
1.2. Key Elements of RL 
 
1.3. Related Fields 
2. Multi-Armed Bandit Problems 
 
2.1. Sample-Average Estimation of Action Values 
 
2.2. Exponential Recency-Weighted Average 
 
2.3. Optimistic Initial Estimates 
 
2.4. Upper-Confidence-Bound Action Selection 
 
 
35 


## Page 36

Introduction to Multi-armed Bandits 
ï®Reinforcement learning uses evaluative feedback, not 
instructive feedback 
ï®Instructive feedback (used in supervised learning) tells the agent the 
correct response for a given situation 
ï®Evaluative feedback: evaluates actions based on their outcome, but 
does not specify the correct action 
ï®Evaluative feedback tells the agent how good an action is, not what 
the best action is 
ï®This leads to the need for active exploration 
ï®The agent must try different actions and learn from the outcomes 
 
 
36 


## Page 37

Bandits â€“ Slot Machines 
ï®
The term one-armed bandit originates 
from the design and reputation of early 
slot machines 
ï®
Early slot machines featured a single 
lever on one side that players pulled to 
start the game 
ï®
This lever looked much like a human 
arm 
ï®
The machines earned a reputation for 
keeping playersâ€™ coins, which led people 
to compare them to a bandit 
ï®
The combination of a single arm and the 
notion of stealing money resulted in the 
nickname one-armed bandit 


## Page 38

The k-Armed Bandit Problem 
ï®The k-armed bandit problem models decision making with 
multiple options, i.e., k actions 
ï®Actions are analogous to playing levers on a slot machine  
(a bandit) 
ï®Each action has an expected reward 
ï®The goal is to maximize total reward over time 
ï®The multi-armed bandit problem is a simplified RL scenario 
that helps study exploration vs. exploitation 
38 


## Page 39

Expected Value 
ï®The expected value of a random variable X is the average value we 
would get if we sampled X many times 
ï®If X is a discrete random variable with probability mass function 
ğ‘ƒ(ğ‘‹ = ğ‘¥ğ‘–): 
 
 
ï®If X is a continuous random variable with probability density 
function p(x), an integral is used, but in general, we will use the 
discrete form 
ï®The expected value can be estimated from N samples x1, â€¦, xN : 
 
 
ï®This sample average approximates the true expected value as N 
becomes large 
[
]
(
)
i
i
i
X
x
P X
x
ï€½
ïƒ—
ï€½
ïƒ¥
1
1
[
]
N
i
i
X
x
N
ï€½
ï€½
ïƒ¥
39 


## Page 40

Example: Expected Value 
ï®In a bag with a large number of marbles, 30% are red and 70% 
are blue 
ï®If you draw a red marble, you gain 1 point; if you draw a blue 
marble, you gain 4 points 
ï®What is your expected gain? 
ï®ğ‘¥1 = 1 with probability ğ‘ƒ(ğ‘‹ =  1) =  0.3 
ï®ğ‘¥2 = 4 with probability ğ‘ƒ(ğ‘‹ =  4) =  0.7 
ï®Expected value: 
 
ï®Would you pay 300 points to play this game 100 times? 
 
[
]
1 0.3
4 0.7
0.3
2.8
3.1
X ï€½ïƒ—
ï€«ïƒ—
ï€½
ï€«
ï€½
40 


## Page 41

Example: Sample Average 
ï®Suppose we draw 10 samples: 4, 4, 1, 4, 4, 1, 4, 4, 4, 1 
ï®Sample average: 
 
 
 
ï®The sample-based estimate of the expected value is 3.5, which 
is close to the true value 3.1, but not exact due to the limited 
number of samples 
1
35
[
]
(4
4
1
4
4
1
4
4
4
1)
3.5
10
10
X ï€½
ï€«
ï€«ï€«
ï€«
ï€«ï€«
ï€«
ï€«
ï€«
ï€½
ï€½
41 


## Page 42

Defining the k-Armed Bandit Problem 
ï®The agent repeatedly chooses among k different actions 
ï®Each action provides a numerical reward, drawn from a 
stationary probability distribution 
ï®A probability distribution that remains unchanged over time 
ï®The goal is to maximize the total expected reward over a given 
time period (e.g., 1000 action selections) 
ï®Real-world examples:  
ï®A doctor selecting treatments for patients 
ï®A company deciding between advertising campaigns 
 
42 


## Page 43

Example: Clinical Trials 
ï®
A doctor has k treatments to compare 
ï®
Each treatment i has an unknown probability of success piâ€‹ 
ï®
T patients are enrolled sequentially 
ï®
After treating each patient, the doctor observes an outcome (success or 
failure) 
ï®
The objective is to maximize the total number of successes by sequentially 
choosing the best treatment 
ï®
At each step, the doctor chooses a treatment i 
ï®
Exploitation: he assigns patients to the treatment that appears most 
effective 
ï®
Exploration: he assigns patients to lesser-known treatments to learn more 
43 


## Page 44

Exploration vs. Exploitation 
ï®2 actions / treatments / choices 
ï®The (unknown) distributions, i.e., the success probabilities in 
general: p1 = 0.6, p2 = 0.8 
ï®Trials: action (1 or 2) â€“ outcome (1 success or 0 failure) 
ï®1 â€“ 0; 2 â€“ 0; 1 â€“ 1; 2 â€“ 0; 1 â€“ 1; 2 â€“ 1 
ï®The estimated probabilities so far: ğ‘ 1 = 2/3, ğ‘ 2 = 1/3 
ï®Exploitation: keep selecting action 1 
ï®Exploration: try action 2 
ï®Without exploration, the optimal action (2) cannot be found 
44 


## Page 45

k-Armed Bandits: Expected Reward and 
Action Values 
ï®Each action a (out of k possible) has an expected reward (true 
action value): ğ‘âˆ—(ğ‘) = ğ”¼[ğ‘…ğ‘¡âˆ£ğ´ğ‘¡= ğ‘] 
ï®If ğ‘âˆ—(ğ‘) were known, the optimal strategy would be to always 
select ğ‘âˆ—= argmax
ğ‘
ğ‘âˆ—(ğ‘) 
ï®argmax returns the action a that gives the highest estimated value ğ‘âˆ—(ğ‘) 
ï®However, ğ‘âˆ—(ğ‘) is unknown, so it is estimated as ğ‘„ğ‘¡(ğ‘)  
ï®Ideally, ğ‘„ğ‘¡(ğ‘) should be as close as possible to ğ‘âˆ—(ğ‘) 
ï®The agent must also balance exploration (learning about 
actions) and exploitation (using the best-known action) 
45 


## Page 46

The Exploration-Exploitation Dilemma 
ï®Exploitation: choosing the action with the highest estimated 
value ğ‘„ğ‘¡(ğ‘) 
ï®Maximizes short-term gain (greedy) 
ï®May get stuck in suboptimal actions 
ï®Exploration: trying less-known actions to improve knowledge 
ï®Helps find better options in the long run 
ï®May lead to temporary lower rewards 
46 


## Page 47

Action Selection Strategies 
ï®Greedy strategy 
ï®Always select ğ´ğ‘¡= argmax
ğ‘
ğ‘„ğ‘¡(ğ‘) 
ï®Maximizes immediate reward, but will never discover better actions 
ï®Ñ”-greedy strategy 
ï®With probability 1 âˆ’ Ñ”, select argmax
ğ‘
ğ‘„ğ‘¡(ğ‘) 
ï®With probability Ñ”, pick a random action 
ï®Ensures ongoing exploration while still favoring high-value 
actions 
47 


## Page 48

Sample-Average Estimation of Action Values 
ï®The sample average method updates action value estimates as: 
 
 
 
ï®The indicator function ğŸ™ returns 1 if a condition is true, otherwise it returns 0 
ï®If an action is selected many times, Qt(a) converges to qâˆ—(a), according 
to the law of large numbers 
ï®This method is unbiased (produces estimates that are correct on 
average), but slow to adapt in changing environments 
ï®It works best when the reward distribution is stationary 
48 


## Page 49

Incremental Implementation 
49 
the error in the estimate 
Here, n is the 
number of times 
the action has been 
selected up to the 
current time step 


## Page 50

Incremental Implementation 
ï®Advantages 
ï®Requires less memory: constant memory, does not store all rewards 
ï®Computationally efficient: constant-time updates per step 
ï®Disadvantages 
ï®Adapts slowly in nonstationary environments 
ï®Better alternatives exist for dynamic problems, e.g., weighted 
updates (next subsection) 
 
50 


## Page 51

Pseudocode 
51 


## Page 52

Example: A 10-Armed Bandit 
The true value q*(a) of each of the 10 actions was selected according to a normal distribution 
with mean 0 and variance 1, and then the actual rewards were selected according to a normal 
distribution with mean q*(a) and variance 1 


## Page 53

Experiment 1 
ï®1 Simple Bandit.py 
 
ï®The 10-armed testbed 
ï®2000 randomly generated k-armed bandit problems, with k = 10 
actions 
ï®Each action value is chosen randomly from a normal distribution 
ï®The agentâ€™s performance is measured over 10 000 time steps 
53 


## Page 54

Results: Average Reward 
54 


## Page 55

Results: % Optimal Action 
55 


## Page 56

Greedy vs. Ñ”-Greedy 
ï®The greedy method gets stuck and reaches a lower average reward  
of ~1 
ï®It selects the optimal action in only ~1/3 of the cases 
ï®Ñ”-greedy with Ñ” = 0.1 explores more and finds the best action faster 
ï®Ñ”-greedy with Ñ” = 0.01 explores slower, but achieves better long-term 
performance than Ñ” = 0.1 
56 


## Page 57

Greedy vs. Ñ”-Greedy 
ï®If reward variance is 0, greedy can perform well by identifying the 
best action in one trial and exploiting it from then on 
ï®When reward variance is high, Ñ”-greedy performs better than greedy 
since more exploration is needed to estimate action values 
ï®The assumption of stationarity often breaks in practice; action values 
can change due to environment modifications or changes in behavior 
caused by learning 
ï®In nonstationary tasks, continual exploration is necessary to detect 
when previously suboptimal actions become better than the current 
greedy choice 
57 


## Page 58

Asymptotic Guarantees of Ñ”-Greedy 
ï®For a (theoretically) infinite number of steps, the Ñ”-greedy 
method guarantees that: 
ï®Every action will be sampled an infinite number of times  
 â‡’ ğ‘„ğ‘¡(ğ‘) â†’ğ‘âˆ—(ğ‘) for all actions 
ï®The probability of selecting the optimal action exceeds 1 â€“ Ñ” 
(approaches certaintly) 
58 


## Page 59

Introduction. Multi-Armed Bandits 
1. Introduction to Reinforcement Learning 
 
1.1. RL Among Machine Learning Paradigms 
 
1.2. Key Elements of RL 
 
1.3. Related Fields 
2. Multi-Armed Bandit Problems 
 
2.1. Sample-Average Estimation of Action Values 
 
2.2. Exponential Recency-Weighted Average 
 
2.3. Optimistic Initial Estimates 
 
2.4. Upper-Confidence-Bound Action Selection 
 
 
59 


## Page 60

Nonstationary Problems 
ï®Incremental updates avoid the need to store all past rewards 
 
 
ï®For nonstationary environments (where reward distributions change 
over time), this update rule is changed with a constant step-size 
parameter Î± âˆŠ (0, 1]  
 
 
ï®This is called exponential recency-weighted average, because over 
time, the weight of past rewards decays exponentially 
ï®This makes recent rewards more influential, which is essential in 
nonstationary settings 
 
ğ‘„ğ‘›+1  = ğ‘„ğ‘› + ğ›¼ğ‘…ğ‘›âˆ’ğ‘„ğ‘› 
60 


## Page 61

Exponential Recency-Weighted Average 
61 
ï®
(1 âˆ’ğ›¼)ğ‘›âˆ’ğ‘– is the weight given to Ri and it decreases with the number of 
intervening  time steps; older rewards count less 
ï®
Example: assume Î± = 0.1 and n = 100 steps 
ï®
The weight of step 99 is: 0.1 âˆ™1 âˆ’0.1 100âˆ’99 = 0.1 âˆ™0.91 â‰ˆ10âˆ’1 
ï®
The weight of step 20 is: 0.1 âˆ™1 âˆ’0.1 100âˆ’20 = 0.1 âˆ™0.980 â‰ˆ2 âˆ™10âˆ’5 
 


## Page 62

Exponential Recency-Weighted Average 
62 
ï®Larger Î± values make updates more responsive to recent changes 
ï®Smaller Î± values make estimates more stable but slower to adapt 
ï®Adaptive strategies can dynamically adjust Î± over time 


## Page 63

Sample Average vs.  
Exponential Recency-Weighted Average  
ï®Sample average 
ï®Weighs all past rewards equally 
ï®Is slow to adapt when action values change 
ï®Works well for stationary environments 
ï®Exponential recency-weighted average  
ï®Prioritizes recent data over older observations 
ï®Adapts quickly to changing rewards 
ï®Is ideal for nonstationary environments 
 
ï®ERWA with ğ›¼ğ‘›ğ‘= 1
ğ‘›  is equivalent to SA 
63 


## Page 64

Convergence 
ï®Convergence is guaranteed if: 
 
 
ï®The first condition guarantees that the steps are large enough 
to eventually overcome any initial bias or random fluctuations 
ï®The second condition guarantees that eventually the steps 
become small enough to assure convergence  
ï®Suitable functions are: 
ğ›¼ğ‘›ğ‘= 1
ğ‘›  
ğ›¼ğ‘›ğ‘=
1
ğ‘›+1   âˆˆ(0, 1]  
64 


## Page 65

Convergence 
ï®For constant step size, i.e., ğ›¼ğ‘›ğ‘= ğ›¼, convergence is not 
guaranteed 
ï®In practice: 
ï®Adaptive step size sequences may converge very slowly or require 
fine-tuning 
ï®Constant step size methods often perform well enough, even in 
nonstationary environments 
ï®The trade-off between exploration and exploitation remains 
important 
65 


## Page 66

Experiment: Comparing Methods 
ï®An experiment with the 10-armed testbed where the true 
values of the actions change over time 
ï®Uses sample averages vs. a constant step-size method with  
Î± = 0.1 and Ñ” = 0.1 
ï®The experiment is run for 10 000 steps and performance is 
compared  
66 


## Page 67

Experiment 2 
ï®2 Simple Bandit Nonstationary.py 
 
ï®q*(a) start out equal and then take independent random walks 
by adding a normally distributed increment with mean 0 
and standard deviation 0.01 to all the q*(a) on each step  
 
67 


## Page 68

Results: Average Reward 
68 


## Page 69

Results: % Optimal Action 
69 


## Page 70

Introduction. Multi-Armed Bandits 
1. Introduction to Reinforcement Learning 
 
1.1. RL Among Machine Learning Paradigms 
 
1.2. Key Elements of RL 
 
1.3. Related Fields 
2. Multi-Armed Bandit Problems 
 
2.1. Sample-Average Estimation of Action Values 
 
2.2. Exponential Recency-Weighted Average 
 
2.3. Optimistic Initial Estimates 
 
2.4. Upper-Confidence-Bound Action Selection 
 
 
70 


## Page 71

Initial Bias: Example 
ï®Initial action value estimates can influence exploration 
ï®Action A: true value = 0.5 
ï®Action B: true value = 0.8 (best action) 
ï®Suppose the rewards are noisy and vary around the true value. For 
simplicity, assume small noise so that rewards are close to the true 
value 
ï®The agent uses a greedy strategy and updates with sample averages 
71 


## Page 72

Neutral Initial Estimates 
ï®Initial estimates: 
ï®
Q(A) = 0 
ï®
Q(B) = 0 
ï®Assume the agent picks Action A first and gets a reward around 0.5 
ï®Updates: 
ï®
Q(A) = 0.5 
ï®
Q(B) = 0 
ï®Now the greedy policy chooses Action A again. More rewards come in 
around 0.5. Q(A) stays near 0.5. B is never tried 
ï®The agent may never discover that Action B is better. It is stuck 
exploiting a suboptimal action 
 
72 


## Page 73

Optimistic Initial Estimates 
ï®Initial estimates: Q(A) = 5, Q(B) = 5 
ï®Suppose the agent picks Action A first and gets a reward around 0.5 
ï®Updates: Q(A) = ~0.5, Q(B) = 5 
ï®
Q(A) = 5 is only an initial estimate, not a pseudo-sample. The value 5 is not 
considered in the average of real samples 
ï®Now B looks better. The agent chooses Action B next and gets a 
reward around 0.8 
ï®Updates: Q(A) = ~0.5, Q(B) = ~0.8 
ï®Now the greedy strategy favors Action B, which is the optimal action 
73 


## Page 74

Encouraging Exploration with Optimistic 
Values 
ï®The optimistic initial values method sets initial estimates higher to 
encourage exploration: Q1(a) = Qhigh  
ï®The agent begins by selecting actions with high initial estimates 
ï®When an action is chosen, the received reward is lower than expected  
ï®This forces the agent to try all actions, since they all appear 
promising initially  
ï®Unlike Ñ”-greedy, exploration is systematic rather than random 
74 


## Page 75

Optimistic Initialization vs. Ñ”-Greedy 
ï®Optimistic initialization explores early, but stops exploring once values 
stabilize 
ï®Ñ”-greedy continues to explore throughout the learning process 
ï®Optimistic initialization 
ï®Initially performs worse due to exploration, but eventually performs better as 
exploration decreases 
ï®Works well if the best action does not change 
ï®Converges faster in stationary settings 
ï®Works well on stationary problems, but may fail with nonstationary tasks 
ï®Ñ”-greedy 
ï®Adapts to nonstationary environments 
ï®Requires tuning Ñ” to balance exploration and exploitation 
75 


## Page 76

Experiment 3 
ï®3 Simple Bandit Optimistic Start.py 
 
ï®Optimistic initial values (OIV): Q1(a) = +5 ï€¢a, greedy 
ï®Comparison with Ñ”-greedy with Ñ” = 0.1 
76 


## Page 77

Results: Average Reward 
77 
large oscillations 


## Page 78

Results: % Optimal Action 
78 
large oscillations 


## Page 79

Results 
ï®Initially, OIV performs worse due to greater exploration, but over 
time it converges to higher reward levels as exploration drives better 
estimates 
ï®OIV does a lot of forced exploration early, due to optimism 
ï®The effect of randomness in early rewards is magnified, because only 
one sample is used at first 
ï®As a result, the early performance is unstable, even when averaged 
over many runs 
79 


## Page 80

Introduction. Multi-Armed Bandits 
1. Introduction to Reinforcement Learning 
 
1.1. RL Among Machine Learning Paradigms 
 
1.2. Key Elements of RL 
 
1.3. Related Fields 
2. Multi-Armed Bandit Problems 
 
2.1. Sample-Average Estimation of Action Values 
 
2.2. Exponential Recency-Weighted Average 
 
2.3. Optimistic Initial Estimates 
 
2.4. Upper-Confidence-Bound Action Selection 
 
 
80 


## Page 81

Upper Confidence Bound (UCB) Action 
Selection 
ï®
Instead of random exploration, actions could be selected according to their 
potential for being optimal 
ï®
How close their estimates are to being maximal  
ï®
The uncertainties in those estimates 
ï®
Upper Confidence Bound (UCB) selection rule: 
 
 
 
ï®
Qt(a) is the exploitation part 
ï®
ln ğ‘¡
ğ‘ğ‘¡(ğ‘) is the exploration part (bonus for less-visited actions) 
ï®
c controls the degree of exploration; usually, c = 2 
ï®
If Nt(a) = 0, then a is considered to be a maximizing action  
 
81 
ln
argmax
( )
( )
t
t
a
t
t
A
Q a
c
N a
ïƒ¦
ïƒ¶
ï€½
ï€«
ïƒ§
ïƒ·
ïƒ§
ïƒ·
ïƒ¨
ïƒ¸
Nt(a) is the number 
of times that action a 
has been selected 
before time step t 


## Page 82

UCB Action Selection 
ï®The square root term in UCB reflects the uncertainty in the estimated 
value of action a 
ï®The UCB formula sets an upper bound on the possible true value of 
action a 
ï®Selecting action a reduces uncertainty by increasing Nt(a) 
ï®Not selecting a increases t but not Nt(a), which raises the uncertainty 
term 
ï®The logarithmic term grows slowly, and this ensures eventual 
selection of all actions 
ï®Actions with low values or high selection counts are chosen less 
frequently over time 
82 


## Page 83

UCB vs. Ñ”-Greedy 
ï®UCB prioritizes actions with high uncertainty, unlike Ñ”-greedy 
which selects randomly 
ï®Exploration reduces naturally over time, which ensures 
efficient long-term learning 
ï®Advantages:  
ï®No need to tune Ñ” 
ï®Better theoretical guarantees 
ï®Disadvantages:  
ï®Assumes a stationary environment 
ï®More complex to implement than Ñ”-greedy 
 
83 


## Page 84

Experiment 4 
ï®4 Simple Bandit UCB.py 
84 


## Page 85

Results: Average Reward 
85 
Similar results  
for Ñ” = 0.01 


## Page 86

Results: % Optimal Action 
86 
Similar results  
for Ñ” = 0.01 


## Page 87

Results 
ï®UCB generally achieves higher rewards over time 
ï®Early exploration is more structured, and leads to faster 
convergence 
ï®UCB often outperforms Ñ”-greedy except in nonstationary cases 
ï®If the optimal action changes over time, UCB may get stuck due to 
insufficient exploration 
ï®Hybrid approaches like UCB with weighted averages may perform 
better in such cases (Qt , t and Nt are weighted with discount factors) 
87 


## Page 88

Conclusions 
ï®Reinforcement learning is about agents learning to make 
decisions through interaction and delayed reward 
ï®The agent-environment framework defines how actions, states, 
and rewards influence learning over time 
ï®Bandit problems are a simplified form of RL, focused only on 
action selection and reward estimation, without states 
ï®Exploration strategies like Ñ”-greedy, optimistic values, and UCB 
are important to avoid getting stuck on suboptimal actions 
88 


## Page 89

Main References 
ï®Sutton, R.S. and Barto, A.G. (2018). Reinforcement Learning: An 
Introduction. 2nd edition. MIT Press, Cambridge, MA. 
http://incompleteideas.net/book/the-book-2nd.html 
ï®Plaat, A. (2022). Deep Reinforcement Learning, Springer. 
https://arxiv.org/pdf/2201.02135 
 
89 
