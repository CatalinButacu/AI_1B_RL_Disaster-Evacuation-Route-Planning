# RL05_ModelBased

**Source:** c:\Users\catalin.butacu\Downloads\RL\resourses\lecture\RL05_ModelBased.pdf

**Pages:** 69

---


## Page 1

Reinforcement Learning 
5. Planning and Model-Based Methods 
 
Florin Leon 
 
â€œGheorghe Asachiâ€ Technical University of IaÈ™i, Romania 
Faculty of Automatic Control and Computer Engineering 
 
https://florinleon.byethost24.com/lect_rl.html 
 
2025 
 


## Page 2

Planning and Model-Based Methods 
1. Model-Based Reinforcement Learning 
2. Dyna-Q 
3. Imperfect Models  
4. Prioritized Sweeping 
5. Monte Carlo Tree Search (MCTS)  
2 


## Page 3

Planning and Model-Based Methods 
1. Model-Based Reinforcement Learning 
2. Dyna-Q 
3. Imperfect Models  
4. Prioritized Sweeping 
5. Monte Carlo Tree Search (MCTS)  
3 


## Page 4

Model-Free vs. Model-Based RL 
ï®Model-free methods such as Monte Carlo and temporal-difference 
(TD) learning use real experience without estimating transition 
probabilities 
ï®Model-based RL relies on a model that predicts next states and 
rewards given current state-action pairs 
ï®Both types compute value functions and use backed-up estimates as 
update targets for value approximation 
ï®Learning uses real experience from the environment, while planning 
uses simulated experience generated by the model 
ï®Learning and planning methods can be integrated in a unified 
architecture 
4 


## Page 5

Types of Environment Models 
ï®A model maps each (state, action) pair to predicted next state and 
reward outcomes 
ï®
A model is anything that the agent can use to predict how the environment will 
respond to its actions 
ï®Distribution models return the full set of possible outcomes with 
probabilities p(s', r âˆ£ s, a) 
ï®Sample models return a single sampled outcome drawn from the 
underlying distribution: (s', r) 
ï®Sample models are easier to construct in practice but provide less 
information per query compared to distribution models 
 
5 


## Page 6

Planning with Simulated Experience 
ï®Planning refers to computation that uses a model to improve the 
agentâ€™s policy 
 
 
ï®State-space planning methods involve computing value functions 
as a key intermediate step toward improving the policy 
ï®They compute value functions by backup operations (updates) 
applied to simulated experience  
 
6 


## Page 7

Model Learning 
reconstructing an underlying 
probability density function (PDF) 
from a set of observations 
7 


## Page 8

Example: Simple Maze 
8 
(1, 3) 


## Page 9

Model-Based Value Iteration 
9 


## Page 10

Planning and Model-Based Methods 
1. Model-Based Reinforcement Learning 
2. Dyna-Q 
3. Imperfect Models  
4. Prioritized Sweeping 
5. Monte Carlo Tree Search (MCTS)  
10 


## Page 11

Dyna Architecture 
ï®
Real experience serves two roles:  
ï®
Model learning: improving the model 
ï®
Direct RL: directly updating values and 
policy (like in MC or TD) 
ï®
Planning uses simulated transitions 
from the learned model to refine value 
estimates and policies 
ï®
Indirect RL: improving value functions 
and policies via the model 
ï®
In Dyna architecture, planning, acting, 
model learning, and direct RL run 
concurrently 
ï®
The same algorithm (e.g., Q-learning) 
is used for both real and simulated 
experiences 
 
11 


## Page 12

Direct vs. Indirect RL 
ï®Indirect methods often make fuller use of a limited amount of 
experience  
ï®They achieve a better policy with fewer environmental interactions 
ï®On the other hand, direct methods are much simpler and are not 
affected by biases in the design of the model 
12 


## Page 13

Dyna-Q Algorithm 
ï®Dyna-Q is a model-based extension of Q-learning 
ï®After taking action A in state S, observe reward R and next state S', 
then apply the one-step Q-learning update 
ï®The model stores the deterministic prediction: Model(S, A) â† (R, S') 
ï®Planning performs n iterations by sampling past (S, A) pairs, 
retrieves (R, S') from the model, and applies Q-learning 
(imagination) 
ï®Planning steps are limited to previously observed (S, A) pairs; the 
model is only queried with known inputs 
ï®If planning is skipped, the algorithm reduces to standard Q-learning 
on real experience 
13 


## Page 14

14 


## Page 15

Dyna-Q for Nondeterministic Environments 
15 
ï®
The model becomes generative: 
ï®
Input: a state-action pair (s, a)  
ï®
Output: a sampled next state and reward (s', r) drawn from the learned 
distribution 
ï®
Instead of storing one next state and one reward per pair, the model can: 
ï®
Keep a list of observed outcomes for each (s, a): [(s1', r1), (s2', r2), ...] and 
sample one uniformly when planning 
ï®
Keep empirical probabilities (counts) and sample the next state and estimated 
reward with probability proportional to the frequency 
ï®
Fit a parametric function approximator that, given (s, a), produces a distribution 
over next states and rewards 
ï®
The Dyna-Q loop remains unchanged: 
ï®
Use Q-learning on real experience (which already works fine in stochastic MDPs) 
ï®
Update the model from that experience 
ï®
Do planning updates by sampling hypothetical transitions from the model 


## Page 16

Example: Another Maze 
ï®
The maze task uses 47 states, 4 actions per state, and deterministic transitions  
ï®
The reward is 0 everywhere except +1 on transitions into the goal; episodes 
restart from a fixed start state 
ï®
Dyna-Q agents differ by the number of planning steps n 
ï®
Larger n leads to faster convergence to optimal policy; fewer episodes are 
needed to reach efficient navigation 
ï®
Even with identical real experience, agents with more planning steps generalize 
and improve policies more quickly 
16 


## Page 17

Program 
ï®1 DynaMaze.py 


## Page 18

Program 
ï®1 DynaMaze.py 


## Page 19

Learned Policies 
ï®
Policies found by planning and nonplanning Dyna-Q agents halfway through the 
second episode 
 
 
 
 
 
 
ï®
Without planning, each episode adds only one additional step to the policy, and only 
the last step is learned 
ï®
With planning, again only one step is learned during the first episode, but during the 
second episode an extensive policy is developed  
ï®
This policy is built by planning while the agent is still wandering near the start state 
ï®
By the end of the third episode a complete optimal policy is found and perfect 
performance is attained 
19 


## Page 20

Planning and Model-Based Methods 
1. Model-Based Reinforcement Learning 
2. Dyna-Q 
3. Imperfect Models  
4. Prioritized Sweeping 
5. Monte Carlo Tree Search (MCTS)  
20 


## Page 21

Imperfect Models 
ï®Models may be inaccurate due to: 
ï®
Small number of samples 
ï®
Function approximation 
ï®
Nonstationary (changing) environments 
ï®Planning with an incorrect model can produce suboptimal policies 
because of model errors 
ï®Optimistic model errors may be self-correcting 
ï®The model predicts greater rewards or state transitions than possible, the 
agent tries to exploit them, and discovers they are false 
 
21 


## Page 22

Dyna-Q+ with Exploration Bonus 
ï®Exploration in Dyna-Q comes from: 
ï®Ñ”-greedy randomness  
ï®Initial optimism in Q-values at the start 
ï®Dyna-Q does not consider that â€œthis part of the world might have 
changed since I last checkedâ€ 
ï®Dyna-Q+ adds an explicit exploration bonus in the model based on 
recency 
ï®For every state-action pair, it stores Ï„(s, a) = how many time steps 
have passed since (s, a) was last tried in the real environment 
ï®When it uses the model during planning, the reward for that 
transition is:  
22 
r
r
ï«ï´
ï€½ï€«


## Page 23

Exploration in Dyna-Q+ 
ï®Planning with this optimistic model will tend to increase Q-values for 
long-untried actions 
ï®If an action was tried recently, Ï„ is small, and the bonus is small 
ï®If an action has not been tried for a long time, Ï„ is large, and the 
bonus becomes noticeable 
ï®The model becomes optimistic about neglected transitions 
ï®Bonus-based planning promotes curiosity that can improve  
long-term model accuracy and policy quality 
ï®
But Dyna-Q+ is only a heuristic, it is not guaranteed to be better than Dyna-Q 
in general  
23 


## Page 24

Example: Blocking Maze 
ï®The environment changes after 1000 steps by blocking the shortest 
path and opening a longer route to the goal 
24 


## Page 25

Programs 
ï®2 BlockingMaze.py, 2a BlockingMaze.py 
25 


## Page 26

Algorithm Behavior 
ï®Dyna-Q agents initially follow the now-invalid short path, leading to 
a temporary performance drop 
ï®After exploration, they discover and exploit the new path, recovering 
performance over time 
ï®Dyna-Q+ includes an exploration bonus to encourage retrying long-
unvisited actions 
26 


## Page 27

Example: Shortcut Maze 
ï®A shorter path to the goal is added after 3000 steps, but the old path 
remains valid 
27 


## Page 28

Programs 
ï®2 BlockingMaze.py, 2a BlockingMaze.py 
28 


## Page 29

Final Policies 
Policy for: Dyna-Q  (16 steps) 
 
  N  S  N  E  E  E  E  S  F 
  S  E  S  N  N  N  N  E  N 
  E  E  E  E  E  E  E  N  N 
  N  X  X  X  X  X  X  X  N 
  N  W  W  W  W  W  W  W  W 
  N  N  N  N  N  N  N  N  N 
 
Policy for: Dyna-Q+  (10 steps) 
 
  W  E  E  N  N  E  E  E  F 
  E  E  S  S  E  E  E  E  N 
  E  E  E  E  E  E  E  E  N 
  N  X  X  X  X  X  X  X  N 
  W  W  N  E  E  E  E  E  N 
  N  W  N  E  E  E  E  N  N 
29 


## Page 30

Algorithm Behavior 
ï®A shorter path to the goal is added after 3000 steps, but the old path 
remains valid 
ï®Standard Dyna-Q agents fail to discover the shortcut because it is not 
explored under the current policy 
ï®Planning based on outdated models reinforces obsolete trajectories, 
which reduces the chance of model correction 
ï®Exploration is needed not just for learning values but also for 
maintaining an accurate model 
ï®Without sufficient exploration, the agent remains unaware of 
beneficial changes in the environment 
30 


## Page 31

Planning and Model-Based Methods 
1. Model-Based Reinforcement Learning 
2. Dyna-Q 
3. Imperfect Models  
4. Prioritized Sweeping 
5. Monte Carlo Tree Search (MCTS)  
31 


## Page 32

Inefficient Uniform Planning 
ï®
Standard Dyna-Q samples state-action pairs 
uniformly from past experience during planning 
ï®
Early in learning, most value updates are 
ineffective because they propagate zero or 
uninformative values 
ï®
Efficient planning should prioritize updates 
where value changes are likely or have recently 
occurred 
ï®
Uniform sampling wastes computation on 
updates that have no meaningful effect on value 
propagation 
ï®
Targeting impactful updates aims to reduce 
redundancy 
32 
At the beginning of the second 
episode, only the state-action pair 
leading into the goal has a positive 
value; the values of all other pairs 
are still zero 
 
The updates along other transitions 
have no effect: they are from one  
zero-valued state to another 


## Page 33

Prioritized Sweeping Algorithm 
ï®Prioritized Sweeping maintains a priority queue of state-action pairs 
ordered by estimated impact of their updates 
ï®After updating a high priority pair, predecessor pairs are considered 
and reprioritized based on expected change 
ï®Only pairs whose updates exceed a threshold Î¸ are added or 
reprioritized in the queue 
ï®Planning steps repeat by selecting top priority pairs and propagating 
their effects through the model 
ï®The algorithm converges more efficiently by focusing computational 
effort on value updates that matter most 
 
33 


## Page 34

34 


## Page 35

Explanation of Loop (g) 
ï®
A high-priority pair has just been popped from the queue: 
ï®
S, A â† first(PQueue) 
ï®
R, S' â† Model(S, A) 
ï®
Q(S, A) â† Q(S, A) + Î± [R + Î³ maxa Q(Sâ€², a) âˆ’ Q(S, A)] 
ï®
The model provided (R, Sâ€²) 
ï®
A Q-learning style backup/update on (S, A) is performed using the model 
ï®
This changes Q(S, A), and often affects the value of the state S, because  
maxa Q(S, a) may now take a different value 
ï®
The key idea of prioritized sweeping is: if the value of S has changed, then 
any state-action pair that leads into S might now have a significant error as 
well 
ï®
Therefore, the algorithm looks backwards through the model 
 
35 


## Page 36

Explanation of the Nested Loop 
ï®
z 
ï®
Consider all predecessors of S in the model: all state-action pairs (ğ‘† , ğ´ ) such 
that: ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™ğ‘† , ğ´ = ğ‘… , ğ‘† 
ï®
In many implementations, each state S maintains a list such as:  
predecessors(S) = ğ‘†1 , ğ´1 , ğ‘†2, ğ´2 , â€¦  
ï®
z 
ï®
For each predecessor, the error is computed using the newly updated value of S 
ï®
This is the TD error that would be produced if ğ‘„ğ‘† , ğ´  were updated using 
successor state S:  
ï®
The priority is: P = |Î´| 
ï®
a 
ï®
If the potential error is large enough, the predecessor is pushed onto the priority 
queue; the predecessor becomes a candidate for a future backup 
 
max
( , )
( , )
a
R
Q S a
Q S A
ï¤
ï§
ï€½
ï€«
ï€­
36 


## Page 37

Performance of Prioritized Sweeping 
ï®In maze navigation tasks, Prioritized Sweeping can reduce the 
number of required updates by factors of 5 to 10 over Dyna-Q 
ï®Larger environments benefit more from prioritization due to the 
number of irrelevant transitions 
 
37 


## Page 38

Program 
ï®3 PrioritizedSweeping.py 
38 


## Page 39

Example: Rod Maneuvering 
39 


## Page 40

Planning and Model-Based Methods 
1. Model-Based Reinforcement Learning 
2. Dyna-Q 
3. Imperfect Models  
4. Prioritized Sweeping 
5. Monte Carlo Tree Search (MCTS)  
40 


## Page 41

Planning at Decision Time 
ï®Planning can occur between actions (background planning) or 
exactly when selecting an action (decision-time planning) 
ï®Background planning improves the policy or value function gradually 
by exploiting simulated experience from a model 
ï®Decision-time planning simulates trajectories from the current state 
and chooses the best action from outcomes 
ï®In small state spaces, the results of decision-time planning can 
update value functions (both planning styles are combined) 
ï®Decision-time planning suits slow-response tasks like chess, while 
low-latency tasks benefit more from background planning 
41 


## Page 42

Heuristic Search 
ï®
Heuristic search is a decision-time planning method applied separately in each state 
ï®
It builds a large search tree over possible future state sequences from the current state 
ï®
Leaf nodes receive evaluations, and these values propagate backward to update the 
intermediate state-action nodes (e.g., minimax) 
ï®
The agent then selects the action whose backed-up value is maximal for the current state 
ï®
Heuristic search targets only relevant successor states and actions, so it is efficient 
42 


## Page 43

MCTS: Motivation 
ï®The agent is in a certain state sâ‚€, the environment may be complex, 
the model may be computationally expensive, and the agent has only 
limited time to decide the action 
ï®Monte Carlo Tree Search (MCTS) treats control as online planning 
and does not learn a global value function over states 
ï®Instead, it builds a local value function focused on states relevant to 
the imminent decision at sâ‚€ 
ï®The algorithm runs many simulated trajectories from sâ‚€ , and each 
one produces a return estimate for the visited state-action pairs 
ï®These sampled returns are aggregated into empirical estimates of 
Q(s, a) near the root, which then guide the final action choice 
43 


## Page 44

MCTS vs. MC 
ï®Standard MC makes rollouts from the root estimate Q(sâ‚€, a) but 
wastes computation on irrelevant parts of state space 
ï®MCTS reuses earlier simulation results and focuses future 
simulations on branches that currently appear more promising 
ï®Each simulation follows a path of state-action pairs, then propagates 
its return to update value estimates along that path 
ï®A search tree (expectimax) grows from the root by adding nodes for 
newly visited states and approximating local Q(s, a) around sâ‚€ 
 
44 


## Page 45

MCTS for Games and RL 
ï®MCTS is famous for searching for solutions in games with large 
branching factors 
ï®It was successfully used by Google DeepMind for AlphaGo, in 
combination with deep neural networks and RL methods 
ï®Silver et al. (2016). Mastering the Game of Go with Deep Neural 
Networks and Tree Search, Nature 
 
ï®But MCTS can also be used for standard planning RL problems 
45 


## Page 46

MCTS Overview 
46 


## Page 47

47 


## Page 48

1. Selection 
ï®We start at the root node, and successively select a child node until 
we reach a node that is not fully expanded 
White: agent nodes (agent takes action a or b) 
Black: environment nodes (next state is nondeterministic: t or t') 
48 


## Page 49

Types of Selection 
ï®
The children of each node correspond to the available actions in the parent 
node 
ï®
If not all child nodes are expanded (some actions have not been tried), select 
an untried action 
ï®
If all actions have been tried, use UCB1 / UCT selection 
ï®
UCB1 = Upper Confidence Bound, UCT = Upper Confidence bounds for Trees 
 
 
 
ï®
The UCB1 / UCT selection is similar to that in bandits (lecture 1), but here 
each parent node is a distinct bandit 
ï®
However, this choice is not fixed in the MCTS architecture; any selection 
method can be used, e.g., Ñ”-greedy or optimistic initial estimates for Q values 
(lecture 1) 
 
ln
( )
Ë†
argmax
( , )
( , )
a
N s
a
Q s a
c
N s a
ï‚¢
ïƒ©
ïƒ¹
ï‚¢
ï€½
ï€«
ïƒª
ïƒº
ï‚¢ï€«
ïƒª
ïƒº
ïƒ«
ïƒ»
49 


## Page 50

Exploration-Exploitation in UCB1 
ï®First term: exploitation â€“ choosing the action that currently looks 
best 
ï®Second term: exploration â€“ trying actions that are uncertain but 
might be better 
ï®If an action is less explored (N(S, a) is smaller), then this term is higher 
ï®The value of c depends on the scale and variability of rewards  
ï®When rewards âˆŠ [0, 1], c can be ~1 or 2 
ï®When rewards are very noisy, c can be higher 
50 


## Page 51

The Exploration Term 
ï®
The equation is not empirical, but derived from theoretical analysis of 
multi-armed bandits in statistical decision theory 
ï®
sqrt: the standard deviation of the sample mean shrinks as 1/ ğ‘› 
ï®
Related to the width of the confidence interval 
ï®
ln: some actions that may look bad early may not be actually bad;  
ln controls how many times we can still give a change to a seemingly bad 
action before effectively giving it up 
ï®
ln N(s) increases over time, but slowly, so the chance to re-test actions grows 
only gradually 
ï®
This relates to the balance with the exploitation term, but unlike the c constant, 
it grows to allow more chances to test 
ï®
Related to the confidence level, i.e., how sure we are that the true value lies 
within that interval 
51 


## Page 52

52 


## Page 53

2. Expansion 
ï®Unless the node we end up at is a terminal state, we expand the 
children of the selected node by choosing an action and creating new 
nodes using the action outcomes 
53 


## Page 54

54 


## Page 55

3. Simulation 
ï®We choose one of the new nodes and perform a random simulation 
(rollout) of the MDP to a terminal state 
55 


## Page 56

Algorithm (Function â€“ Simulate(s : S)) 
  
Input:  state s, discount factor Î³ 
Output:  cumulative discounted reward G 
  
G â† 0 
Î³t â† 1 
  
while s is not a terminating state do 
    Randomly select action a âˆˆ A(s)  // or use a heuristic rollout policy 
    Choose one outcome s' according to Pa(s' | s) and observe reward r 
    G â† G + Î³t Â· r 
    Î³t â† Î³ Â· Î³t 
    s â† s' 
end while 
  
// No states or actions from this simulation are added to the search tree 
  
return G 
 
56 


## Page 57

4. Backpropagation 
ï®Given the reward r at the terminating state, we backpropagate the 
reward to calculate the value V(s) at each state along the path 
 
 
 
 
 
 
 
 
 
ï®This MCTS step is not related to the backpropagation algorithm for 
training neural networks! 
57 


## Page 58

58 


## Page 59

Choosing the Actual Action 
ï®At the end of the search, we have the empirical estimates Q(s0, a)  
and the visit counts N(s0, a) at the root node  
ï®Usually, the action with maximum N(s0, a) is chosen 
ï®Q can be very noisy, especially when some actions have only a few 
simulations 
59 


## Page 60

Example: The Simple Mars Rover 
60 
initial  
state 
obstacle (â€“20) 
agent 
goal state  
(+100) 
unsafe  
termination  
(â€“100) 
Actions: 
â€¢ move: +1 cell (forward) with P = 0.9;   0 (stays in place) with P = 0.1 
â€¢ speed: +2 with P = 0.8;   +1 with P = 0.1;   0 with P = 0.1 


## Page 61

Iteration 1 
ï®Selection: The tree has only the root node s0 (state 0) 
ï®There are no children, so we apply Expansion from the root 
ï®Pick an untried action at root: move 
ï®Outcome: +1 (0 â†’ 1), R = â€“1 
ï®Create child node: s1 (state 1, child of root for action move) 
ï®Simulation from s1 (rollout): 1 â†’ 1 (stay, â€“1), 1 â†’ 3 (â€“1), 3 â†’ 5 (â€“100) 
ï®Return: G = â€“1 â€“ 1 â€“ 1 â€“ 100 = â€“103 
ï®Backpropagation 
ï®Statistics: N - visit count, S - sum of returns, V - estimated value (mean return) 
ï®Node s1: N(s1) = 1, S(s1) = â€“103, V(s1) = â€“103 
ï®Root s0: N(s0) = 1, S(s0) = â€“103, V(s0) = â€“103 
 
61 


## Page 62

Iteration 2 
ï®Selection: in s0, speed is untried. Expansion for speed 
ï®Outcome: +2 (0 â†’ 2), R = â€“20 
ï®Create child node: s2 (state 2, child of root for action speed) 
ï®Simulation from s2: 0 â†’ 2  (â€“20), 2 â†’ 4 (+100) 
ï®Return: G = â€“20 + 100 = 80 
ï®Backpropagation 
ï®N(s2) = 1, S(s2) = 80, V(s2) = 80  
ï®N(s0) = 2, S(s0) = â€“103 + 80 = â€“23, V(s0) = â€“11.5 
ï®s1 remains unchanged 
 
62 


## Page 63

Iteration 3 
ï®Selection using UCT 
ï®
Q(s, a) corresponds to the value V of the child following a 
ï®Root: N(s0) = 2, S(s0) = â€“103 + 80 = â€“23, V(s0) = â€“11.5 
ï®Child move: N(s1) = 1, S(s1) = â€“103, V(s1) = â€“103 
ï®Child speed: N(s2) = 1, S(s2) = 80, V(s2) = 80 
 
 
 
 
 
 
ï®Select speed 
 
ln
( )
Ë†
argmax
( , )
( , )
a
N s
a
Q s a
c
N s a
ï‚¢
ïƒ©
ïƒ¹
ï‚¢
ï€½
ï€«
ïƒª
ïƒº
ï‚¢ï€«
ïƒª
ïƒº
ïƒ«
ïƒ»
ğ‘ˆğ¶ğ‘‡1 = âˆ’103 +
2 ln 2
1
= âˆ’103 + 1.18 = âˆ’101.82 
ğ‘ˆğ¶ğ‘‡2 = 80 +
2 ln 2
1
= 80 + 1.18 = 81.18 
63 


## Page 64

Iteration 3 
ï®Select speed, move to s2 
ï®Pick an untried action at s2: move 
ï®Expansion for move 
ï®Outcome: +1 (2 â†’ 3), R = â€“1 
ï®Create child node: s3 
ï®Simulation from s3: 3 â†’ 5 (â€“100) 
ï®Return: G = â€“200â†’2 â€“ 12â†’3 â€“ 1003â†’5 = â€“121 
ï®Backpropagation 
ï®N(s3) = 1, S(s3) = â€“121, V(s3) = â€“121 
ï®N(s2) = 2, S(s2) = 80 â€“ 121 = â€“41, V(s2) = â€“20.5 
ï®N(s0) = 3, S(s0) = â€“23 â€“ 121 = â€“144, V(s0) = â€“48 
ï®s1 remains unchanged 
 
64 


## Page 65

Program 
ï®4 MCTS_cartpole.py 
65 


## Page 66

Example: Simple Maze 
https://gibberblot.github.io/rl-notes/gifs/mcts.gif 
66 


## Page 67

MCTS vs. VI 
67 


## Page 68

Conclusions 
ï®Model-based RL learns a predictive model for planning, which trades 
more computation and complexity for higher sample efficiency 
ï®Dyna reuses real experience by updating from both actual and 
simulated transitions, so each interaction becomes more valuable 
ï®Exploration bonuses in Dyna-Q+ push the agent toward rarely tried 
actions and can handle nonstationary environments 
ï®Prioritized sweeping and decision-time planning methods like MCTS 
focus computation on high-impact states, and this enables scalable 
planning in large problems 
68 


## Page 69

Main References 
ï®Sutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: 
An Introduction. 2nd edition. MIT Press, Cambridge, MA. 
http://incompleteideas.net/book/the-book-2nd.html 
ï®Miller, T. (2023). Monte-Carlo Tree Search (MCTS), The 
University of Queensland. https://gibberblot.github.io/rl-
notes/single-agent/mcts.html 
69 
