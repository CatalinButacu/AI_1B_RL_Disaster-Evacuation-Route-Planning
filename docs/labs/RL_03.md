# RL_03

**Source:** c:\Users\catalin.butacu\Downloads\RL\resourses\labs\RL_03.pdf

**Pages:** 7

---


## Page 1

Reinforcement Learning
Laboratory 03
Laboratory 3: Algorithms for Markov Decision Processes
Objectives
This laboratory introduces dynamic programming methods for finite Markov Decision Processes
(MDPs) with known dynamics. Specifically, by the end of this lab, you will be able to:
• formalize a finite MDP (S, A, p, r, γ) and encode its dynamics using transition and reward
matrices;
• state and use the Bellman expectation and optimality equations for vπ, v∗(and q∗) in tabular
form;
• implement value iteration and policy iteration ;
• extract and visualize optimal state-value functions and greedy policies on small MDPs;
• perform a convergence analysis;
• discuss computational trade-offs between value iteration and policy iteration in tabular set-
tings.
1
Theoretical Background
1.1
The Agent–Environment Interface
Figure 1: The agent-environment interaction in a Markov decision process (Reinforcement Learning
- R. Sutton and A. Barto)
Master’s Program – Artificial Intelligence
Page 1


## Page 2

Reinforcement Learning
Laboratory 03
In reinforcement learning, the agent interacts with an environment at a sequence of discrete
time steps t = 0, 1, 2, . . . . At each time step, the agent observes the current state St, selects an
action At, and receives a scalar reward Rt+1 from the environment along with a new state St+1.
This cycle then repeats.
The agent chooses actions according to a policy π(a | s), which specifies the probability of
selecting action a when in state s. The environment responds according to its dynamics, producing
the next state and reward. Over time, the agent’s goal is to learn to select actions that maximize
long-term reward.
This framework defines a clear separation between the agent and environment.
While this
boundary may be intuitive (e.g., the agent is a robot and the environment is the world around
it), it is not always obvious. For example, sensors and motors might be considered part of the
environment, even if physically inside the robot, and reward signals may originate within the agent
(e.g., dopamine) but are still modeled as part of the environment.
1.2
Goals and Rewards
The objective of the agent is expressed through a scalar reward signal. This idea is summarized by
the reward hypothesis:
All of what we mean by goals and purposes can be well thought of as the
maximization of the expected value of the cumulative sum of a received scalar
signal (called reward).
The reward signal is the only feedback the agent receives about its performance. It tells the
agent what outcomes are desirable, but not how to achieve them.
1.3
Returns and Episodes
To measure the agent’s performance, we define the return Gt as the total future reward:
Gt = Rt+1 + Rt+2 + Rt+3 + · · · =
∞
X
k=0
Rt+k+1.
This definition is suitable for episodic tasks, where interaction naturally ends after some time.
Examples include playing a game, navigating a maze, or completing a task with a defined end.
For continuing tasks, where interaction may go on indefinitely, we use discounted return:
Gt = Rt+1 + γRt+2 + γ2Rt+3 + · · · =
∞
X
k=0
γkRt+k+1,
where 0 ≤γ ≤1 is called the discount rate. Discounting ensures that the return remains finite and
gives more importance to immediate rewards over distant ones.
1.4
Unified Notation for Episodic and Continuing Tasks
To unify episodic and continuing tasks, we express the return as a finite or infinite sum:
Gt =
T−t−1
X
k=0
γkRt+k+1,
where T is the (possibly infinite) time step at which the episode ends. For episodic problems, T is
finite, and for continuing tasks, we assume γ < 1 so that the sum converges.
Master’s Program – Artificial Intelligence
Page 2


## Page 3

Reinforcement Learning
Laboratory 03
1.5
The Markov Property
A process is said to satisfy the Markov property if the current state captures all relevant information
from the history. Formally, a state St is Markov if:
Pr{Rt+1 = r, St+1 = s′ | S0, A0, . . . , St = s, At = a} = Pr{Rt+1 = r, St+1 = s′ | St = s, At = a}.
This means the future is conditionally independent of the past, given the present state and action.
The Markov assumption simplifies the learning problem and is crucial in defining and solving MDPs.
1.6
Markov Decision Process (MDP)
An MDP is a formal model for sequential decision-making and consists of:
(S, A, p, r, γ),
where S is the set of states, A is the set of actions, γ is the discount factor, and p(s′, r | s, a)
specifies the probability of transitioning to state s′ and receiving reward r after taking action a in
state s. We refer to p as the dynamics of the MDP.
In this lab, we assume a finite MDP where both the transition function p and expected reward
r(s, a, s′) are known in advance. This is known as the planning setting, where the agent does not
need to learn the model but uses it to compute optimal behavior.
1.7
Policies and Value Functions
A policy π defines the agent’s behavior by assigning probabilities to actions in each state. The
value function vπ(s) measures the expected return when starting in state s and following policy π:
vπ(s) = Eπ [Gt|St = s] = Eπ
" ∞
X
k=0
γkRt+k+1
 St = s
#
.
Similarly, the action-value function qπ(s, a) gives the expected return after taking action a in state
s and then following π:
qπ(s, a) = Eπ [Gt|St = s, At = a] = Eπ
" ∞
X
k=0
γkRt+k+1
 St = s, At = a
#
.
These functions satisfy a recursive relationship known as the Bellman equation. For vπ:
vπ(s) =
X
a
π(a | s)
X
s′,r
p(s′, r | s, a)

r + γvπ(s′)

.
1.8
Optimal Policies and Value Functions
A policy π∗is said to be optimal if it yields the highest expected return from every state:
v∗(s) = max
π
vπ(s),
∀s ∈S.
Similarly, the optimal action-value function is:
q∗(s, a) = E [Rt+1 + γv∗(St+1) | St = s, At = a] .
Master’s Program – Artificial Intelligence
Page 3


## Page 4

Reinforcement Learning
Laboratory 03
The Bellman optimality equations characterize the optimal value functions:
v∗(s) = max
a∈A(s)
X
s′,r
p(s′, r | s, a)

r + γv∗(s′)

,
q∗(s, a) =
X
s′,r
p(s′, r | s, a)

r + γ max
a′
q∗(s′, a′)

.
These equations are central to the methods implemented in this lab.
1.9
Optimality and Approximation
Solving the Bellman optimality equations exactly can be computationally expensive, especially
when the state space is large. In many practical situations, it is not feasible to find the exact
optimal policy. Instead, we seek approximate solutions that perform well in frequently encountered
situations. Reinforcement learning methods often focus on learning such approximations efficiently,
rather than solving the full system exactly.
2
Policy Iteration
Policy iteration is a fundamental dynamic programming method used to compute the optimal policy
and value function in finite Markov Decision Processes with known dynamics.
The algorithm
alternates between two main steps: policy evaluation and policy improvement. Together, these
steps guarantee convergence to an optimal policy in a finite number of iterations.
2.1
Policy Evaluation
Given a fixed policy π, we compute its value function vπ by solving the Bellman expectation
equation:
vπ(s) =
X
a
π(a | s)
X
s′,r
p(s′, r | s, a)

r + γvπ(s′)

,
∀s ∈S.
This equation defines a system of |S| linear equations in |S| unknowns. Solving this system exactly
can be expensive, so in practice we use iterative policy evaluation. Starting with an initial guess
v0, we apply the following update repeatedly:
vk+1(s) =
X
a
π(a | s)
X
s′,r
p(s′, r | s, a)

r + γvk(s′)

.
This process continues until the value function converges to within a small threshold θ of stability:
max
s
|vk+1(s) −vk(s)| < θ.
2.2
Policy Improvement
Once we have an estimate of vπ, we can construct a new policy π′ by acting greedily with respect
to vπ:
π′(s) = arg max
a
X
s′,r
p(s′, r | s, a)

r + γvπ(s′)

.
This new policy selects, in each state, the action that maximizes the expected return after one step
of lookahead under the current value function. The policy improvement theorem ensures that the
new policy π′ is at least as good as the current policy π, and strictly better if the greedy action
yields a higher value than vπ(s) in any state.
Master’s Program – Artificial Intelligence
Page 4


## Page 5

Reinforcement Learning
Laboratory 03
2.3
Iterative Policy Iteration
The full policy iteration algorithm alternates between evaluation and improvement:
1. Evaluate the current policy πk to compute vπk;
2. Improve the policy by computing πk+1 as the greedy policy w.r.t. vπk;
3. Repeat until the policy no longer changes: πk+1 = πk.
Since the number of deterministic policies is finite and each iteration yields a strictly better
policy (unless the current policy is already optimal), this process is guaranteed to converge to the
optimal policy π∗and its corresponding value function v∗in a finite number of iterations.
2.4
Algorithm (Pseudocode)
A version of policy iteration using iterative evaluation is shown below:
Policy iteration is a powerful planning method when dynamics are known. It is guaranteed to
converge to the optimal policy for finite MDPs and often does so in relatively few iterations. Its
strength lies in its ability to leverage exact environment models to make globally optimal decisions.
In the next section, we consider value iteration, a related algorithm that simplifies policy iteration
by combining evaluation and improvement in each step.
3
Value Iteration
Value iteration is a dynamic programming algorithm that computes the optimal value function v∗
by iteratively applying the Bellman optimality operator. Unlike policy iteration, which separates
policy evaluation and improvement into distinct phases, value iteration combines both steps into a
single update rule.
Master’s Program – Artificial Intelligence
Page 5


## Page 6

Reinforcement Learning
Laboratory 03
The Bellman optimality equation for the value function is:
v∗(s) = max
a
X
s′,r
p(s′, r | s, a)

r + γv∗(s′)

.
Value iteration starts with an arbitrary initialization v0(s) and applies the update:
vk+1(s) = max
a
X
s′,r
p(s′, r | s, a)

r + γvk(s′)

,
for all s ∈S.
Each iteration performs a one-step lookahead: it evaluates the expected return of each action
using the current value estimate vk, and updates the value of state s with the maximum. This
process is repeated until the value function changes very little between iterations.
Although value iteration theoretically requires an infinite number of updates to converge exactly
to v∗, in practice we stop once the value function changes less than a small threshold θ > 0:
max
s
|vk+1(s) −vk(s)| < θ.
This criterion ensures that the value function is approximately optimal. After convergence, we
extract the greedy policy:
π∗(s) = arg max
a
X
s′
p(s′ | s, a)

r(s, a, s′) + γv∗(s′)

.
3.1
Value Iteration vs Policy Iteration
Value iteration can be seen as a special case of policy iteration where the policy evaluation phase is
truncated to just one update (one sweep). It combines the benefits of evaluation and improvement
into a single expected update. Each sweep through the state space improves the value function and
implicitly refines the policy.
Whereas policy iteration may require solving a system of equations during each policy evaluation
step, value iteration avoids this by performing smaller, incremental updates. This can make it
simpler and more memory-efficient for large problems.
3.2
Algorithm (Pseudocode)
Master’s Program – Artificial Intelligence
Page 6


## Page 7

Reinforcement Learning
Laboratory 03
Value iteration is one of the simplest and most powerful methods for computing optimal policies
in finite MDPs.
It avoids the full policy evaluation step required by policy iteration, yet still
converges to the optimal value function and policy. For small problems, it often converges quickly,
and for large problems, its simplicity makes it an attractive baseline. In the next stages of this lab,
we will implement value iteration, visualize convergence, and compare its performance with policy
iteration.
Problem Set
Problem 1
Write a function value iteration() that implements the value iteration algorithm for the envi-
ronment FrozenLake-v1 (with is slippery=False to ensure deterministic transitions). Use the
Bellman optimality equation to update the value function and extract the greedy policy. Run the
algorithm until the value function converges (maximum change below a threshold θ). Return the
final value function and policy.
Problem 2
Write a function policy iteration() to implement the policy iteration algorithm for FrozenLake-v1
(again with is slippery=False). Alternate between policy evaluation and policy improvement
until the policy becomes stable.
Return the optimal policy and value function.
Use the same
convergence threshold θ as in Problem 1.
Problem 3
Analyse the convergence behaviour of both algorithms. For each of the two methods:
• track and plot the maximum change in the value function at each iteration;
• record the number of iterations until convergence.
Compare the two methods in terms of convergence speed and stability. Which one converges faster
and why?
Problem 4
Write a function compare policies() that evaluates the optimal policies obtained from both value
iteration and policy iteration. For each policy:
• run 1000 episodes in FrozenLake-v1 using the policy;
• track the percentage of episodes that reach the goal state.
Compare the performance of the two policies. Are they identical? If not, explain possible reasons
for the difference.
Master’s Program – Artificial Intelligence
Page 7
