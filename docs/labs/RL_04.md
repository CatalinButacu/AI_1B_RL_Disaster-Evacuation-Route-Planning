# RL_04

**Source:** c:\Users\catalin.butacu\Downloads\RL\resourses\labs\RL_04.pdf

**Pages:** 6

---


## Page 1

Reinforcement Learning
Laboratory 04
Laboratory 4: Monte Carlo Methods
Objectives
This lab introduces Monte Carlo prediction methods for estimating the value function of a fixed
policy in model-free, episodic environments. By the end of this lab, you will be able to:
• understand and implement the First-Visit Monte Carlo Prediction algorithm;
• generate and analyse sampled episodes under a given policy;
• estimate state-value functions from observed returns;
• evaluate the convergence behaviour and variance of Monte Carlo value estimates;
• compare first-visit and every-visit MC predictions empirically.
1
Theoretical Background
1.1
First-Visit Monte Carlo Prediction
In many reinforcement learning problems, the agent does not have access to a model of the environ-
ment (i.e., transition probabilities or rewards). In such cases, we must estimate the value function
vπ(s) directly from experience.
Monte Carlo (MC) prediction methods learn vπ by averaging returns observed in sample episodes
generated while following a fixed policy π. These methods are particularly suitable for episodic
tasks where interaction ends in finite time.
The goal is to estimate the value function vπ(s) = Eπ[Gt | St = s] for a given policy π, using
sampled episodes of experience.
Monte Carlo prediction methods estimate value functions by sampling full episodes of interaction
with the environment. Rather than relying on a model to compute expected values, these methods
directly average the actual returns observed after visiting states. This allows learning from raw
experience without bootstrapping or knowledge of transition dynamics:
• Run episodes under policy π.
• For each state s visited, observe the return Gt following that state.
• Use the first time s appears in the episode to update the estimate V (s).
• Average the returns over multiple episodes.
Master’s Program – Artificial Intelligence
Page 1


## Page 2

Reinforcement Learning
Laboratory 04
This results in a value estimate:
V (s) ≈
1
N(s)
N(s)
X
i=1
G(i)(s),
where G(i)(s) is the return following the first visit to s in the i-th episode, N(s) is the total number
of episodes in which s was visited at least once.
Convergence and Properties
First-visit Monte Carlo prediction provides an unbiased estimate of the true value function vπ(s)
by averaging sampled returns. As the number of first visits to each state s increases, the estimate
V (s) converges to the expected return under policy π, as guaranteed by the law of large numbers.
Because updates are made using complete observed returns, the method does not rely on the values
of successor states — that is, it does not bootstrap.
This property distinguishes Monte Carlo
methods from dynamic programming. When episodes are long, first-visit Monte Carlo typically
exhibits lower variance than every-visit methods, as it avoids correlated returns from repeated visits
to the same state within a single episode.
Advantages
Monte Carlo prediction is model-free, meaning it does not require knowledge of the environment’s
transition or reward functions.
It is simple to implement and analyze, making it an excellent
baseline method. Additionally, it naturally focuses learning on frequently visited states, improving
estimates where data is more abundant.
Limitations
This method is limited to episodic tasks, as it relies on episodes terminating to compute returns.
Moreover, a large number of episodes may be required to accurately estimate the value of states
that are visited infrequently, which can result in slow convergence for some parts of the state space.
1.2
Sampling Episodes
Monte Carlo methods learn entirely from experience by observing sampled episodes of interaction
between the agent and the environment. Each episode is a sequence of state transitions generated
by following a fixed policy π, beginning from some initial state and terminating at a terminal state.
An episode has the following general structure:
S0, A0, R1, S1, A1, R2, . . . , ST−1, AT−1, RT , ST ,
where S0 is the initial state, At ∼π(· | St), and T denotes the final time step when the episode
terminates.
For each time step t within the episode, we define the return Gt as the total future discounted
reward:
Gt =
T−t−1
X
k=0
γkRt+k+1,
Master’s Program – Artificial Intelligence
Page 2


## Page 3

Reinforcement Learning
Laboratory 04
where γ ∈[0, 1] is the discount factor. In purely episodic settings, we often use γ = 1.
Unlike dynamic programming, which computes expectations over all possible next states and
rewards, Monte Carlo methods only observe and update based on a single sampled trajectory. This
sampled experience reflects the actual behavior of the agent under the policy, making it suitable
for learning when the environment model is unknown.
Because value updates are made only at the end of an episode (once Gt is available), Monte
Carlo methods are inherently delayed-update algorithms. This property makes them well-suited
for environments where episodes are well-defined and finite.
1.3
Estimating State-Value Functions from Experience
The goal of prediction in reinforcement learning is to estimate the value function vπ(s), defined as
the expected return when starting in state s and following policy π:
vπ(s) = Eπ[Gt | St = s].
Since we do not assume access to the environment’s transition probabilities, we must estimate
vπ(s) directly from data collected during interaction. Monte Carlo methods achieve this by aver-
aging observed returns collected from sample episodes.
Each time a state s is visited during an episode, we record the return Gt following that visit.
Over multiple episodes, we collect a set of returns:
{G(1)(s), G(2)(s), . . . , G(N(s))(s)},
where N(s) is the number of times state s has been visited (first-visit or every-visit, depending on
the method used).
The value estimate is then computed as:
V (s) =
1
N(s)
N(s)
X
i=1
G(i)(s),
Master’s Program – Artificial Intelligence
Page 3


## Page 4

Reinforcement Learning
Laboratory 04
which converges to the true value vπ(s) as N(s) →∞.
Rather than storing all returns, we can update the estimate V (s) incrementally after each new
sample:
V (s) ←V (s) + α (G −V (s)) ,
where G is the new return observed for state s, and α is a step-size parameter. If α =
1
N(s), this
corresponds to the running average used in the first-visit MC algorithm.
This approach is justified by the law of large numbers: as more independent samples of Gt
are collected, their average converges to the expected value. Thus, V (s) becomes a statistically
consistent estimator of vπ(s).
Because the method uses only actual experience and does not rely on value estimates of successor
states, it does not bootstrap. This makes Monte Carlo methods simple and robust, albeit less
efficient than bootstrapped methods like temporal-difference learning in some settings.
Monte Carlo estimation of vπ(s) is especially useful when:
• A model of the environment is unavailable or too complex.
• The task is episodic and episodes terminate reliably.
• High-variance estimates are acceptable or can be averaged over many runs.
1.4
Evaluation of Convergence Speed and Variance
Monte Carlo prediction methods converge to the true value function vπ(s) as more episodes are
sampled. This convergence is supported by the law of large numbers: given enough independent
returns G(i)(s) for a state s, their average approaches the expected return.
1.4.1
Convergence Speed
The convergence speed depends on the number of visits to each state. For states that are fre-
quently visited under the policy π, value estimates converge more quickly. For rarely visited states,
convergence may be slow due to a lack of data.
Formally, the standard error of the sample mean scales as:
Standard Error ∼
σ
p
N(s)
,
where σ2 is the variance of returns and N(s) is the number of (first) visits to state s.
As a result, doubling the number of visits to s reduces the standard error by approximately
1/
√
2. Therefore, even though Monte Carlo estimates are unbiased, their precision improves slowly
— at a rate proportional to 1/√n.
1.4.2
Variance Considerations
Monte Carlo estimates often suffer from high variance due to:
• stochasticity in environment dynamics and rewards,
• the length of episodes, which leads to wide variation in returns Gt,
• delayed updates — values are updated only at the end of episodes.
For example, long or variable-length episodes introduce variability in the total return, which
translates into noisy updates to V (s). This can slow learning and make the estimates unstable
early on.
Master’s Program – Artificial Intelligence
Page 4


## Page 5

Reinforcement Learning
Laboratory 04
1.4.3
First-Visit vs. Every-Visit
• First-visit MC updates the value function using only the first occurrence of a state per
episode. It generally provides unbiased estimates with slightly lower variance per update.
• Every-visit MC uses all occurrences of the state in an episode, leading to more updates
and potentially faster convergence, though estimates may be more correlated and biased in
theory.
In practice, every-visit MC often performs well and is simpler to implement incrementally,
especially when the number of visits per episode is small.
1.4.4
Empirical Evaluation
To assess convergence in practice, it is common to:
• plot the value estimate V (s) over time for selected states,
• track the number of visits N(s) and the corresponding error,
• compute mean squared error (MSE) if the true vπ(s) is known or approximated,
• visualize convergence behaviour using learning curves (e.g., MSE vs. episodes).
These evaluations can reveal how quickly the estimates stabilize, how much variability exists in
early learning, and how sensitive performance is to the underlying policy or environment random-
ness.
Problem Set
In this problem set, you will apply Monte Carlo prediction methods to estimate the state-value
function of a fixed policy in the stochastic FrozenLake-v1 environment.
Problem 1
Define a simple deterministic policy π—for example, always move right if possible, otherwise move
down. Generate a large number of episodes (e.g., 10,000) using this policy, allowing each episode
to terminate when the agent either falls into a hole or reaches the goal. For each episode, record
the first time each state is visited and calculate the return Gt from that time step to the end of
the episode. Use these samples to estimate the value function V (s) via First-Visit Monte Carlo
prediction. At the end of training, visualize the value function across the grid to show how the
agent evaluates the different states under policy π.
Problem 2
Choose a few representative states—for example, the initial state, a central state, and one near
the goal—and monitor how their estimated values V (s) evolve over time. Plot the value estimates
as a function of the number of episodes and describe the convergence behavior you observe. Con-
sider whether the values stabilize quickly or slowly, and what factors might influence the speed of
convergence in different regions of the grid.
Master’s Program – Artificial Intelligence
Page 5


## Page 6

Reinforcement Learning
Laboratory 04
Problem 3
For selected states, collect the complete sequence of observed returns G(i)(s) throughout training.
Compute the empirical variance of these returns and compare across states that are visited fre-
quently versus those visited rarely. Reflect on how episode length, transition stochasticity, and
reward sparsity influence the variability in the returns and the resulting estimates of V (s).
Problem 4
Implement the Every-Visit Monte Carlo prediction algorithm, where the value of a state is updated
using all occurrences of that state in each episode, rather than just the first. Run the same number
of episodes and use the same policy and environment settings as in the previous tasks. Compare the
convergence behaviour of Every-Visit and First-Visit MC by examining value estimates for several
key states over time. Visualize the differences between the two methods using plots or heatmaps,
and reflect on any practical trade-offs you observe, such as differences in stability, convergence
speed, or implementation complexity.
Master’s Program – Artificial Intelligence
Page 6
