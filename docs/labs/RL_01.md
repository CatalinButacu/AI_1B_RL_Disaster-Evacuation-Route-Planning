# RL_01

**Source:** c:\Users\catalin.butacu\Downloads\RL\resourses\labs\RL_01.pdf

**Pages:** 9

---


## Page 1

Reinforcement Learning
Laboratory 01
Laboratory 1: Introduction to Gymnasium
Objectives
This laboratory serves as the foundation for understanding reinforcement learning environments
through practical interaction with the Gymnasium API. Specifically, by the end of this lab, you will
be able to:
• understand Gymnasium environments;
• explore action and observation spaces;
• implement basic policies;
• log and visualize rewards;
1
Theoretical Background
Reinforcement learning (RL) is learning what to do - how to map situations to actions
- so as to maximize a numerical reward signal. The learner is not told which actions
to take, but instead must discover which actions yield the most reward by trying them.
(Barto, Reinforcement Learning, 2nd edition)
1.1
Elements of Reinforcement Learning
Beyond the agent and the environment, one can identify four main subelements of a reinforce-
ment learning system: a policy, a reward signal, a value function, and, optionally, a model of the
environment.
Policy: defines the agent’s way of behaving at any given time.
It is a mapping from the
perceived states of the environment to actions to be taken when in those states.
Reward Signal: the reward defines the goal of the reinforcement learning problem. At each
time step, the environment sends the RL agent a single number, a reward. It is the agent’s sole
objective to maximize this reward. In a biological system, we might think of rewards as analogous
to pain and pleasure. The reward sent at any time depends on the agent’s current action and the
agent’s current state. If my state is hungry and I choose the action of eating, I receive positive
reward.
Value function: reward functions indicate what is good immediately, but value functions
specify what is good in the long run. The value function is the total expected reward an agent
is likely to accumulate in the future, starting from a given state. E.g. a state might always yield
a low immediate reward, but is normally followed by a string of states that yield high reward.
Or the reverse. Rewards are, in a sense, primary, whereas values, as predictions of rewards, are
secondary. Without rewards there could be no value. Nevertheless, it is values with which we are
Master’s Program – Artificial Intelligence
Page 1


## Page 2

Reinforcement Learning
Laboratory 01
most concerned when evaluating decisions. We seek actions that bring the highest value, not the
highest reward, because they obtain the greatest amount of reward over the long run. Estimating
values is not trivial, and efficiently and accurately estimating them is the core of RL.
Model of environment (optionally): something that mimics the behaviour of the true
environment, to allow inferences to be made about how the environment will behave. Given a state
and action, the model might predict the resultant next state and next reward. They are used for
planning, that is, deciding on a course of action by considering possible future situations before
they are actually experienced.
1.2
Key Reinforcement Learning Concepts for Gymnasium
Before we dive into the lab, let’s review a few key ideas that form the foundation of reinforcement
learning and will help you navigate Gymnasium. RL involves an agent interacting with an environ-
ment, making decisions (actions) based on what it observes (states), and receiving rewards for its
choices. The following definitions introduce the basic terms you’ll encounter throughout the lab:
Core elements
• Agent (agent): a learner and decision maker whose goal is to learn a strategy to accomplish
its objective.
• Environment: a simulated world where the agent is located, consisting of different states.
The agent cannot control the environment directly.
• State (s): information describing the current environment that helps the agent decide on an
action. The statespace,S, is the set of all possible states that the agent can be in, including
s and all possible future states the agent can reach from s.
• Observation: the information the agent gathers about a state. The observation may be a
full or partial representation of the current state.
• Action (a): a decision made by the agent in a state s. The set of actions from s is As, and
the overall action space is A = S
s∈S As.
• Reward (r): a real value indicating progress toward the agent’s objective. The set of possible
rewards is R.
Time and interaction
• Timestep (t): a single interaction step consisting of one state, one action, and one reward.
• Episode: a finite sequence of timesteps starting at some initial state s0 (at t0) and ending
in a terminal state sT . The set of all timesteps is T = {0, 1, . . . , T}.
Learning functions
• Value function (V (st)): the expected future rewards from a state or state-action pair,
estimating how good a choice is in the long run.
• Policy (π): a strategy mapping states to actions.
• Action-value function for a policy π (q(s, a)): the expected future return from taking
action a in state s at some period t and following policy π thereafter.
Master’s Program – Artificial Intelligence
Page 2


## Page 3

Reinforcement Learning
Laboratory 01
2
Environment Setup
Prerequisites
• Python 3.8-3.11 installed
• PyCharm
• Virtual Environment (recommended)
Install Gymnasium[?]:
pip install gymnasium[classic_control] matplotlib
To include rendering:
pip install pygame # Required for window rendering
(Optional) To install extra environments:
pip install gymnasium[box2d] # e.g., LunarLander
pip install gymnasium[toy_text] # e.g., Taxi-v3
Check version:
import gymnasium
print(gymnasium.__version__)
3
Gymnasium
Gymnasium is a python library and API standard for developing and testing reinforcement learning
algorithms. It provides a common interface that allows agents to interact with a wide variety of
environments in a consistent way.
Each environment in Gymnasium is a different scenario with a defined objective and states,
actions, and rewards for the agent to interact with. The complete list of available environments,
along with the official documentation, is available at https://gymnasium.farama.org/.
3.1
Initializing the environment
Initializing an environment in Gymnasium is straightforward using the make() function:
\begin{verbatim}
import gymnasium as gym
# Create a beginner-friendly card game environment
env = gym.make(’Blackjack-v1’)
# The Blackjack environment:
# - Task: Play the popular casino card game against a dealer
# - Difficulty: Easy to understand, with stochastic dynamics
# - Benefits: Ideal for learning value-based RL methods
\end{verbatim}
The make() function returns an Env object that you can interact with using Gymnasium’s
standard API. To view all available environments, you can use pprint registry().
Additionally, make() accepts optional arguments, allowing you to configure environments (e.g.,
keyword parameters, wrappers) to suit your experiments. For details, refer to the official documen-
tation.
Master’s Program – Artificial Intelligence
Page 3


## Page 4

Reinforcement Learning
Laboratory 01
3.2
Understanding the Agent-Environment Loop
In reinforcement learning, the classic agent-environment loop, Figure 1, illustrates how an agent
interacts with its surroundings to learn over time. The process is straightforward:
• Observation: the agent perceives the current state of the environment (e.g., viewing a game
screen).
• Action: based on this information, the agent selects an action (e.g., pressing a button).
• Feedback: the environment responds by transitioning to a new state and providing a reward
(e.g., the game state changes and the score updates).
• Iteration: this cycle repeats until the episode ends.
Although this loop appears simple, it forms the foundation of how agents learn to perform
complex tasks—ranging from playing chess to controlling robots to optimizing real-world decision-
making systems.
Figure
1:
Agent-environment
loop
(image
source:
https://gymnasium.farama.org/
introduction/basic_usage/)
3.3
Your First RL Program: Exploring the Blackjack Environment
To get started with reinforcement learning in Gymnasium, we will demonstrate how to interact
with an environment using the classic card game Blackjack-v1.
Blackjack is a simple yet strategic game played with a standard deck of cards. The player is
dealt two cards and aims to achieve a total card value as close to 21 as possible without going over.
In this version of Blackjack:
• an ace counts as 1 or 11;
• face cards (jack, queen, king) count as 10;
Master’s Program – Artificial Intelligence
Page 4


## Page 5

Reinforcement Learning
Laboratory 01
• on each turn, the player can either draw another card (hit) or stop drawing (stand);
• if the card total exceeds 21, the player automatically loses;
• after the player finishes their turn, the dealer plays. The closest to 21 without exceeding it
wins.
Before using any Gymnasium environment, it must first be initialized and reset. Resetting
ensures that the environment starts in a clean state—in this case, dealing a fresh game of Blackjack.
Once you’re done with the environment, it is good practice to close it, signaling Gymnasium to
stop running the environment in the background.
# Run ‘pip install "gymnasium[toy-text]"‘ for this example.
import gymnasium as gym
# Create our training environment - a simple Blackjack game
env = gym.make("Blackjack-v1", render_mode=None)
# Reset environment to start a new game
observation, info = env.reset()
# observation: (player’s current sum, dealer’s visible card, whether player has a usable
ace)
# info: extra debugging information (usually not needed for basic learning)
print(f"Starting observation: {observation}")
# Example output: (15, 10, False)
# Meaning: Player sum = 15, Dealer shows 10, No usable ace
episode_over = False
total_reward = 0
while not episode_over:
# Choose an action: 0 = stick (stop drawing cards), 1 = hit (take another card)
action = env.action_space.sample() # Random action for now - real agents will be
smarter!
# Take the action and see what happens
observation, reward, terminated, truncated, info = env.step(action)
# reward: +1 if player wins, -1 if player loses, 0 for a draw
# terminated: True when the game is over (win, lose, or draw)
# truncated: True if the episode ended for another reason (rare for Blackjack)
total_reward += reward
episode_over = terminated or truncated
print(f"Game finished! Total reward: {total_reward}")
env.close()
Explaining the Code Step by Step
1. Creating the environment: we use gym.make("Blackjack-v1") to set up a new Blackjack
game environment. This initializes a game in which the agent interacts with the dealer by
selecting actions.
2. Resetting the environment: calling env.reset() starts a new game and deals the initial
cards. The returned observation is a tuple (player sum, dealer card, usable ace), where:
• player sum: total value of the player’s hand;
• dealer card: value of the dealer’s visible card;
• usable ace: boolean indicating if the agent has an ace counted as 11 without busting.
Master’s Program – Artificial Intelligence
Page 5


## Page 6

Reinforcement Learning
Laboratory 01
3. Game loop: we define episode_over as False to keep the game running until it ends
naturally. Each iteration of the loop represents a timestep in reinforcement learning.
4. Choosing actions: the agent selects an action using env.action_space.sample(), which
randomly picks either:
• 0 = stick: stop drawing cards;
• 1 = hit: draw another card.
5. Environment response: the environment updates the game state using env.step(action)
and returns:
• observation: new game state after the action;
• reward: +1 (win), 0 (draw), or -1 (loss);
• terminated: true if the game ends naturally;
• truncated: true if the game ends unexpectedly (rare in Blackjack).
6. Ending the episode: when either terminated or truncated is True, the game ends. We
print the total reward and close the environment with env.close().
3.4
Understanding Environments
In Gymnasium, both the action space and observation space of an environment are represented
numerically. To effectively work with any environment, it is essential to understand what these
numbers mean. Gymnasium provides well-structured documentation for most environments. This
documentation typically includes: a clear description of the environment and its objectives, detailed
information about the action space (possible moves or decisions the agent can take), detailed
information about the observation space (the data received from the environment at each step),
an explanation of how rewards are calculated.
Referring to this documentation is strongly recommended when experimenting with new envi-
ronments, as it helps interpret the numerical inputs and outputs correctly.
In addition to reading documentation, some environments are easier to understand through
visual feedback. For example, the environment "Acrobot-v1" simulates a two-link pendulum that
swings under applied forces.
By rendering the environment, you can watch the pendulum’s
movements in real time, making it easier to understand how different actions influence its behavior.
To render an environment, you must set the argument render mode=’human’. This opens a
visual display window that shows the environment’s dynamics step by step as the agent interacts
with it.
Example: Rendering Acrobot-v1
# Run ‘pip install "gymnasium[classic-control]"‘ if needed
import gymnasium as gym
env = gym.make(’Acrobot-v1’, render_mode=’human’)
observation, info = env.reset()
done = False
trunc = False
while not done and not trunc: # Until the environment terminates...
# Take random step
random_action = env.action_space.sample()
obs, reward, done, trunc, info = env.step(random_action)
env.close()
Master’s Program – Artificial Intelligence
Page 6


## Page 7

Reinforcement Learning
Laboratory 01
3.5
Action and Observation Spaces
Each Gymnasium environment specifies the valid formats for both actions and observations using
the action space and observation space attributes. These spaces define: what the agent can
do (action space), what the agent can observe or sense (observation space).
All valid actions and observations must be contained within their respective spaces. In the pre-
vious examples, we used env.action space.sample() to select random actions. Later in this lab,
you will replace this random behaviour with a learned policy that maps observations to meaningful
actions.
Action space: defines the set of all possible actions your agent can take (for example, pressing
a button, moving left or right, applying a continuous force, etc.).
Observation space: defines the information your agent receives about the environment (for
example, numbers, images, or structured data describing the current state).
Both attributes are instances of the python class Space, which provides:
• Space.contains(x) – checks if x is a valid element of the space.
• Space.sample() – randomly samples a valid element from the space.
Gymnasium supports various types of spaces (discrete, continuous, text-based, structured, etc.),
depending on the environment. For a full list and detailed explanations of available spaces, refer
to the official documentation at: https://gymnasium.farama.org/api/spaces/.
Understanding these spaces is crucial before building agents, as they define both what your
agent sees and what it can do.
3.6
Running Episodes
An episode in Gymnasium represents one complete interaction cycle between the agent and the
environment, starting from an initial state and ending when the task is finished or interrupted.
Each episode begins by resetting the environment using env.reset(), which sets everything to its
initial state and returns the first observation. From there, the agent repeatedly selects actions, and
the environment responds by updating its state, returning a new observation, a reward, and two
status flags: terminated and truncated. The flag terminated signals that the task has naturally
ended (for example, the game is won or lost), while truncated indicates that the episode stopped
due to reaching a predefined time or step limit. When either flag becomes True, the episode is
considered complete and can be restarted by calling env.reset() again.
import gymnasium as gym
env = gym.make("Blackjack-v1")
observation, info = env.reset()
done = False
while not done:
action = env.action_space.sample() # Random action
observation, reward, terminated, truncated, info = env.step(action)
done = terminated or truncated
print("Episode finished!")
env.close()
Running episodes like this allows you to observe how your agent interacts with the environment
over time, collect data, and prepare the ground for training more sophisticated agents.
Master’s Program – Artificial Intelligence
Page 7


## Page 8

Reinforcement Learning
Laboratory 01
3.7
Logging and Visualizing Results
When running reinforcement learning experiments, it is often helpful to keep track of the agent’s
performance at each timestep or for each episode. This process, known as logging, allows you to
analyze how well an agent is doing, whether it is improving over time, and how different strategies
compare.
A simple way to log results is to record the total reward obtained during an episode. At the
start of each episode, you can initialize a variable to store rewards, and during every step, you add
the received reward to this total. Once the episode ends, the total reward can be printed, saved to
a file, or plotted for further analysis.
import gymnasium as gym
env = gym.make("Blackjack-v1")
num_episodes = 5
episode_rewards = []
for episode in range(num_episodes):
observation, info = env.reset()
done = False
total_reward = 0
while not done:
action = env.action_space.sample()
observation, reward, terminated, truncated, info = env.step(action)
total_reward += reward
done = terminated or truncated
episode_rewards.append(total_reward)
print(f"Episode {episode+1} finished with total reward: {total_reward}")
env.close()
print("All episode rewards:", episode_rewards)
Logging results like this provides valuable feedback on how the agent interacts with the envi-
ronment and is a critical step before moving to more advanced techniques, such as plotting learning
curves or using specialized logging libraries for large-scale experiments.
While logging results in text format is helpful, visualizing them often makes it much easier to
understand the agent’s performance over multiple episodes. A simple way to do this is to plot the
total rewards obtained in each episode using a library such as matplotlib. This gives a quick
overview of whether the agent is improving, staying the same, or performing randomly.
# Plotting the results
plt.plot(episode_rewards)
plt.title("Total Rewards per Episode")
plt.xlabel("Episode")
plt.ylabel("Total Reward")
plt.grid(True)
plt.show()
This visualization provides an immediate understanding of the agent’s overall behaviour. For
example, a flat line around zero could indicate that the agent is acting randomly, while a positive
trend suggests that the agent is finding strategies to perform better in the environment.
Master’s Program – Artificial Intelligence
Page 8


## Page 9

Reinforcement Learning
Laboratory 01
3.8
Basic Policy
So far, we have used env.action space.sample() to select random actions in the environment.
While this is useful for testing, real agents should follow a policy, which is a function that maps
observations to actions. A policy allows the agent to make decisions based on what it ¨sees¨ın the
environment rather than choosing actions blindly.
For the Blackjack-v1 environment, a very simple policy can be defined:
• if the player’s current hand value is less than 18, choose action hit (take another card).
• otherwise, choose action stick (stop taking cards).
This basic policy demonstrates how agents can make decisions based on their observations
instead of purely random actions.
Problem set
Problem 1
Write a function random blackjack() that accepts an integer n. Run and initialize the environment
Blackjack-v1 for a total of n episodes, and in each episode take random actions until the game is
terminated. Return the percentage of games the player wins. Use your function to print the win
percentage after 50000 episodes (i.e., after 50000 games).
Problem 2
Write a function blackjack() which runs a na¨ıve algorithm to play blackjack. The function should
receive an integer n as input. If the player’s hand is less than or equal to n, the player should draw
another card. If the player’s hand is more than n, they should stop playing. Within the function,
run the algorithm for 10000 episodes and return the percentage of games the player wins. For
n = 1, 2, . . . , 21, plot the average win rate returned by your function. Identify which value(s) of n
win most often.
Problem 3
Write a function random blackjack tracking() that plays a given number of episodes in the
environment Blackjack-v1 using random actions (as in Problem 1). Instead of returning only the
overall win percentage, track the cumulative win percentage after every 1000 episodes. Plot the
evolution of the win rate over time to observe how results stabilize as more episodes are played.
Problem 4
Write a function compare policies() that runs both:
• The random blackjack() strategy, and
• The na¨ıve threshold strategy blackjack(n) for all n = 1, 2, . . . , 21
Compute the win rate over 10000 episodes for each policy. Plot a bar chart comparing the win rate
of the random policy and the best-performing threshold policy. Which strategy performs better,
and why?
Master’s Program – Artificial Intelligence
Page 9
