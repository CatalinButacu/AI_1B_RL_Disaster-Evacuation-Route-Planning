# RL_06

**Source:** c:\Users\catalin.butacu\Downloads\RL\resourses\labs\RL_06.pdf

**Pages:** 5

---


## Page 1

Reinforcement Learning
Laboratory 06
Laboratory 6: Actor-Critic Methods
Objectives
This laboratory introduces actor-critic methods, a fundamental class of reinforcement learning
algorithms that combine the advantages of both policy-based and value-based approaches. In these
methods, two components: the actor and the critic, work together to learn an optimal policy:
the actor updates the policy parameters in the direction suggested by the critic, while the critic
evaluates how good the current policy is by estimating value functions.
By the end of this laboratory, you will be able to:
• understand the theoretical foundations of actor-critic methods and their connection to policy
gradient approaches;
• explain the distinction between the actor (policy) and the critic (value function) and how
they interact during learning;
• implement a one-step actor-critic algorithm for on-policy control in a simple environment;
• apply function approximation (e.g., linear regression) to represent both the policy and value
functions;
• analyse the effect of key hyperparameters such as learning rates for the actor and critic;
• compare the stability and convergence behaviour of actor-critic methods with pure value-
based methods like Q-Learning.
1
Theoretical Background
1.1
One–step actor–ritic
The one–step actor–critic algorithm represents the simplest form of policy–gradient methods
combined with value estimation. It integrates the ideas of temporal–difference (TD) learning and
policy gradient ascent within a single framework.
At each time step t, the agent observes a transition (St, At, Rt+1, St+1) while following its current
policy π(a|s, θ). The critic estimates the value of the current state using a parameterized function
v(St, w) and computes the one–step TD error:
δt = Rt+1 + γv(St+1, w) −v(St, w),
where γ is the discount factor. This quantity expresses how much better or worse the observed
outcome was compared to the critic’s prediction.
The critic’s parameters are updated by a semi–gradient TD(0) rule:
w ←w + αw δt ∇wv(St, w),
Master’s Program – Artificial Intelligence
Page 1


## Page 2

Reinforcement Learning
Laboratory 06
and the actor updates its policy parameters using the same TD error as a learning signal:
θ ←θ + αθ δt ∇θ ln π(At|St, θ).
In this way, the critic provides an immediate numerical estimate of the advantage of the chosen
action, guiding the actor toward actions that yield higher expected returns.
1.2
Combining policy and value updates
In actor–critic architectures, learning occurs through a tight interaction between two distinct update
processes:
• the critic update reduces prediction error by improving the value estimate of states under
the current policy;
• the actor update adjusts the policy parameters in the direction that increases the likelihood
of advantageous actions.
These two updates form a feedback loop:
1. The critic evaluates how well the actor is performing by computing δt.
2. The actor then changes its policy according to the critic’s feedback.
3. As the actor changes its behaviour, the critic must continuously re–estimate the new value
function.
This dynamic coupling enables the algorithm to converge more smoothly than pure policy–gradient
methods such as REINFORCE, while maintaining faster responsiveness than purely value–based
methods like Q–Learning.
1.3
On–policy learning
Actor–critic methods are typically implemented in an on–policy setting. The same policy π(a|s, θ)
that is being optimized is also used to generate the experience on which learning is based. This
ensures that the TD error δt provides an unbiased estimate of the true return under the current
policy.
Because the learning process depends entirely on trajectories generated by the policy being
improved, on–policy actor–critic algorithms are stable and well–suited for environments where
exploratory actions must remain consistent with safe or realistic behaviour. Exploration is usually
ensured by employing a stochastic policy (e.g., a soft–max or Gaussian distribution) so that all
actions have non–zero probability of selection.
1.4
Implementation using function approximation
In most practical problems, the number of states or state–action pairs is too large for tabular
representations. Hence, both the actor and critic employ function approximation to generalize
across similar states.
A common and efficient approach is to use linear regression models for both components:
v(s, w) = w⊤x(s),
π(a|s, θ) =
exp(θ⊤x(s, a))
P
b exp(θ⊤x(s, b))
,
Master’s Program – Artificial Intelligence
Page 2


## Page 3

Reinforcement Learning
Laboratory 06
where x(s) or x(s, a) are feature vectors that encode relevant characteristics of the state or state–
action pair.
Under this parameterization:
• the critic’s update becomes equivalent to an incremental linear regression minimizing the TD
prediction error;
• the actor’s update performs a stochastic gradient ascent step in the policy parameters, scaled
by the TD error.
Linear function approximation yields simple, interpretable updates while maintaining differen-
tiability of both components, making it a practical choice for small to medium–sized problems. In
modern extensions, the same principles are implemented using deep neural networks, giving rise to
Deep Actor–Critic algorithms.
2
Implementation overview
The one–step actor–critic algorithm presented in this laboratory is based directly on the formu-
lation from Sutton and Barto (Reinforcement Learning: An Introduction, 2nd ed., Chapter 13,
Section 13.5).
It is an on–policy, incremental method that combines the gradient–based policy update of the
actor with the temporal–difference (TD) value estimation of the critic.
At each time step t, given the transition (St, At, Rt+1, St+1), the critic estimates how good the
action At was under the current policy π(a|s, θ) by computing the one–step TD error:
δt = Rt+1 + γ ˆv(St+1, w) −ˆv(St, w),
Master’s Program – Artificial Intelligence
Page 3


## Page 4

Reinforcement Learning
Laboratory 06
where ˆv(s, w) is the critic’s estimate of the state–value function. This TD error serves as a low–
variance, online estimate of the advantage of the action just taken.
The critic updates its parameters by a semi–gradient TD(0) rule:
w ←w + αw δt ∇wˆv(St, w),
while the actor performs a policy–gradient step using the same δt signal:
θ ←θ + αθ δt ∇θ ln π(At|St, θ).
This dual update mechanism reflects the actor–critic interplay: the critic evaluates actions and
produces a scalar feedback signal, while the actor uses that signal to improve its policy in the
direction that increases expected return.
This algorithm is fully on–policy: both the actor and critic learn exclusively from trajectories
generated by the current policy π(a|s, θ). The TD error δt thus provides an unbiased estimate of
the performance gradient under the same policy being improved.
2.1
Function approximation and linear models
We represent both the actor and critic with differentiable linear models.
Critic (state–value function):
ˆv(s, w) = w⊤x(s),
where x(s) is a feature vector representing the state.
Actor (soft–max policy):
π(a|s, θ) =
exp(θ⊤x(s, a))
P
b exp(θ⊤x(s, b))
.
For this parameterization, the gradient of the log–policy (the eligibility vector in the book’s
notation) is given by:
∇θ ln π(a|s, θ) = x(s, a) −
X
b
π(b|s, θ) x(s, b).
This linear representation allows the algorithm to generalize from limited experience, reduce
variance in updates, and align directly with the form used in the theoretical derivations of Chap-
ter 13.
2.2
Relationship to REINFORCE with baseline
The one–step actor–critic can be viewed as a bootstrapped version of the REINFORCE with baseline
algorithm (Section 13.4, Equation 13.11). Instead of waiting until the end of the episode to compute
the full return Gt, the critic supplies an incremental baseline v(St, w), and the TD error δt replaces
(Gt −v(St, w)). This modification introduces a small bias but substantially reduces variance and
allows for fully online learning.
Master’s Program – Artificial Intelligence
Page 4


## Page 5

Reinforcement Learning
Laboratory 06
Problem Set
Problem 1
Implement the one–step actor–critic algorithm using the FrozenLake-v1 environment from
OpenAI Gym. This environment provides a compact gridworld where the agent must learn to
navigate from the start state to the goal while avoiding holes. Its stochastic transitions make it
ideal for testing on–policy methods.
Represent the policy π(a|s, θ) as a soft–max distribution over linear action preferences and
approximate the state–value function as ˆv(s, w) = w⊤x(s), where x(s) is a one–hot vector encoding
the agent’s current state.
Use the following update equations at each time step:
δt = Rt+1+γˆv(St+1, w)−ˆv(St, w),
w ←w+αw δt ∇wˆv(St, w),
θ ←θ+αθ δt ∇θ ln π(At|St, θ).
Train the agent for approximately 5,000 episodes using αw = 0.1, αθ = 0.01, and γ = 0.99. At
the end of training, extract the greedy policy π∗(s) = arg maxa π(a|s, θ) and visualize it over the
grid to show the optimal direction of movement from each state.
Problem 2
Track how the agent’s performance evolves during training. Record the total reward per episode
and compute a moving average over the last 100 episodes to smooth the learning curve. Observe
whether the average return increases and stabilizes over time. Compare your results with those
from the Q–Learning lab: does the actor–critic agent converge more smoothly, and if so, why?
In addition to the rewards, record the average magnitude of the TD error δt. A decreasing trend
in |δt| indicates that the critic’s predictions are becoming more accurate, reflecting stable learning
dynamics.
Problem 3
Investigate the effect of the actor’s and critic’s step sizes on stability and convergence. Repeat
training with different pairs of (αθ, αw), such as (0.001, 0.01), (0.01, 0.05), and (0.1, 0.1). Plot the
corresponding learning curves and discuss the differences.
Explain qualitatively how the two components interact: if the actor learns too quickly, it may
change the policy before the critic has accurately evaluated it; if the critic learns too slowly, the
actor receives outdated feedback.
Problem 4
Modify the feature representations used in the actor and critic to explore their impact on learning
performance. For the critic, try representing the state features x(s) as:
• one-hot vectors (tabular features), suitable for discrete environments such as FrozenLake-v1;
• spatial encodings based on the grid layout (for example, use the row and column indices of
the current cell as numerical features, or include a binary indicator for proximity to the goal);
• normalized or smoothed feature representations that encourage generalization between neigh-
bouring states.x
Similarly, for the actor, alter the features x(s, a) in the soft–max policy and observe how this
affects exploration and convergence.
Master’s Program – Artificial Intelligence
Page 5
