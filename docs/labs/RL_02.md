# RL_02

**Source:** c:\Users\catalin.butacu\Downloads\RL\resourses\labs\RL_02.pdf

**Pages:** 6

---


## Page 1

Reinforcement Learning
Laboratory 02
Laboratory 2: Multi-Armed Bandits
Objectives
This laboratory focuses on the exploration-exploitation trade-off in Multi-Armed Bandits problems
and provides hands-on experience with different action selection strategies. Specifically, by the end
of this lab, you will be able to:
• understand the Multi-Armed Bandits problem and its role in reinforcement learning;
• implement the ε-greedy algorithm for action selection;
• explore the effect of optimistic initial values on exploration;
• apply the Upper-Confidence-Bound (UCB) method for decision making;
• track and update average action-value estimates incrementally;
• compare the performance of different exploration strategies in stationary settings.
1
Theoretical Background
1.1
The multi-armed bandit problem
In the multi-armed bandit problem, you are repeatedly faced with a choice among n different
actions (also called arms). Each time you select an action, you receive a reward drawn from
a stationary probability distribution unique to that action. The objective is to maximize
the total reward over a given time horizon, for example, 100 time steps. The name of the
problem comes from the analogy of a slot machine, or one-armed bandit, but here there are
n arms instead of one. Each action has an expected or mean reward determined by its
underlying probability distribution, which we call the value of the action. However, these values
are unknown to the decision maker. Because of this uncertainty, there is always a fundamental
trade-off between exploration and exploitation. At any instant, we may have one action that
appears to be the most valuable based on past experience, yet it is highly likely, especially in the
early stages, that other actions we have not explored sufficiently could yield even greater rewards.
1.2
ε-greedy exploration
One of the simplest and most widely used methods for balancing exploration and exploitation in
the multi-armed bandit problem is the ε-greedy strategy. In this method, at each time step,
the agent selects the action that currently appears to have the highest estimated value (greedy
action) with probability 1 −ε, where 0 ≤ε ≤1. With the remaining probability ε, the agent
instead selects an action uniformly at random from all available actions. This occasional random
Master’s Program – Artificial Intelligence
Page 1


## Page 2

Reinforcement Learning
Laboratory 02
Figure 1: Average performance of ε-greedy action-value methods on the 10-armed testbed. These
data are averages over 2000 runs with different bandit problems. All methods used sample averages
as their action-value estimates.
exploration allows the agent to try out actions that may not currently seem optimal, but could
lead to the discovery of better long-term rewards. The choice of ε determines the balance between
exploitation and exploration: a smaller ε leads to more exploitation of known good actions, while
a larger ε increases the frequency of exploration. In practice, small values such as ε = 0.1 often
provide a good compromise, ensuring that all actions are sampled infinitely often in the long run
while still favouring the best-known actions most of the time.
To improve learning efficiency over time, a decaying ε can be employed: initially, ε is set high
to promote exploration when knowledge about the environment is limited, and gradually decreased
to favour exploitation as the agent gathers more information. A common linear decay schedule is
given by:
δ = ε1 −εN
N
,
ε1 > εN
(1)
εi = ε1 −(i −1)δ
(2)
where ε1 is the starting exploration rate, εN is the final exploration rate after N episodes, and εi
Master’s Program – Artificial Intelligence
Page 2


## Page 3

Reinforcement Learning
Laboratory 02
is the exploration rate at episode i. This approach allows the agent to explore sufficiently early on,
while progressively focusing on exploiting high-reward actions as learning progresses.
1.3
Optimistic initial values
The optimistic initial values method is another strategy for encouraging exploration in the multi-
armed bandit problem. Instead of selecting random actions explicitly (as in ε-greedy), this method
drives exploration by initializing the estimated value of each action to a deliberately high value.
Because all initial action-value estimates are overly optimistic, the agent is naturally encouraged
to explore all actions in the beginning to determine their true values.
As the agent gathers experience and updates its estimates based on actual rewards received, the
values gradually converge to more accurate predictions. In effect, this approach biases the agent
toward trying all actions at least once, but does not require random exploration.
This method can be especially effective in stationary environments, where the reward distribu-
tions do not change over time. However, it is sensitive to the magnitude of the initial values: if the
values are too high, it may take longer to converge; if they are not optimistic enough, the agent
may still fall into premature exploitation. Compared to ε-greedy, this strategy encourages more
systematic early exploration, while allowing purely greedy action selection throughout.
1.4
Upper confidence bound (UCB) action selection
The ε-greedy strategy explores by selecting random actions, regardless of their potential. In con-
trast, the Upper Confidence Bound (UCB) method selects actions based on both their esti-
mated value and the uncertainty or variance in those estimates. The idea is to prefer actions that
have been tried less often, since their value estimates are more uncertain and may still turn out to
be optimal.
At each time step t, the UCB action selection rule chooses the action a that maximizes the
following expression:
at = arg max
a
"
Qt(a) + c
s
ln t
Nt(a)
#
(3)
Here, Qt(a) is the current estimated value of action a, Nt(a) is the number of times action a
has been selected so far, t is the current time step, and c > 0 is a constant that controls the degree
of exploration. The second term acts as an exploration bonus: actions with low Nt(a) (i.e., not
tried often) get a higher score, encouraging the agent to explore them.
UCB balances exploration and exploitation in a more targeted way than random exploration. It
chooses actions optimistically based on both estimated value and potential for improvement. This
method performs particularly well in stationary settings and has strong theoretical guarantees for
minimizing regret.
1.5
Average reward tracking
To estimate the value of each action, agents maintain an action-value estimate, denoted Qt(a),
which approximates the expected reward of action a based on observed outcomes. A straightforward
approach is to compute the sample average of rewards obtained when selecting that action:
Qt(a) =
1
Nt(a)
Nt(a)
X
i=1
Ri
(4)
Master’s Program – Artificial Intelligence
Page 3


## Page 4

Reinforcement Learning
Laboratory 02
where Nt(a) is the number of times action a has been selected, and Ri are the observed rewards.
In practice, it is inefficient to store all past rewards. Instead, we use an incremental update
rule to maintain the running average:
Qt+1(a) = Qt(a) + α (Rt −Qt(a))
(5)
Here, Rt is the reward received after selecting action a at time t, and α is a step-size parameter.
If α =
1
Nt(a), this update is equivalent to the sample average. However, in non-stationary envi-
ronments (where reward distributions may change over time), it is beneficial to use a constant α
(e.g., 0.1) to give more weight to recent rewards.
Tracking average reward using this method allows the agent to learn efficiently without retaining
historical data, and enables adaptability in both stationary and dynamic environments.
1.6
Comparison of strategies in stationary settings
In stationary bandit problems, the reward distributions for each action do not change over time.
This allows us to directly compare the performance of different exploration strategies under stable
conditions.
Three widely used strategies are:
1.6.1
ε-greedy
Simple and effective. Randomly explores with probability ε and exploits with 1−ε. Its performance
depends on choosing a good ε: too small leads to insufficient exploration; too large results in too
much randomness.
1.6.2
Optimistic Initial Values
Encourages early exploration by initializing all action-value estimates to high values. Unlike ε-
greedy, it uses pure greedy action selection after initialization. Performs well in stationary settings,
but may take longer to converge if initial values are poorly chosen.
1.6.3
Upper Confidence Bound (UCB)
Balances estimated value and uncertainty. Selects actions with high value estimates and/or low
visit counts. Often outperforms ε-greedy in stationary settings due to more directed exploration.
Performance metrics commonly used for comparison include:
• Average reward over time: how quickly and how well the agent learns to select high-
reward actions.
• Percentage of optimal action selections: measures how often the agent chooses the best
possible action.
Empirical results from the book Reinforcement Learning: An Introduction by Sutton and Barto
(Chapter 2, 10-armed testbed) show that:
• UCB typically achieves higher average rewards and selects the optimal action more frequently
than ε-greedy with fixed ε.
• Optimistic initial values can perform comparably or better than ε-greedy when the initial
values are well chosen.
Master’s Program – Artificial Intelligence
Page 4


## Page 5

Reinforcement Learning
Laboratory 02
• All methods improve with time, but their speed and efficiency of learning differ significantly.
In summary, while ε-greedy is easy to implement, methods like UCB and optimistic initialization
provide more targeted exploration and may yield better performance in stationary problems.
Table 1: Comparison of action selection strategies in stationary bandit problems
Strategy
Exploration type
Hyperparameters
Key characteristics
ε-greedy
random, uniform ex-
ploration
ε
simple to implement; may re-
peatedly explore suboptimal ac-
tions; performance sensitive to ε
choice
Optimistic
Initial
Values
implicit via high ini-
tial estimates
initial Q0 values
pure greedy selection;
encour-
ages early exploration; effective
in stationary settings if Q0 is well
chosen
Upper
Confidence
Bound (UCB)
optimism in the face
of uncertainty
c (confidence level)
directed exploration using uncer-
tainty;
favours under-explored
actions; strong theoretical per-
formance bounds
Problem set
Problem 1
Write a function compare epsilon greedy random(k, steps, runs) that implements a k-armed
bandit problem with stationary reward distributions. For each of the following values of k: 5, 25,
and 100, run two agents:
• one that selects actions completely at random,
• one that uses the ε-greedy strategy with ε = 0.1.
Track and plot the average reward over time and the percentage of optimal action selections.
Measure how many steps it takes until the average reward stabilizes close to its maximum. Use
your function to produce and compare plots for each value of k.
Problem 2
Write a function compare variance(steps, runs) that implements a 10-armed bandit where the
true action values are sampled from N(µ, σ2) with µ ∈[1, 2]. Compare two settings: σ2 = 1 and
σ2 = 5. For each setting, run three agents:
• a greedy agent with Q0 = 0,
• a greedy agent with optimistic initial values Q0 = 10,
• an ε-greedy agent with ε = 0.1.
Plot the average reward and the percentage of optimal action selections. Compare and discuss how
increased variance affects the performance of each agent.
Master’s Program – Artificial Intelligence
Page 5


## Page 6

Reinforcement Learning
Laboratory 02
Problem 3
Write a function compare ucb c(steps, runs) that implements the UCB action selection rule (see
equation 3). Use a 10-armed bandit with rewards drawn from N(2, 2). Run experiments for c = 1,
c = 2, and c = 4. Compare the performance of UCB with:
• an ε-greedy agent with ε = 0.1,
• an optimistic greedy agent with Q0 = 5.
Plot the average reward and the percentage of optimal actions for each c and discuss how the
parameter c influences exploration.
Problem 4
Write a function compare strategies(steps, runs) that creates a testbed of 2000 independent
10-armed bandit problems, each running for 1000 steps. The true action values should be sampled
from N(µ, σ2) with µ ∈[1, 2] and σ2 = 2. Compare the following agents:
• ε-greedy with fixed ε = 0.1,
• ε-greedy with linearly decaying ε,
• greedy with optimistic initialization Q0 = 10,
• UCB with c = 2.
For each strategy, compute and plot the average cumulative reward, the speed of convergence (how
quickly the agent consistently selects the optimal action), and the percentage of optimal action
selections. Summarize your findings in a short written analysis.
Master’s Program – Artificial Intelligence
Page 6
