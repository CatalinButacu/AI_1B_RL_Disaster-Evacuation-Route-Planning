# RL_05

**Source:** c:\Users\catalin.butacu\Downloads\RL\resourses\labs\RL_05.pdf

**Pages:** 7

---


## Page 1

Reinforcement Learning
Laboratory 05
Laboratory 5: Q-Learning Algorithm
Objectives
This laboratory focuses on the implementation and analysis of the Q-Learning algorithm, one
of the most widely used methods for model-free, off-policy reinforcement learning. Through this
session, students will explore how an agent can learn optimal action policies by interacting with an
environment and updating its action-value estimates using temporal-difference learning.
By the end of this lab, you will be able to:
• understand the concept of off-policy control and distinguish it from on-policy approaches
such as SARSA;
• implement the tabular Q-Learning algorithm with ε-greedy exploration;
• analyse the effect of the exploration–exploitation trade-off on learning performance;
• evaluate the convergence behaviour of Q-Learning through cumulative reward plots;
• visualize the learned Q-values and extract the corresponding optimal policy.
The overarching goal is to build intuition about how temporal-difference control methods enable
agents to learn optimal behaviour in stochastic environments without requiring a model of the
environment’s dynamics.
1
Theoretical Background
Temporal-Difference (TD) learning lies at the heart of modern reinforcement learning.
It
combines key ideas from both Monte Carlo methods and Dynamic Programming (DP). Like Monte
Carlo methods, TD learning estimates value functions directly from raw experience without re-
quiring a model of the environment’s dynamics. Like DP, it updates its estimates based on other
learned estimates, a process known as bootstrapping.
At each time step t, the TD method updates its value estimate using the observed reward and
the estimated value of the next state:
V (St) ←V (St) + α

Rt+1 + γV (St+1) −V (St)

,
where α is the learning rate and γ the discount factor. The term in brackets is the TD error,
representing the difference between the current estimate and a better, more informed estimate
based on the next state.
TD methods have two key advantages:
1. They learn online and incrementally, updating estimates at each step rather than waiting
until the end of an episode.
Master’s Program – Artificial Intelligence
Page 1


## Page 2

Reinforcement Learning
Laboratory 05
2. They do not require knowledge of transition probabilities or reward models.
These properties make TD learning suitable for large, stochastic, or continuous environments where
model-based methods are impractical.
1.1
On-Policy Control: the SARSA algorithm
To move from prediction (estimating values) to control (finding optimal actions), TD learning can
be extended to learn action-value functions, Q(s, a), representing the expected return of taking
action a in state s and following a policy π thereafter.
In on-policy control, the agent learns about the same policy it uses to interact with the
environment. A standard on-policy TD control algorithm is SARSA, whose name reflects the
quintuple (St, At, Rt+1, St+1, At+1) used in its update:
Q(St, At) ←Q(St, At) + α

Rt+1 + γQ(St+1, At+1) −Q(St, At)

.
The policy is typically chosen to be ε-greedy with respect to Q, meaning that most of the time the
agent selects the action with the highest estimated value, but occasionally explores other actions
with probability ε. This ensures a balance between exploitation (choosing the best-known action)
and exploration (trying new actions to gather information).
SARSA is a natural and safe learning strategy: it evaluates and improves the same policy it
follows, making it well-suited for environments where risky exploratory actions should be avoided.
However, this coupling between learning and behaviour can slow convergence to the optimal policy.
1.2
Off-Policy Control: the Q-Learning algorithm
Q-Learning represents an important step beyond SARSA by decoupling the behaviour policy
(used to gather experience) from the target policy (used to evaluate actions). This makes it an off-
policy method: it can learn the optimal action-value function q∗(s, a) while following a different,
exploratory behaviour.
The core Q-Learning update rule is:
Q(St, At) ←Q(St, At) + α

Rt+1 + γ max
a
Q(St+1, a) −Q(St, At)

.
Unlike SARSA, which updates using the value of the next action actually taken, Q-Learning updates
using the value of the best possible action in the next state. This use of the maximum future Q-
value allows the algorithm to approximate the optimal policy even while exploring under an ε-greedy
strategy.
Over time, as all state–action pairs are sufficiently explored and the learning rate decays ap-
propriately, Q converges to the optimal action-value function q∗(s, a). The corresponding optimal
policy is then derived as:
π∗(s) = arg max
a
Q(s, a).
Thus, Q-Learning enables agents to learn optimal behaviours off-policy, combining efficient
online learning with robust convergence properties.
In this laboratory, we will implement and
analyse this algorithm to visualize how an agent learns to act optimally through trial and error.
Master’s Program – Artificial Intelligence
Page 2


## Page 3

Reinforcement Learning
Laboratory 05
1.3
ε-greedy behaviour policy
A central challenge in reinforcement learning is the exploration–exploitation dilemma: the
agent must balance exploiting the best-known actions to maximize immediate reward with exploring
other actions to discover potentially better long-term strategies.
Exploitation relies on current estimates of the action-value function Q(s, a) to select the action
believed to yield the highest expected return:
a∗= arg max
a
Q(s, a).
However, if the agent only exploits, it may never try alternative actions and could converge prema-
turely to a suboptimal policy. To avoid this, we introduce controlled randomness into the policy
through exploration.
A simple and widely used exploration mechanism is the ε-greedy policy. Under this strategy,
the agent selects a random action with small probability ε and follows the greedy action (the one
maximizing Q(s, a)) with probability 1 −ε:
π(a|s) =





1 −ε +
ε
|A(s)|,
if a = arg maxa′ Q(s, a′),
ε
|A(s)|,
otherwise.
This policy ensures that all actions continue to be sampled with non-zero probability, satisfying
the conditions required for convergence of Q-Learning. At the same time, the probability of selecting
the best-known action remains high, preserving efficient learning.
In practice, the exploration rate ε is often annealed (gradually decreased) during training:
εt = ε0 e−kt,
where k controls the decay rate. Early in learning, a higher ε encourages broader exploration; as
learning progresses, ε is reduced to promote exploitation of the near-optimal policy.
Together with the temporal-difference update rule, the ε-greedy behaviour policy enables an
agent to learn optimal decision-making strategies in uncertain and stochastic environments.
1.4
Tabular Q-learning implementation
The following is the tabular Q-Learning algorithm, as presented in Sutton and Barto’s Rein-
forcement Learning: An Introduction (2nd ed., p. 131). It describes an off-policy temporal-difference
(TD) control method for estimating the optimal action-value function q∗(s, a).
The following explanation outlines the purpose of each step in the Q-Learning algorithm and
highlights the intuition behind its update rule:
• Initialization: each state–action pair (s, a) is assigned an initial value Q(s, a), often zero.
The terminal state’s Q-values are set to zero since no further rewards are expected after
termination.
• Behaviour policy: the agent follows a policy derived from Q, typically an ε-greedy policy.
This allows occasional exploration (random actions) with probability ε while mostly exploiting
the current best-known actions.
• Action and transition: at each step, the agent executes an action A in state S, observes
the immediate reward R, and transitions to a new state S′.
Master’s Program – Artificial Intelligence
Page 3


## Page 4

Reinforcement Learning
Laboratory 05
• Update rule: the key operation is the Q-Learning update:
Q(St, At) ←Q(St, At) + α

Rt+1 + γ max
a
Q(St+1, a) −Q(St, At)

.
This adjusts the estimate Q(St, At) toward a target value based on the immediate reward
plus the discounted estimate of the best possible future action. The quantity in brackets is
known as the TD error:
δt = Rt+1 + γ max
a
Q(St+1, a) −Q(St, At).
• Bootstrapping and off-policy learning: the update uses the maximum future Q-value,
regardless of which action is actually taken next. Therefore, learning is based on the optimal
greedy policy while behaviour remains exploratory. This separation of target and behaviour
policies is what makes Q-Learning an off-policy algorithm.
• Convergence: provided all state–action pairs continue to be updated and the learning rate α
decreases appropriately, Q-Learning converges with probability 1 to the optimal action-value
function q∗(s, a).
Interpretation:
intuitively, Q-Learning can be seen as a continual process of reducing the dif-
ference between what the agent expected and what it actually observed. The update rule moves
Q(S, A) toward a better estimate of the long-term return, making the agent’s policy increasingly
greedy with respect to Q over time. Eventually, this leads to the emergence of an optimal policy:
π∗(s) = arg max
a
Q(s, a),
which prescribes the best action to take in every state.
1.5
Visualizing learned policies
Once the Q-Learning algorithm has converged, the learned action-value function Q(s, a) encodes
the agent’s knowledge about which actions lead to higher long-term rewards. To interpret and
assess the quality of this knowledge, it is instructive to visualize both the learned value function
and the resulting policy.
Master’s Program – Artificial Intelligence
Page 4


## Page 5

Reinforcement Learning
Laboratory 05
1. Extracting the greedy policy:
after training, the optimal action for each state is obtained
by selecting the action with the highest estimated Q-value:
π∗(s) = arg max
a
Q(s, a).
This policy represents the agent’s best-known strategy for maximizing cumulative reward.
For
environments with a spatial or grid-like structure (e.g., Gridworld or FrozenLake), each state can
be visualized as a cell, with arrows or symbols indicating the chosen optimal action.
2. Visualizing the value function:
the learned Q-values can also be aggregated to obtain a
scalar estimate of each state’s value under the optimal policy:
V ∗(s) = max
a
Q(s, a).
Plotting V ∗(s) as a heatmap reveals how the agent perceives the environment in terms of expected
return. States near the goal or high-reward regions should appear with larger values, while states
associated with penalties or risky transitions (such as cliffs or holes) typically have lower values.
3.
Interpreting the visualization:
visualizing learned policies provides qualitative insights
into the learning process:
• it allows verification that the agent has learned intuitively correct behaviour (e.g., navigating
efficiently toward the goal);
• it highlights areas where the policy remains uncertain or suboptimal due to insufficient ex-
ploration or stochastic rewards;
• comparing intermediate visualizations during training illustrates how the policy evolves from
random exploration to structured, goal-directed behaviour.
4. Practical implementation:
in practical experiments, visualization can be achieved using:
• matplotlib for heatmaps of V ∗(s) or per-action Q-values,
• arrows or colour-coded directions overlaid on grid maps to display π∗(s),
• episode trajectory plots showing paths taken by the agent under the learned policy.
These visual tools transform raw numerical Q-values into an interpretable representation of
learned behaviour, allowing deeper understanding of how Q-Learning constructs and refines an
optimal policy through experience.
Problem Set
Problem 1
Implement the Q-Learning algorithm. Use a small gridworld environment such as FrozenLake-v1
from the OpenAI Gym library to test your implementation. Begin by initializing the action-value
function Q(s, a) to zero for all states and actions. Adopt the following hyperparameters throughout
the experiment: learning rate α = 0.1, discount factor γ = 0.99, and exploration rate ε = 0.1. Train
the agent for approximately 10,000 episodes, ensuring that each episode terminates when a terminal
Master’s Program – Artificial Intelligence
Page 5


## Page 6

Reinforcement Learning
Laboratory 05
state is reached (i.e., when the agent reaches the goal or falls into a terminal cell). At the end of
training, extract the greedy policy defined as π∗(s) = arg maxa Q(s, a) and visualize it over the grid
to show the optimal action at each state.
Note: Because actions in FrozenLake-v1 have stochastic outcomes (the agent may slip unpre-
dictably), training should be repeated for a large number of episodes to ensure that the policy
converges despite environmental uncertainty.
Problem 2
Monitor how the agent’s performance evolves during training. Record the total reward obtained in
each episode and plot a learning curve showing the moving average of episode rewards to smooth
out variability. Observe whether the cumulative reward increases and eventually stabilizes, and
discuss what this behaviour indicates about the convergence of the learning process. Highlight any
oscillations or plateaus in the curve and relate them to exploration, environment stochasticity, or
the choice of hyperparameters. In addition to total rewards, track the success rate (percentage of
episodes where the agent reaches the goal). Compare how both metrics evolve to evaluate whether
high reward corresponds to consistent goal-reaching behaviour.
Problem 3
Examine how key hyperparameters influence the efficiency and stability of Q-Learning. Vary the
learning rate α (for example, test values 0.1, 0.5, and 0.9) to observe how aggressively the Q-values
are updated and how quickly the algorithm converges. Similarly, experiment with different explo-
ration rates ε (such as 0.01, 0.1, and 0.3) to analyse how the balance between exploration and
exploitation affects performance. Optionally, replace the fixed ε with an exponentially decaying
schedule εt = ε0e−kt to investigate how gradually reducing exploration impacts learning. Sum-
marize your findings and identify parameter combinations that yield the most stable and efficient
learning.
Examine whether large α values cause oscillations or divergence in Q-values and whether low ε
values result in premature exploitation.
Problem 4
After training, visualize the learned policy and the corresponding value function to gain insight
into the agent’s behaviour. Compute the optimal state-value function V ∗(s) = maxa Q(s, a) for
all states, and represent it as a heatmap to illustrate how the agent values different regions of the
environment. Overlay arrows on the grid to indicate the greedy actions π∗(s), thereby showing the
optimal direction of movement at each state. Analyse the visualization and comment on whether
the agent’s strategy reflects efficient, goal-oriented navigation, or if any suboptimal regions remain
due to insufficient exploration.
Optionally, modify the environment by increasing its grid size or altering the number of holes,
and observe whether the learned policy generalizes or adapts effectively to the new dynamics.
Reflection and Discussion
Conclude the lab by addressing several conceptual questions to consolidate your understanding.
Explain why Q-Learning is considered an off-policy algorithm and describe how this property dif-
ferentiates it from on-policy approaches such as SARSA. Discuss the role of the ε-greedy behaviour
policy in ensuring adequate exploration and its importance for convergence to the optimal solution.
Master’s Program – Artificial Intelligence
Page 6


## Page 7

Reinforcement Learning
Laboratory 05
Finally, reflect on how the bootstrapping nature of temporal-difference learning allows Q-Learning
to learn more efficiently from limited experience compared to Monte Carlo methods, which require
full episodes to compute returns.
Master’s Program – Artificial Intelligence
Page 7
