# ENVIRONMENT SETTINGS
GRID_SIZE = 20
NUM_AGENTS = 15
NUM_EXITS = 1
NUM_OBSTACLES = 10

AGENT_START = None
EVACUATION_ZONES = None

# REALISM SETTINGS
COLLISION_AVOIDANCE = True
VISION_RADIUS = 3

# HAZARD SETTINGS
INITIAL_HAZARDS = [] 
SPREAD_PROBABILITY = 0.1 
MAX_SPREAD_STEPS = GRID_SIZE * 3 

# REWARD STRUCTURE
REWARD_STEP = -1
REWARD_EVACUATION = 100
REWARD_HAZARD = -50
REWARD_WALL = -2
REWARD_OBSTACLE = -2
REWARD_COLLISION = -2
REWARD_CLOSER_TO_EXIT = 3
REWARD_FARTHER_FROM_EXIT = -1

# DYNA-Q HYPERPARAMETERS (Project #7: Dyna-Q, not Dyna-Q+)
ALPHA = 0.15
GAMMA = 0.95
EPSILON = 0.3
EPSILON_DECAY = 0.999
EPSILON_MIN = 0.05
PLANNING_STEPS = 50
KAPPA = 0.0  # 0 for Dyna-Q, >0 for Dyna-Q+

# TRAINING SETTINGS
NUM_EPISODES = 100
MAX_STEPS_PER_EPISODE = GRID_SIZE * 4
EVAL_FREQUENCY = 100

# PATHS
OUTPUTS_DIR = "outputs"
PLOTS_DIR = "outputs/plots"
MODELS_DIR = "outputs/models"
